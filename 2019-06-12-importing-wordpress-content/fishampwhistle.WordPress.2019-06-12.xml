<?xml version="1.0" encoding="UTF-8" ?>
<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your site. -->
<!-- It contains information about your site's posts, pages, comments, categories, and other content. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your site. -->

<!-- To import this information into a WordPress site follow these steps: -->
<!-- 1. Log in to that site as an administrator. -->
<!-- 2. Go to Tools: Import in the WordPress admin panel. -->
<!-- 3. Install the "WordPress" importer from the list. -->
<!-- 4. Activate & Run Importer. -->
<!-- 5. Upload this file using the form provided on that page. -->
<!-- 6. You will first be asked to map the authors in this export file to users -->
<!--    on the site. For each author, you may choose to map to an -->
<!--    existing user on the site or to create a new user. -->
<!-- 7. WordPress will then import each of the posts, pages, comments, categories, etc. -->
<!--    contained in this file into your site. -->

	<!-- generator="WordPress/5.1.1" created="2019-06-12 20:59" -->
<rss version="2.0"
	xmlns:excerpt="http://wordpress.org/export/1.2/excerpt/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.2/"
>

<channel>
	<title>Fish &#38; Whistle</title>
	<link>https://apps.fishandwhistle.net</link>
	<description>environmental computing for the rest of us</description>
	<pubDate>Wed, 12 Jun 2019 20:59:28 +0000</pubDate>
	<language>en-US</language>
	<wp:wxr_version>1.2</wp:wxr_version>
	<wp:base_site_url>https://apps.fishandwhistle.net</wp:base_site_url>
	<wp:base_blog_url>https://apps.fishandwhistle.net</wp:base_blog_url>

		<wp:author><wp:author_id>1</wp:author_id><wp:author_login><![CDATA[dewey]]></wp:author_login><wp:author_email><![CDATA[apps@fishandwhistle.net]]></wp:author_email><wp:author_display_name><![CDATA[dewey]]></wp:author_display_name><wp:author_first_name><![CDATA[]]></wp:author_first_name><wp:author_last_name><![CDATA[]]></wp:author_last_name></wp:author>

				
	<generator>https://wordpress.org/?v=5.1.1</generator>

<image>
	<url>https://apps.fishandwhistle.net/wp-content/uploads/2013/10/cropped-fish_600-32x32.png</url>
	<title>Fish &amp; Whistle</title>
	<link>https://apps.fishandwhistle.net</link>
	<width>32</width>
	<height>32</height>
</image> 

		<item>
		<title>
				Prairie Coordinates version 0.2 release		</title>
		<link>https://apps.fishandwhistle.net/archives/498</link>
		<pubDate>Mon, 17 Sep 2012 17:00:03 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://dewey.fishandwhistle.net/?p=498</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.pcoord"><img class="alignleft size-thumbnail wp-image-678" alt="Priaire Coordinates" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/prairie-dog-wallpaper-150x150.jpg" width="150" height="150" /></a>I started writing Prairie Coordinates back in March after being completely and utterly confused with the Alberta Township System, which is how the entire oil &amp; gas industry refers to job sites in western Canada. The only other apps that manage this cost money and require you to be connected to the internet, two things I feel nobody should have to do considering the entire dataset is available for free, online. A month later I had Prairie Coordinates 0.1, covering Alberta, then updated to 0.12 adding support for Saskatchewan. Unfortunately, Manitoba doesn't publish their data in text format, and I'm not quite up to parsing ArcGIS shapefiles, but for version 0.2 I added the features I had always wanted, namely the ability to export to and import from Google Earth, since that's where I keep my personal list of sites. The official write-up on the new version:

New in version 0.2:
<ul>
	<li>Import and export KML and CSV files for import into Google Earth (KML) or other GIS programs (CSV).</li>
	<li>National Topographic System (NTS) coordinates are now supported for Canada, including series, map area, mapsheet, block, and unit.</li>
	<li>Look up NTS mapsheet information and download digital maps from GeoGratis</li>
</ul>
Visit the page on <a href="https://play.google.com/store/apps/details?id=ca.fwe.pcoord">Google Play</a> for more!]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>498</wp:post_id>
		<wp:post_date><![CDATA[2012-09-17 11:00:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-09-17 17:00:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[prairie-coordinates-version-0-2-release]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="prairie-coordinates"><![CDATA[prairie coordinates]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				GopherNotes Release 0.1		</title>
		<link>https://apps.fishandwhistle.net/archives/570</link>
		<pubDate>Sat, 03 Nov 2012 02:46:20 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://dewey.fishandwhistle.net/?p=570</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.gophernotes"><img class="alignleft size-thumbnail wp-image-678" alt="GopherNotes" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/prairie-dog-wallpaper-150x150.jpg" width="150" height="150" /></a>After thinking about writing this app for the last 10 months and getting sidetracked with two others, I finally finished a passably stable version of the field-note taking app GopherNotes. It basically takes over the role of the field notebook, so that data entry and note taking are done at the same time, simplifying the import to GIS programs or Excel dramatically. Locations can be entered by UTM, Lat/Lon, picking a point from a Google Map, or from the phone's (admittedly crappy) GPS. You can add photos to datapoints (geotagged automatically), export to Google Earth, and send your exported files by Email, Bluetooth, or whatever your favourite file-sharing app happens to be.

Current issues include a problem rotating photos and viewing them in GoogleEarth, which doesn't read EXIF data. The phone apparently is too lazy to do this itself, but diligently records the information about how the photo should be rotated. Rest assured, the team is working on it, although dealing with multimedia is not my strong suit.
<ul>
	<li><a href="https://play.google.com/store/apps/details?id=ca.fwe.gophernotes">GopherNotes on Google Play</a></li>
</ul>]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>570</wp:post_id>
		<wp:post_date><![CDATA[2012-11-02 20:46:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-11-03 02:46:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[gophernotes-release-0-1]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="gophernotes"><![CDATA[gophernotes]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Prairie Coordinates 0.21 update		</title>
		<link>https://apps.fishandwhistle.net/archives/576</link>
		<pubDate>Fri, 16 Nov 2012 17:52:31 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://dewey.fishandwhistle.net/?p=576</guid>
		<description></description>
		<content:encoded>
				<![CDATA[Just a quick update, fixing a bug with UTM coordinates in the southern hemisphere, updating the look of the search dialog, and adding support for more touch gestures (double tap and long press) in the map activity. The next updates should tackle the issue of a) manitoba, and b) townships with a letter in them.

<ul>
	<li><a href="https://play.google.com/store/apps/details?id=ca.fwe.pcoord">Prairie Coordinates on Google Play</a></li>
</ul>

]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>576</wp:post_id>
		<wp:post_date><![CDATA[2012-11-16 10:52:31]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-11-16 17:52:31]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[prairie-coordinates-0-21-update]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="prairie-coordinates"><![CDATA[prairie coordinates]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				(Yet another) Canada Weather App		</title>
		<link>https://apps.fishandwhistle.net/archives/578</link>
		<pubDate>Sun, 18 Nov 2012 04:38:08 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://dewey.fishandwhistle.net/?p=578</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather"><img class="alignleft size-full wp-image-685" alt="Canada Weather on Google Play" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/HBP8Lg3dkSzEfc7HhoQ9YHdazuMmL5mAKDlwhL8IT2U2Z7NziiNuwq9aAFsyEJ89Qiow124.png" width="124" height="124" /></a>I check the weather about fifty times a day. Okay, not really, but between seeing where it's snowing the most and checking to see if it will be raining too hard to dig holes (my job), I check it a lot, and usually in five or six locations. I've always liked Environment Canada's forecasts, but always found it hard to figure out which location I should be using, since there's no good map of the cities where forecasts are issued for. Current apps on Google Play all have issues that make them annoying to use, and are usually just a little too complicated for their own good.

this realization coincided with three days of watching trucks at work for 12 hours a day, so I wrote a better (maybe just simpler) one. after building the database of forecast locations using the <a href="http://www.weatheroffice.gc.ca/city/js/forecastQuickLinkArraysEn.js">javascript on Environment Canada's own website</a> (parsed with a simple java program), I used <a href="http://developers.google.com/maps/documentation/geocoding/">Google's HTTP geocoding API</a> to get lat/lons for the cities, filling in a few by hand. the rest was just minor parsing of <a href="http://www.weatheroffice.gc.ca/business/index_e.html#rss">Environment Canada's RSS feed</a>, with a little help from the open-source Canadian weather app <a href="http://play.google.com/store/apps/details?id=com.EmWeather.Em_Weather">EmWeather </a>around turning text into weather icons. after all of this, I realized Environment Canada puts out an <a href="http://dd.weatheroffice.ec.gc.ca/about_dd_apropos.txt">XML forecast format</a>, although I quite like the simplicity of the RSS feed.
<ul>
	<li><a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather">Canada Weather on the Google Play Store </a>(free)</li>
</ul>]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>578</wp:post_id>
		<wp:post_date><![CDATA[2012-11-17 21:38:08]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-11-18 04:38:08]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[yet-another-canada-weather-app]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="canada-weather"><![CDATA[canada weather]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[pw_single_layout]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Find-A-CBC		</title>
		<link>https://apps.fishandwhistle.net/archives/584</link>
		<pubDate>Fri, 30 Nov 2012 05:43:58 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://dewey.fishandwhistle.net/?p=584</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.findacbc"><img class="alignleft size-thumbnail wp-image-749" alt="Find-A-CBC" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/ic_launcher-web-150x150.png" width="150" height="150" /></a>another week of watching trucks at work, another app. one of the first things I ever wished my phone would do is try to find the local CBC station while driving across the country, or driving to work. thanks to Wikipedia's <a href="http://en.wikipedia.org/wiki/List_of_radio_stations_in_Alberta">'list of radio stations' for provinces</a> and territories in Canada, I put together a database of CBC stations, geocoded the locations, and made a few icons. everything else is more or less copy and paste from <a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather">Canada Weather,</a> which does about the same thing except for Environment Canada weather applications. For fun, a crude french translation is included.
<ul>
	<li><a href="https://play.google.com/store/apps/details?id=ca.fwe.findacbc">Find-A-CBC on Google Play</a></li>
</ul>]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>584</wp:post_id>
		<wp:post_date><![CDATA[2012-11-29 22:43:58]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-11-30 05:43:58]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[find-a-cbc]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="find-a-cbc"><![CDATA[find-a-cbc]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Canada Weather 0.2		</title>
		<link>https://apps.fishandwhistle.net/archives/633</link>
		<pubDate>Thu, 07 Feb 2013 21:18:34 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://dewey.fishandwhistle.net/?p=633</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather"><img src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/HBP8Lg3dkSzEfc7HhoQ9YHdazuMmL5mAKDlwhL8IT2U2Z7NziiNuwq9aAFsyEJ89Qiow124.png" alt="Canada Weather on Google Play" width="124" height="124" class="alignleft size-full wp-image-685" /></a>This update was a long time coming, partly because so many people are using this app now that I'm nervous to try anything new, and partly because it required that I learn how to do three completely new things in Android - Services, Broadcast Receivers, and WidgetProviders. Widgets in particular can be very tricky, and Services seem like they're straightforward but get killed whenever the phone sleeps, which is very often. Thus, the quest to migrate Canada Weather to Environment Canada's XML feed, add radar imagery, and make a desktop widget turned out to be a tall order, now hopefully complete. I can only hope that it works as well on every single other phone as it does on mine. If not, I'm deeply sorry to all 5,804 of you, but it might be a rough couple of days.

<a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather">
<ul>
	<li>Canada Weather on Google Play</li>
</ul>
</a>

Next up: French language translation, animated radar imagery, and more widget options. Stay tuned!]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>633</wp:post_id>
		<wp:post_date><![CDATA[2013-02-07 14:18:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-02-07 21:18:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[canada-weather-0-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="canada-weather"><![CDATA[canada weather]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				UnitCalc 0.2		</title>
		<link>https://apps.fishandwhistle.net/archives/635</link>
		<pubDate>Wed, 13 Feb 2013 21:59:34 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://dewey.fishandwhistle.net/?p=635</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.unitcalc"><img class="alignleft size-thumbnail wp-image-720" alt="UnitCalc" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/globe-150x150.png" width="150" height="150" /></a>This was an extremely easy but very exciting update. Before, this app was very bad at being a converter, and very bad at being a calculator. Now, I would argue that it's usable as both...but more so as a unit converter. Thus, the name change to "Unit Converter &amp; Calculator". Also included are major updates to the unit library, which I tried to document on a <a href="https://code.google.com/p/java-units/">Google Code page</a>, although it's still being tested (mostly using this app).

&nbsp;
<ul>
	<li>UnitCalc on Google Play</li>
</ul>
&nbsp;]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>635</wp:post_id>
		<wp:post_date><![CDATA[2013-02-13 14:59:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-02-13 21:59:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[unitcalc-0-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
		<category domain="post_tag" nicename="unitcalc"><![CDATA[unitcalc]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Prairie Coordinates 0.3		</title>
		<link>https://apps.fishandwhistle.net/archives/677</link>
		<pubDate>Sat, 30 Mar 2013 18:38:34 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=677</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.pcoordplus"><img class="alignleft size-thumbnail wp-image-678" alt="Prairie Coordinates" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/prairie-dog-wallpaper-150x150.jpg" width="150" height="150" /></a>This version of Prairie Coordinates (renamed Prairie Coordinates Plus) is a completely rewritten version of the app. Because this was the first app I ever wrote (a year ago), the code was extremely complicated and needed some serious upgrading - hence the complete rewrite. As part of this I added the two features that were most requested - support for using quarter sections (SE, SW, NE, NW, etc.) instead of legal subdivision, and support for locations west of the 1st meridian. There's also plenty of new features like searching by address, searching by mapsheet name and more.

Getting technical, this app completely changes the way data is stored. Before, the database contained all 4 corners of each township; now, it just contains a north/south/east/west/ limit. This means each township is approximated, although from testing so far it appears to be a very good and extremely effective space-saving approximation.

This version now costs $2, although the <a href="https://play.google.com/store/apps/details?id=ca.fwe.pcoord">old version</a> is still free...mostly reflecting the large amount of time it took me to process data from 4 provinces and rewrite the entire application. Enjoy!
<ul>
	<li><a href="https://play.google.com/store/apps/details?id=ca.fwe.pcoordplus">Prairie Coordinates Plus on Google Play</a></li>
</ul>]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>677</wp:post_id>
		<wp:post_date><![CDATA[2013-03-30 11:38:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-30 18:38:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[prairie-coordinates-0-3]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="prairie-coordinates"><![CDATA[prairie coordinates]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Canada Weather hits 10,000 users		</title>
		<link>https://apps.fishandwhistle.net/archives/731</link>
		<pubDate>Tue, 23 Apr 2013 21:29:47 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=731</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather"><img src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/HBP8Lg3dkSzEfc7HhoQ9YHdazuMmL5mAKDlwhL8IT2U2Z7NziiNuwq9aAFsyEJ89Qiow124.png" alt="Canada Weather on Google Play" width="124" height="124" class="alignleft size-full wp-image-685" /></a>With over 15,000 downloads and 10,000 active users, in just 5 months Canada Weather & Radar has become a top-rated Canadian weather app and (according to the reviews) the top free app. Who thought it would catch on? I certainly didn't. Thanks to you for using!]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>731</wp:post_id>
		<wp:post_date><![CDATA[2013-04-23 14:29:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-23 21:29:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[canada-weather-hits-10000-users]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="canada-weather"><![CDATA[canada weather]]></category>
		<category domain="category" nicename="general"><![CDATA[General]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Website Launch (finally!)		</title>
		<link>https://apps.fishandwhistle.net/archives/733</link>
		<pubDate>Tue, 23 Apr 2013 21:41:14 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=733</guid>
		<description></description>
		<content:encoded>
				<![CDATA[It's been a year since I started developing apps for Android, and it's high time the website got underway. Complete with pages for each app and a donate page, it should function for now. In the future, I'd like to add online versions of some of the conversion functions my apps use and expand the description of each to include a bit of a tutorial on how to get the most out of it, but we'll see if I ever get the time amidst updating all the apps. Happy app-ing and thanks for your support!]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>733</wp:post_id>
		<wp:post_date><![CDATA[2013-04-23 14:41:14]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-23 21:41:14]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[website-launch-finally]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="general"><![CDATA[General]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Canada Weather 0.3 Release		</title>
		<link>https://apps.fishandwhistle.net/archives/785</link>
		<pubDate>Wed, 08 May 2013 04:50:55 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=785</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather"><img class="alignleft size-full wp-image-685" alt="Canada Weather on Google Play" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/HBP8Lg3dkSzEfc7HhoQ9YHdazuMmL5mAKDlwhL8IT2U2Z7NziiNuwq9aAFsyEJ89Qiow124.png" width="124" height="124" /></a>After a few months of comments on the last release of <a href="/canada-weather">Canada Weather</a>, I decided fixing the widget layout and adding animated radar were worth it. In addition, this release includes the ability to switch icon sets (environment canada just released a new set) and switch the background to light (because the new icon set looks terrible on a dark background, and a few people have requested this feature for the widget). The only feature request missing from this version is the ability to look up an Environment Canada location using the phone's current location.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>785</wp:post_id>
		<wp:post_date><![CDATA[2013-05-07 21:50:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-08 04:50:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[canada-weather-0-3-release]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="canada-weather"><![CDATA[canada weather]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Canada Weather 0.33 release		</title>
		<link>https://apps.fishandwhistle.net/archives/788</link>
		<pubDate>Fri, 28 Jun 2013 21:04:29 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=788</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather"><img class="alignleft size-full wp-image-685" alt="Canada Weather on Google Play" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/HBP8Lg3dkSzEfc7HhoQ9YHdazuMmL5mAKDlwhL8IT2U2Z7NziiNuwq9aAFsyEJ89Qiow124.png" width="124" height="124" /></a>After a few months of complaints of missing features and bugs, I finally get around to this release, adding a few features (GPS location, menus for later versions of Android) and refined a few things (widget updating, linking to the mobile site for weather warnings, fixed a partly cloudy/mostly cloudy issue). Next on the agenda: Marine weather, current observations, and possible NOAA integration. Check it out!]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>788</wp:post_id>
		<wp:post_date><![CDATA[2013-06-28 14:04:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-06-28 21:04:29]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[canada-weather-0-33-release]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="canada-weather"><![CDATA[canada weather]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Gophernotes 0.2!		</title>
		<link>https://apps.fishandwhistle.net/archives/792</link>
		<pubDate>Mon, 01 Jul 2013 22:49:46 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=792</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.gophernotes"><img class="alignleft size-thumbnail wp-image-793" alt="note-pencil-icon" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/07/note-pencil-icon-150x150.png" width="150" height="150" /></a>Compared to the 17,000 of you that use Canada Weather, the 33 loyal GopherNotes users have been on the backburner for quite some time. As the pet project that I started writing apps to make, GopherNotes hasn't exactly caught on, probably because of the lack of attention to things like crashing and user interface. I still haven't gotten the interface as smooth as I would like it, but it's certainly better than it was before, and thanks to some experience with Prairie Coordinates and Canada Weather, it's running a whole lot more smoothly.

Also implemented was a suggestion by a geologist friend to allow collection of audio as well as video and photos so that it's possible to describe what you're seeing in case you're not in a position to write it down or type it with a really terrible phone keyboard. Unfortunately I can't figure out how to get these to play in a KMZ file in GoogleEarth, but they do play on your phone and export by ZIP if you'd like that to happen.

But what I'm really excited about is the new Maps feature. This lets you add custom KMZ files to your projects that contain Ground Overlays, and while it's tough to line them up perfectly, it extends the offline capacity for data collection.

This brings me to the part that I'm really really really excited about (but probably just because of how long it took me to write...): the interpolation feature. If you don't know what interpolation is...think pretty color map telling you where the highs and lows of your data are just minutes after you collect your data. This is viewed within the Google Maps activity, so you can do all the lovely things you've always been able to do there, such as view your GPS location, add points, and edit data. On top of all of that, any custom maps you add to the project (including interpolations) can be nicely exported to KMZ format and emailed at a moment's notice.

[caption id="attachment_800" align="alignnone" width="200"]<a href="http://apps.fishandwhistle.net/wp-content/uploads/2013/07/SC20130701-154827.png"><img class="size-medium wp-image-800 " alt="see!? so pretty!" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/07/SC20130701-154827-200x300.png" width="200" height="300" /></a> see!? so pretty![/caption]

There's certainly a long way to go to make this app the kind of 'GIS lite' I'd imagined it to be, but until I get a few more bites it's hard to justify spending the time to make it happen. Keep sending in the suggestions!

And did I mention the new icon? Now I really only have one app that is just a picture of a gopher.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>792</wp:post_id>
		<wp:post_date><![CDATA[2013-07-01 15:49:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-07-01 22:49:46]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[gophernotes-0-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="gophernotes"><![CDATA[gophernotes]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Prairie Coordinates bugfix 0.31		</title>
		<link>https://apps.fishandwhistle.net/archives/807</link>
		<pubDate>Wed, 17 Jul 2013 21:13:13 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=807</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.pcoordplus"><img class="alignleft size-thumbnail wp-image-678" alt="Priaire Coordinates" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/prairie-dog-wallpaper-150x150.jpg" width="150" height="150" /></a>It appears that Google Maps' latest update removed support for sending names (other than addresses) to Maps, and now only takes a location. This is bad news for anybody that wanted to save locations in Google Maps, and while there may be a workaround in the future, this appears to be the best case scenario for now. With this release the bug should be fixed, and while I was at it I updated a few of the icons to look pretty and fixed a crash with the 'zoom to me' feature on the map.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>807</wp:post_id>
		<wp:post_date><![CDATA[2013-07-17 14:13:13]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-07-17 21:13:13]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[prairie-coordinates-bugfix-0-31]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="prairie-coordinates"><![CDATA[prairie coordinates]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Canada Marine Launch		</title>
		<link>https://apps.fishandwhistle.net/archives/814</link>
		<pubDate>Thu, 01 Aug 2013 01:11:26 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=814</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a title="Canada Marine on Google Play" href="https://play.google.com/store/apps/details?id=ca.fwe.camarine"><img class="alignleft size-thumbnail wp-image-815" alt="Canada Marine" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/07/ic_launcher-web-150x150.png" width="150" height="150" /></a>With the discovery of the Environment Canada Marine XML feed, I decide to tackle the challenge of an additional weather app. It takes quite a bit of moving things around to create a base weather app that can then be extended to make Canada Weather &amp; Radar, Canada Marine, and eventually a NOAA version when I get the time. I've had quite a few requests from Canada Weather users to make a foray into the marine end of things, so I'm glad to have finished the project, which also helped in improving my KML library and gave me some practice parsing XML and mining web pages for useful information. Probably the biggest accomplishment is knowing how to structure custom Android libraries...such as one for mapping, location/geometry, dealing with units...to avoid copy/pasting too much code. Next step: porting the weather library to NOAA and the U.S. (which also provides a decent XML format) and translating the Canadian apps into French.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>814</wp:post_id>
		<wp:post_date><![CDATA[2013-07-31 18:11:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-08-01 01:11:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[canada-marine-launch]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="canada-marine"><![CDATA[canada marine]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Canada Weather 0.5 Series Updates		</title>
		<link>https://apps.fishandwhistle.net/archives/828</link>
		<pubDate>Mon, 09 Sep 2013 06:57:30 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=828</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather"><img src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/HBP8Lg3dkSzEfc7HhoQ9YHdazuMmL5mAKDlwhL8IT2U2Z7NziiNuwq9aAFsyEJ89Qiow124.png" alt="Canada Weather on Google Play" width="124" height="124" class="alignleft size-full wp-image-685" /></a>For a few months there have been a number of comments on how to improve Canada Weather, the most important of which was a French translation. With the help of <a href="http://translatorsmusings.blogspot.com/" title="Translator's Musings">O. Kuzin</a> (a retired professional translator) and my limited french I was able to piece together a workable french translation of the Canada Weather and Marine Weather apps, fixing a few bugs in the process. The widget is now slightly smaller (3x1...which I've already had complaints about because it won't center on a 4x5 screen), it updates more reliably, and you can now get notifications to pop up whenever there's a warning for any of the cities you have a widget for (and it never bugs you about the same warning more than once!).

Next up: Current temperature in the menu bar, and transparency options for the widget background, and a NOAA (U.S.) version of the app (coming very soon!).

Also...more than 25,000 users! Keep at it, folks.

]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>828</wp:post_id>
		<wp:post_date><![CDATA[2013-09-08 23:57:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-09-09 06:57:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[canada-weather-0-5-series-updates]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="canada-marine"><![CDATA[canada marine]]></category>
		<category domain="post_tag" nicename="canada-weather"><![CDATA[canada weather]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				NOAA Weather Launch!		</title>
		<link>https://apps.fishandwhistle.net/archives/852</link>
		<pubDate>Sun, 15 Sep 2013 02:24:54 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=852</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=ca.fwe.noaaweather"><img src="http://apps.fishandwhistle.net/wp-content/uploads/2013/09/Objects-Umbrella-icon-150x150.png" alt="Objects-Umbrella-icon" width="100" height="100" class="alignleft size-thumbnail wp-image-835" /></a>From the creator of Canada Weather & Radar and Canada Marine Weather comes an ad-free, feature rich and conveniently <a href="https://play.google.com/store/apps/details?id=ca.fwe.noaaweather">free NOAA weather app</a>. Links directly to the NOAA mobile site for more detailed forecasts, warnings, and radar coverage. Includes widget and the ability to save as many locations as you choose to quickly switch between forecasts. Turn on warning notifications to have notifications sent to your phone any time NOAA issues a watch or warning by adding a new widget.

There are, of course, a number of other options for NOAA weather apps. NOAA itself has a <a href="http://mobile.weather.gov/">wonderful mobile site</a>, in addition to <a href="https://play.google.com/store/search?q=noaa%20weather&c=apps">several apps</a> that offer radar and better tablet integration (but no warning notifications even in the paid versions!). Seeing as people pay me to buy my apps sometimes, the fact that I'm too cheap to pay for the NOAA apps that currently exist seems ironic, but the gold standard Weather.com app is also free, so people generally aren't willing to pay if there isn't something unique about the app, which for this app (and the other NOAA apps) really doesn't apply. Enjoy and send me suggestions!]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>852</wp:post_id>
		<wp:post_date><![CDATA[2013-09-14 19:24:54]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-09-15 02:24:54]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[noaa-weather-launch]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="noaa-weather"><![CDATA[noaa-weather]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				How to do() more with dplyr		</title>
		<link>https://apps.fishandwhistle.net/?p=1114</link>
		<pubDate>Mon, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1114</guid>
		<description></description>
		<content:encoded>
				<![CDATA[Post on using dplyr::do()

1. Summarising data (applying a function to groups of data)

Kind of like summarise()

2. Applying a function to each row of a data frame

But can return a data.frame() with multiple rows.

3. Split/Apply/Combine (rbinding a bunch of data frames)

like plyr but with group_by if you need to operate on a whole data frame

Super useful!



]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1114</wp:post_id>
		<wp:post_date><![CDATA[2016-07-14 15:13:05]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="general"><![CDATA[General]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Books in WordPress		</title>
		<link>https://apps.fishandwhistle.net/?p=1191</link>
		<pubDate>Mon, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1191</guid>
		<description></description>
		<content:encoded>
				<![CDATA[As a Christmas present, I am currently transforming my girlfriend's enormous book collection into a WordPress-powered library. I've always been keen on WordPress but my web development skills are woefully incomplete. Still, after browsing a book in Barnes & Noble while in Vermont about WordPress-powered web apps, I realized that what I needed was a WordPress site with a custom post type ("book") to represent each book. Custom taxonomies could be used to keep track of authors/illustrators/genres/shelving locations, and post metadata could be used to store information like ISBN numbers that could be used to enrich the content on the page (use the Goodreads/Google Books APIs to get further information, etc.). The process, as I saw it, had three components. First, I had to make a giant list of the books in the book case (and beyond, if I could stealthily track down and catalog the books in her backpack, etc.). Second, I had to develop a simple WordPress plugin that handled the book post type and custom taxonomies, as well as handle importing a huge list of books. Third, I had to develop a WordPress theme to display the books a little more smartly than posts, since they contained different information than a post

<h3>Make a huge list of books</h3>


<h3>Make a WordPress Plugin</h3>


<h3>Make a WordPress Theme</h3>

The theme is probably the most difficult of the three tasks. I used the twentytwelve theme to modify, but it is theoretically possible to modify any theme to fit the needs of a custom post type. Essentially, most themes have a "content-{post-format}.php" series of templates, for which there should be a "content-book.php" template for the post type "book". The problem is, "book" is a post type and not a post format, and unfortunately there are no catch-all methods to deal with this.

After some sleuthing, it seems that WordPress displays posts using 4 main templates: archive.php, single.php, tag.php, category.php, taxonomy.php, author.php, and search.php. Adding an archive-book.php file (copied from archive.php) and a single-book.php file (copied from single.php) will ensure that archive and single-item pages specific to the 'book' post type will be directed to that template. Adding a taxonomy.php file (copied from tag.php) will handle all custom taxonomy browsing, but if there are other custom taxonomies that do not apply to 'books', they will also call this template. For all the others, it is necessary to modify "the loop" such that the 'content-book.php' template is called instead of the regular template. This is fairly easy to do:

[markdown]
```php
if($post->post_type == 'book') {
    get_template_part( 'content', 'book' );
} else {
    get_template_part( 'content', get_post_format() );
}
```
[/markdown]

[caption id="attachment_1192" align="alignleft" width="300"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/11/template-hierarchy-300x188.png" alt="This beautiful image from the Codex describes which templates are called when" width="300" height="188" class="size-medium wp-image-1192" /> This beautiful image from the Codex describes which templates are called when[/caption]

Specialized sidebars are just sidebar-{sidebarnme}.php, and can be called using "get_sidebar("sidebarname")". 

[markdown]
```php
function twentytwelve_widgets_init() {
    register_sidebar( array(
        'name' => __( 'Books Widget Area', 'twentytwelve' ),
        'id' => 'sidebar-4',
        'description' => __( 'Appears when browsing books archives, books taxonomy, or a single book', 'twentytwelve' ),
        'before_widget' => '<aside id="%1$s" class="widget %2$s">',
        'after_widget' => '</aside>',
        'before_title' => '<h3 class="widget-title">',
        'after_title' => '</h3>',
    ) );
}
// make sure this hook is added after:
add_action( 'widgets_init', 'twentytwelve_widgets_init' );
```
[/markdown]

Then change "sidebar-1" to "sidebar-4" in the sidebar-book.php template.

Custom search?
]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1191</wp:post_id>
		<wp:post_date><![CDATA[2016-11-25 18:55:56]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Building the KINEROS hydrologic modeling program, 25 years later		</title>
		<link>https://apps.fishandwhistle.net/?p=1231</link>
		<pubDate>Mon, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1231</guid>
		<description></description>
		<content:encoded>
				<![CDATA[The KINEROS model is a legacy DOS program used to model erosion and discharge based on rainfall and slope characterization inputs.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1231</wp:post_id>
		<wp:post_date><![CDATA[2017-02-28 14:54:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="fortran"><![CDATA[Fortran]]></category>
		<category domain="category" nicename="general"><![CDATA[General]]></category>
		<category domain="post_tag" nicename="hydrology"><![CDATA[hydrology]]></category>
		<category domain="post_tag" nicename="kineros"><![CDATA[KINEROS]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				New Logo!		</title>
		<link>https://apps.fishandwhistle.net/archives/855</link>
		<pubDate>Mon, 14 Oct 2013 17:43:48 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=855</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<img class="alignleft size-thumbnail wp-image-856" alt="Fish &amp; Whistle Environmental" src="http://apps.fishandwhistle.net/wp-content/uploads/2013/10/fish_600-150x150.png" width="150" height="150" />Thanks to the magical graphic design of one Lauren Peters-Collaer, Fish &amp; Whistle has a logo! As Fish &amp; Whistle expands to other platforms and into custom technical drawing, it feels good to have a solid image (and a <a href="http://www.youtube.com/watch?v=G487EDeXadA">solid song</a>) behind the company's work. To the future!]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>855</wp:post_id>
		<wp:post_date><![CDATA[2013-10-14 10:43:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-10-14 17:43:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[new-logo]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="general"><![CDATA[General]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>254</wp:comment_id>
			<wp:comment_author><![CDATA[GEORGE KELLY]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[gdk536@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[75.145.15.18]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-02-21 08:25:54]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-02-21 15:25:54]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[How can I purchase Two of the logo for my boat as decals as the name on the boat is fish n' whistle?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:4:{s:4:"time";d:1392996354.62782192230224609375;s:7:"message";s:28:"Akismet cleared this comment";s:5:"event";s:9:"check-ham";s:4:"user";s:0:"";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>
				Canada Weather 1.0!		</title>
		<link>https://apps.fishandwhistle.net/archives/873</link>
		<pubDate>Tue, 10 Jun 2014 18:33:39 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=873</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/HBP8Lg3dkSzEfc7HhoQ9YHdazuMmL5mAKDlwhL8IT2U2Z7NziiNuwq9aAFsyEJ89Qiow124.png"><img src="http://apps.fishandwhistle.net/wp-content/uploads/2013/04/HBP8Lg3dkSzEfc7HhoQ9YHdazuMmL5mAKDlwhL8IT2U2Z7NziiNuwq9aAFsyEJ89Qiow124.png" alt="Canada Weather on Google Play" width="124" height="124" class="alignleft size-full wp-image-685" /></a>It's been a 6-month odyssey, but finally Canada Weather 1.0 is ready! The trip consisted of a complete rewrite of what was otherwise a completely disjointed assembly of libraries that worked poorly together and made it difficult to fix problems. Also, trying to maintain backwards compatibility to Android 2.2 made life difficult, so once I got my Android 4.0 phone up and running I started writing...and a semester of school later here we are! New features include improved widget updating, a resizable widget, smarter weather notifications (for when warnings are issued), faster loading, improved look & feel, and from a developer standpoint (not that you really care) - easier to understand! Thanks for all the feedback in the past few days, all the big things wrong should now be fixed. Also, thanks for all the <a href="http://apps.fishandwhistle.net/donate" title="Donate">donations</a>! You guys are the best.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>873</wp:post_id>
		<wp:post_date><![CDATA[2014-06-10 11:33:39]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-06-10 18:33:39]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[canada-weather-1-0]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="canada-weather"><![CDATA[canada weather]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Introducing Canada Topo!		</title>
		<link>https://apps.fishandwhistle.net/archives/891</link>
		<pubDate>Thu, 30 Oct 2014 18:26:46 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=891</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://play.google.com/store/apps/details?id=net.fishandwhistle.ctexplorer"><img src="http://apps.fishandwhistle.net/wp-content/uploads/2015/02/icon4-150x150.png" alt="icon4" width="150" height="150" class="alignleft size-medium wp-image-878" /></a>After months of poking away at this project, Fish & Whistle is proud to release the first version of Canada Topo. Canada Topo takes advantage of the free online topographic maps published by the Government of Canada. Canada Topo is designed to display the National Topographic System (NTS) grid for Canada and display topographic maps, as well as assist in locating other government-published data organized by the NTS grid. It also acts as a basic GPS, offering tracking and waypoint functions as well as GPX/KML export. Search for addresses and cities, measure areas and distances, and easily view locations in Google Earth, Google Maps, or other maps applications. Because this app downloads and stores topographic maps on local storage, topo maps are available for offline use.

<a href="https://play.google.com/store/apps/details?id=net.fishandwhistle.ctexplorer"><img alt="Get it on Google Play" src="https://developer.android.com/images/brand/en_generic_rgb_wo_60.png" /></a>]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>891</wp:post_id>
		<wp:post_date><![CDATA[2014-10-30 11:26:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-10-30 18:26:46]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[introducing-canada-topo]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Basemaps, eh?: A guide to creating Canadian basemaps using {rcanvec}, {rosm} and {prettymapr}		</title>
		<link>https://apps.fishandwhistle.net/archives/924</link>
		<pubDate>Sun, 22 Nov 2015 19:49:46 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=924</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
## Install the packages

See if you can install the packages – they're on CRAN and should be available for most verions of R. Updating R is a pain, but it may be what you have to do to make sure you can install the packages. If you already have the packages you'll want to do this anyway to make sure you have the latest version of all of them.

``` r
install.packages("prettymapr")
install.packages("rosm")
install.packages("rcanvec")
```

Later on you'll also need some data, which you can find in the [tutorial notes](/wp-content/uploads/2015/11/rcanvec_tutorial.zip).

## Base plotting

Part of being easy to use means these packages work well with the {sp} package, or any other package that uses base plotting in R (e.g. {marmap}, {cartography}, {OpenStreetMap}). Because of this, it's worth taking a look at how to use the plot() function and a few related methods. First, we'll setup the data we'll use in our examples.

``` r
# set up data
xs <- c(-64.35134, -64.36888, -64.35984)
ys <- c(45.06345, 45.08933, 45.09176)
mylabels <- c("Graveyard", "Huggins", "TAN")
```

### plot()

The `plot(x, y)` command creates a new plot and plots its arguments as points. Usually {rcanvec} or {rosm} will take care of creating the initial plot, but it's worth knowing that behind the scenes, this is what's happening. You can use `xlim=c(X, X)`, and `ylim=c(X, X)` to change the extents of the plot.

``` r
plot(xs, ys)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-3-1.png)<!-- -->

Now that we have our basic plot, we'll use a few other functions to draw on top of the plot we have just created.

### points()

It's a bit redundant to add points on top of the ones we've just created, but often you'll want to add points to a plot without creating a new one. The syntax is exactly the same as for `plot()`. Use `cex` (or **c**haracter **ex**pantion factor) to make your points bigger, `pch` (or '**p**lotting **ch**aracter'; see this [Quick-R page](http://www.statmethods.net/advgraphs/parameters.html) for options) to change the symbol, or `col` to change the colour (try "black", "white", "\#554499", etc.).

``` r
# initial plot
plot(xs, ys)
# add points
points(xs, ys, pch=2)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-4-1.png)<!-- -->

### arrows()

This does exactly what you'd think: draw an arrow from `x1`, `y1` to `x2`, `y2`. Use the `length` argument to make the arrows bigger or smaller (in inches).

``` r
# initial plot
plot(xs, ys)
# draw an arrow from "Graveyard" to "Huggins"
arrows(xs[1], ys[1], xs[2], ys[2])
```

![](/wp-content/uploads/2015/11/unnamed-chunk-5-1.png)<!-- -->

### lines()

If you'd like to connect your points with lines, the `lines()` function is for you! In addition to the `col` argument, we can also pass `lwd` (line width; try from 0.5 to 2), or lty (line type; you can pick [any number from 0 to 6](http://www.statmethods.net/advgraphs/parameters.html)).

``` r
# initial plot
plot(xs, ys)
# add lines from the graveyard to huggins to TAN.
lines(xs, ys)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-6-1.png)<!-- -->

### text()

Labels are often useful, and adding them to the map is no problem. Use cex to control font size (try from 0.5 to 2), col to control colour, and adj to offset the label. Generally you will want to use `adj=c(-0.2, 0.5)`, which will offset the label to the right of your points.

``` r
# initial plot
plot(xs, ys)
# add labels
text(xs, ys, labels=mylabels)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-7-1.png)<!-- -->

### polygon()

As advertized, creates a polygon based on the verticies provided. Try border to change the border colour, col to change the fill colour, lty to change the border line type, and lwd to change the border line width.

``` r
# initial plot
plot(xs, ys)
# add a polygon between the three points
polygon(xs, ys, col="green", border="red")
```

![](/wp-content/uploads/2015/11/unnamed-chunk-8-1.png)<!-- -->

### locator()

Often it's desirable to place something on the plot that you don't necessarily know the coordinates to. To find these coordinates, we can capture mouse clicks on a plot using the locator() function. This will return a data.frame of XY coordinates of the points we clicked on. Remember to hit ESC when done!

``` r
# use locator() to find points (hit ESC when done!)
locator()
```

It's also possible to capture this to a variable, but you'll eventually want to somehow hardcode these values into your script (because you won't want to click on the map every single time you run your script).

### More Resources

There are numerous graphical parameters you can pass to these functions, which are documented in great detail at [Quick-R](http://www.statmethods.net/advgraphs/parameters.html) and in the [R man page for par](https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/par.html). The same parameters will be used when you plot spatial data, so it's worth becoming familiar with how to make points, lines, and polygons the way you'd like them to look.

## Plotting Spatial Data

The {rgdal} and {sp} packages provide a powerful engine to render all kinds of geographical data. The data used in this section can be found in the [tutorial notes](/wp-content/uploads/2015/11/rcanvec_tutorial.zip) you've hopefully already downloaded. First, we'll have to load the packages.

``` r
#load packages
library(sp)
library(rgdal)
```

Next, we'll load the data. `readOGR()` from the {rgdal} package lets us load shapefiles, among many other data types. The syntax is a bit tricky, but for shapefiles dsn is the folder that contains your shapefile, and layer is the name of the shapefile (without the ".shp").

``` r
#use rgdal::readOGR() to read shapefile
#dsn=SOURCE FOLDER, #layer=FILENAME (minus .shp)
altashoreline <- readOGR(dsn="data", layer="alta_shoreline") 
```

    ## OGR data source with driver: ESRI Shapefile 
    ## Source: "data", layer: "alta_shoreline"
    ## with 1 features
    ## It has 23 fields

Once we've loaded the data, we can plot it. Use the graphical parameters such as col, border, and lwd we discussed earlier to make the data look decent. You can also use `axes`, `xlim`, and `ylim` to customize the appearance of the plot and extents, respectively. With this plot command we can also use `add=TRUE` to plot another file on top (instead of creating a new plot).

``` r
# plot data
plot(altashoreline, col="lightblue")
```

![](/wp-content/uploads/2015/11/unnamed-chunk-12-1.png)<!-- -->

Now that we’ve plotted our base data, we can draw on top of it using `lines()`, `polygon()`, `arrows()`, and `text()`. As an example, we'll load a file containing the locations of my master's thesis sample locations.

``` r
# plot shoreline
plot(altashoreline, col="lightblue")
# plot Alta Lake and cores
altacores <- read.delim("data/alta_cores.txt")
points(altacores$lon, altacores$lat, pch=15, cex=0.9)
text(altacores$lon, altacores$lat, labels=altacores$name, adj=c(-0.2, 0.5), cex=0.9)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-13-1.png)<!-- -->

## Using {rosm} to plot basemaps

The {rosm} package pulls [Bing Maps](https://www.bing.com/maps/), [Open Street Map](https://www.openstreetmap.org/), and [related maps](http://wiki.openstreetmap.org/wiki/Tile_servers) from the internet, caches them locally, and renders them to provide context to overlying data (your sample sites, etc.). Again, for details, take a look at the [{rosm} manual](https://cran.r-project.org/web/packages/rosm/rosm.pdf). First we'll load the packages.

``` r
library(prettymapr)
library(rosm)
```

### Step 1: Find your bounding box

The {rosm} package plots based on a **bounding box**, or an area that you would like to be visible. There's a few ways to go about doing this, but the easiest way is to visit the [Open Street Maps Export page](http://www.openstreetmap.org/export), zoom to your area of interest, and copy/paste the values into `makebbox(northlat, eastlon, southlat, westlon)` from the {prettymapr} package. You can also use `searchbbox("my location name")`, also from the {prettymapr} package, which will query google for an appropriate bounding box. You'll notice that the bounding box returned by these methods is just a 2x2 matrix, the same as that returned by `bbox()` in the {sp} package.

``` r
altalake <- searchbbox("alta lake, BC")
# or
altalake <- makebbox(50.1232, -122.9574, 50.1035, -123.0042)
```

Make sure you've got your bounding box right by trying `osm.plot()` or `bmaps.plot()` with the bounding box as your first argument.

``` r
osm.plot(altalake)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-16-1.png)<!-- -->

``` r
bmaps.plot(altalake)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-17-1.png)<!-- -->

### Step 2: Choose your map type and zoom level

{rosm} provides access to a number of map types (and even the ability to load your own if you're savvy!), but the most common ones you'll use are `type=osm`, `type="hillshade"`, `type=stamenwatercolor`, and `type=stamenbw` for `osm.plot()` and `type="Aerial"` with `bmaps.plot()`. Look at all of them with `osm.types()` and `bmaps.types()`.

``` r
osm.types()
```

    ##  [1] "hikebike"               "hillshade"             
    ##  [3] "hotstyle"               "lovinacycle"           
    ##  [5] "lovinahike"             "mapquestosm"           
    ##  [7] "mapquestsat"            "opencycle"             
    ##  [9] "openpiste"              "osm"                   
    ## [11] "osmgrayscale"           "osmtransport"          
    ## [13] "stamenbw"               "stamenwatercolor"      
    ## [15] "thunderforestlandscape" "thunderforestoutdoors"

``` r
osm.plot(altalake, type="stamenbw")
```

![](/wp-content/uploads/2015/11/unnamed-chunk-18-1.png)<!-- -->

``` r
bmaps.types()
```

    ## [1] "Aerial"           "AerialWithLabels" "Road"

``` r
bmaps.plot(altalake, type="AerialWithLabels")
```

![](/wp-content/uploads/2015/11/unnamed-chunk-19-1.png)<!-- -->

The next thing we'll adjust is the zoom level. The zoom level (level of detail) is calculated automatically, but it may be that you're looking for higher (or lower) resolution. To specify a resolution specifically, use `res=300` (where 300 is the resolution in dpi; useful when exporting figures), or `zoomin=1`, which will use the automatically specified zoom level and zoom in 1 more.

``` r
bmaps.plot(altalake, zoomin=1) # res=300 will also work
```

![](/wp-content/uploads/2015/11/unnamed-chunk-20-1.png)<!-- -->

### Step 3: Add overlays

Next we'll use the `lines()`, `polygon()`, `arrows()`, and `text()` functions we went over earlier to draw on top of the map we've just plotted. What's important here is that we specifically have to tell {rosm} that we don't want it to project our data (if you plot with `axes=TRUE` you'll see that `osm.plot()` is not plotting in lat/lon. It's plotting in EPSG:3857...which you don't need to understand but may be useful if you do happen to understand it). This sounds intimidating but it's actually very easy. For larger areas this may lead to your points not lining up, but luckily there are `osm.lines()`, `osm.points()`, and `osm.polygon()` functions that will automatically project your data).

``` r
# plot without projecting
osm.plot(altalake, project=FALSE)
points(altacores$lon, altacores$lat, pch=15, cex=0.6)
text(altacores$lon, altacores$lat, labels=altacores$name, adj=c(-0.2, 0.5), cex=0.5)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-21-1.png)<!-- -->

### Step 4: Putting it all together

Putting it all together, an example plotting script might like this:

``` r
library(prettymapr)
library(rosm)

# Find bounding box
altalake <- makebbox(50.1232, -122.9574, 50.1035, -123.0042)
# Plot Bing Maps Aerial
bmaps.plot(altalake, res=300, project=FALSE, stoponlargerequest=FALSE)
# Plot overlays
points(altacores$lon, altacores$lat, pch=15, cex=0.9)
text(altacores$lon, altacores$lat, labels=altacores$name, adj=c(-0.2, 0.5), cex=0.9)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-22-1.png)<!-- -->

You'll notice that it still doesn't have a scale bar or north arrow, and the margins aren't exactly how we'd like them. It's possible to eliminate margins manually using `par(mar=c(0,0,0,0), oma=c(0,0,0,0))`, and add a scale bar using the `addscalebar()` function (in the {prettymapr} package), but the easiest way is to use the `prettymap()` function in {prettymapr} to do it all in one step. This might look like the following:

``` r
library(prettymapr)
library(rosm)

altalake <- makebbox(50.1232, -122.9574, 50.1035, -123.0042)
prettymap({bmaps.plot(altalake, res=300, project=FALSE, stoponlargerequest=FALSE)
           points(altacores$lon, altacores$lat, pch=15, cex=0.9)
           text(altacores$lon, altacores$lat, labels=altacores$name, adj=c(-0.2, 0.5), cex=0.9)})
```

![](/wp-content/uploads/2015/11/unnamed-chunk-23-1.png)<!-- -->

There's tons of options for `prettymap()` that let you customize the north arrow, scale bar etc., which you can find in the [{prettymapr} manual](https://cran.r-project.org/web/packages/prettymapr/prettymapr.pdf).

## Using {rcanvec} plot basemaps

The {rcanvec} package provides access to data produced by the Canadian government (the CanVec+ dataset) that is useful for creating basemaps for small-scale locations in Canada. If you're not in Canada, this won't help you much. Similarly, if you're trying to make a map of Ontario, this is not the package for you. The site we've been looking at so far (Alta Lake, near Whistler BC) is a couple of kilometres wide, which is about right in terms of scale. It also may be that you just want to download some data to use in an external GIS (like Arc or QGIS), in which case this package will happily export the data for you. Let's get started by loading the packages:

``` r
library(prettymapr)
library(rcanvec)
```

### Step 1: Find your bounding box

The {rcanvec} package plots based on a **bounding box**, or an area that you would like to be visible. There's a few ways to go about doing this, but the easiest way is to visit the [Open Street Maps Export page](http://www.openstreetmap.org/export), zoom to your area of interest, and copy/paste the values into `makebbox(northlat, eastlon, southlat, westlon)` from the {prettymapr} package. You can also use `searchbbox("my location name")`, also from the {prettymapr} package, which will query google for an appropriate bounding box. You'll notice that the bounding box returned by these methods is just a 2x2 matrix, the same as that returned by `bbox()` in the {sp} package.

``` r
altalake <- searchbbox("alta lake, BC")
# or
altalake <- makebbox(50.1232, -122.9574, 50.1035, -123.0042)
```

Back in the days of paper maps, NTS references were used to number maps in an orderly way. For example, the "Wolfville" 1:50,000 scale mapsheet would be referred to as "021H01", and our mapsheets for Alta Lake in BC would be 092J02 and 092J03. If you don't know what these are it's still ok, but this is how the government organizes the data. Take a minute to get familiar with your NTS reference(s), if you feel into it.

``` r
# get a list of NTS references based on a bounding box
nts(bbox=altalake)
```

    ## [[1]]
    ## [1] "092" "J"   "03" 
    ## 
    ## [[2]]
    ## [1] "092" "J"   "02"

If you run this command and your bounding box returns more than 4 mapsheets, you're probably going to want to zoom in. If you're looking to export your data, you'll need to have some idea of what these are.

### Step 2: Preview your map

{rcanvec} has a method to quickly plot a bounding box: `canvec.qplot()`. We'll pass our bounding box as the `bbox` argument, and we can use the `layers` argument to customize our layers. If things take too long to plot, you may want to just use `layers="waterbody"` (the "road" layer in particular usually takes a long time to plot). Layers you may want to use are "waterbody", "forest", "river", "contour", "building" and "road". Note that the order you put them in will change the appearance of the map.

``` r
canvec.qplot(bbox=altalake)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-27-1.png)<!-- -->

``` r
canvec.qplot(bbox=altalake, layers=c("waterbody", "river"))
```

![](/wp-content/uploads/2015/11/unnamed-chunk-27-2.png)<!-- -->

### Step 3: Refine your plotting options (optional)

It's possible (but not at all necessary) to load layers individually and plot them manually, giving us more control over the appearance of the map. You'll have to have called `canvec.qplot(bbox=XX)` or `canvec.download(nts(bbox=XX))` before you can load a layer.

``` r
waterbody <- canvec.load(nts(bbox=altalake), layerid="waterbody")
rivers <- canvec.load(nts(bbox=altalake), layerid="river")
forest <- canvec.load(nts(bbox=altalake), layerid="forest")
```

Using the graphic parameters such as `col` and `border`, we can plot our data manually. Note you'll have to use `xlim` and `ylim` arguments to zoom in. Also, for all calls to `plot()` after the first one, you'll have to pass `add=TRUE` or it will create a new plot. Because we're dealing with 2 mapsheets, canvec.load() returns a list of layers, and if you don't understand that you should pobably stick to using `canvec.qplot()`.

``` r
plot(waterbody[[2]], col="lightblue", border=0, xlim=altalake[1,], ylim=altalake[2,])
plot(forest[[2]], col="#D0EADD", border=0, add=TRUE)
plot(rivers[[2]], col="lightblue", add=TRUE)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-29-1.png)<!-- -->

If you'd still like to use the `canvec.qplot()` function, it's also possible to build this customization as a "list of lists", best shown by example:

``` r
plotoptions = list()
plotoptions$waterbody <- list(col="lightblue", border=0)
plotoptions$forest <- list(col="#D0EADD", border="#D0EADD")
plotoptions$river <- list(col="lightblue")

canvec.qplot(bbox=altalake, layers=c("waterbody", "forest", "river"), options=plotoptions)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-30-1.png)<!-- -->

### Step 4: Add overlays

Next we'll use the `lines()`, `polygon()`, `arrows()`, and `text()` functions we went over earlier to draw on top of the map we've just plotted. Unlike {rosm}, {rcanvec} plots in lat/lon natively, so we don't have to worry about projections.

``` r
canvec.qplot(bbox=altalake, layers=c("waterbody", "forest", "river"), options=plotoptions)
points(altacores$lon, altacores$lat, pch=15, cex=0.6)
text(altacores$lon, altacores$lat, labels=altacores$name, adj=c(-0.2, 0.5), cex=0.5)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-31-1.png)<!-- -->

A neat trick is to use the {rosm} package to add a hillshade on top of our map, which we would normally do before plotting our overlays. We'll have to tell `osm.plot()` not to project its data, since we're already in lat/lon.

``` r
canvec.qplot(bbox=altalake, layers=c("waterbody", "forest", "river"), options=plotoptions)
osm.plot(altalake, type="hillshade", project=FALSE, add=TRUE)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-32-1.png)<!-- -->

### Step 5: Putting it all together

Putting it all together, an example plotting script might like this:

``` r
library(prettymapr)
library(rcanvec)
library(rosm)

# Find bounding box
altalake <- makebbox(50.1232, -122.9574, 50.1035, -123.0042)
# Plot waterbody, forest, river, road
canvec.qplot(bbox=altalake, layers=c("waterbody", "forest", "river", "road"))
# Plot overlays
osm.plot(altalake, type="hillshade", project=FALSE, add=TRUE)
points(altacores$lon, altacores$lat, pch=15, cex=0.6)
text(altacores$lon, altacores$lat, labels=altacores$name, adj=c(-0.2, 0.5), cex=0.5)
```

![](/wp-content/uploads/2015/11/unnamed-chunk-33-1.png)<!-- -->

You'll notice that it still doesn't have a scale bar or north arrow, and the margins aren't exactly how we'd like them. It's possible to eliminate margins manually using `par(mar=c(0,0,0,0), oma=c(0,0,0,0))`, and add a scale bar using the `addscalebar()` function (in the {prettymapr} package), but the easiest way is to use the `prettymap()` function in {prettymapr} to do it all in one step. This might look like the following:

``` r
library(prettymapr)
library(rosm)
library(rcanvec)

altalake <- makebbox(50.1232, -122.9574, 50.1035, -123.0042)
prettymap({canvec.qplot(bbox=altalake, layers=c("waterbody", "forest", "river", "road"))
            osm.plot(altalake, type="hillshade", project=FALSE, add=TRUE)
            points(altacores$lon, altacores$lat, pch=15, cex=0.6)
            text(altacores$lon, altacores$lat, labels=altacores$name, adj=c(-0.2, 0.5), cex=0.5)})
```

![](/wp-content/uploads/2015/11/unnamed-chunk-34-1.png)<!-- -->

There's tons of options for `prettymap()` that let you customize the north arrow, scale bar etc., which you can find in the [{prettymapr} manual](https://cran.r-project.org/web/packages/prettymapr/prettymapr.pdf).

### Exporting CanVec data

If you're like me, you'll probably want to make a map in some program other than R (QGIS is my personal favourite). You can do this easily using the `canvec.export()` function. The function combines all the NTS sheets you pass into it and exports to a folder of your choosing (with nice human-readable names!).

``` r
canvec.download(nts(bbox=altalake))
canvec.export(nts(bbox=altalake), "canvec_data")
```

    ## Writing dsn: canvec_data; layer: residential_area
    ## Writing dsn: canvec_data; layer: building
    ## Writing dsn: canvec_data; layer: building_poly
    ## Writing dsn: canvec_data; layer: tank
    ## Writing dsn: canvec_data; layer: power_transmission_line
    ## Writing dsn: canvec_data; layer: contour
    ## Writing dsn: canvec_data; layer: glacial_debris_undifferentiated
    ## Writing dsn: canvec_data; layer: elevation_point
    ## Writing dsn: canvec_data; layer: permanent_snow_and_ice
    ## Writing dsn: canvec_data; layer: manmade_hydrographic_entity
    ## Writing dsn: canvec_data; layer: hydrographic_obstacle_entity
    ## Writing dsn: canvec_data; layer: river
    ## Writing dsn: canvec_data; layer: waterbody
    ## Writing dsn: canvec_data; layer: island
    ## Writing dsn: canvec_data; layer: pit
    ## Writing dsn: canvec_data; layer: mine
    ## Writing dsn: canvec_data; layer: domestic_waste
    ## Writing dsn: canvec_data; layer: lumber_yard
    ## Writing dsn: canvec_data; layer: aboriginal_lands
    ## Writing dsn: canvec_data; layer: nts50k_boundary_polygon
    ## Writing dsn: canvec_data; layer: ski_centre
    ## Writing dsn: canvec_data; layer: golf_course
    ## Writing dsn: canvec_data; layer: park
    ## Writing dsn: canvec_data; layer: trail
    ## Writing dsn: canvec_data; layer: campground
    ## Writing dsn: canvec_data; layer: picnic_site
    ## Writing dsn: canvec_data; layer: wetland
    ## Writing dsn: canvec_data; layer: named_feature
    ## Writing dsn: canvec_data; layer: railway
    ## Writing dsn: canvec_data; layer: runway
    ## Writing dsn: canvec_data; layer: road
    ## Writing dsn: canvec_data; layer: junction
    ## Writing dsn: canvec_data; layer: forest

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>924</wp:post_id>
		<wp:post_date><![CDATA[2015-11-22 12:49:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-11-22 19:49:46]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[basemaps-eh-a-guide-to-creating-canadian-basemaps-using-rcanvec-rosm-and-prettymapr]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="prettymapr"><![CDATA[prettymapr]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="rcanvec"><![CDATA[rcanvec]]></category>
		<category domain="post_tag" nicename="rosm"><![CDATA[rosm]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				{rcanvec} 0.13 release		</title>
		<link>https://apps.fishandwhistle.net/archives/947</link>
		<pubDate>Sun, 18 Oct 2015 22:12:06 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=947</guid>
		<description></description>
		<content:encoded>
				<![CDATA[After taking a GIS in R tutorial, I'm inspired to write an R interface for the CanVec+ dataset provided by GeoGratis. CanVec is a vector dataset of the data used to create Canadian topographic maps, which is very useful in creating basemaps of just about any location in Canada. By translating some code I wrote in Python years ago that calculates NTS sheet by math (and not a database of lat/lon corners) to R, I was able to make NTS sheets (which nobody really knows about anymore) mostly abstracted away from the user (if the user tries to plot a large area they may figure this out in a hurry, though). Using the <a href="http://ftp2.cits.rncan.gc.ca/pub/canvec+/doc/CanVec+_distribution_formats.pdf">documentation provided by GeoGratis</a>, layers are referred to as human-readable names (e.g. "road" instead of "tr_1760009"), which only even makes sense if you've tried to work with CanVec data in the past.

Find the package on <a href="https://cran.r-project.org/package=rcanvec">CRAN</a>, or <a href="http://github.com/paleolimbot/rcanvec">GitHub</a>!]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>947</wp:post_id>
		<wp:post_date><![CDATA[2015-10-18 15:12:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-10-18 22:12:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[rcanvec-0-13-release]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="rcanvec"><![CDATA[rcanvec]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				{prettymapr} 0.1.1 release		</title>
		<link>https://apps.fishandwhistle.net/archives/949</link>
		<pubDate>Sun, 11 Oct 2015 22:28:03 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=949</guid>
		<description></description>
		<content:encoded>
				<![CDATA[When trying to make maps in R, a few things jumped out at me as being way harder than they had to be. In particular, scale bars, north arrows, and finding lat/lon bounding boxes seemed like projects I could solve by translating old Python code I'd written. Instead of specifying a length, position, number of divisions, etc. for a scale bar, it's now possible to type <code>addscalebar()</code> and be done with it. <code>addnortharrow()</code> does the same thing, and <code>prettymap()</code> takes the liberty of fixing your margins and adding both a scale bar and north arrow (unless you specify otherwise, of course). <code>searchbbox()</code> is an adaptation of <code>ggmap::geocode()</code> that spits out a bounding box instead of a single lat/lon position.

View the new release on <a href="https://cran.r-project.org/package=prettymapr">CRAN</a> or the <a href="http://github.com/paleolimbot/prettymapr">GitHub repository</a>.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>949</wp:post_id>
		<wp:post_date><![CDATA[2015-10-11 15:28:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-10-11 22:28:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[prettymapr-0-1-1-release]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="prettymapr"><![CDATA[prettymapr]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				{rosm} 0.1.2 release		</title>
		<link>https://apps.fishandwhistle.net/archives/951</link>
		<pubDate>Tue, 20 Oct 2015 22:40:07 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=951</guid>
		<description></description>
		<content:encoded>
				<![CDATA[After playing a little bit with the <a href="https://cran.r-project.org/package=OpenStreetMap">OpenStreetMap R package</a>, I decided that I couldn't quite deal with its idiosyncrasies and that a new package could do things like enter coordinates in the usual (x, y) order and provide support for wrapping around the 180/-180 meridian (so one can make a proper map of Alaska). Like rcanvec, rosm works on the concept of a bounding box, and will happily plot any of <a href="http://wiki.openstreetmap.org/wiki/Tile_servers">16 defined tile sources</a> or Bing Maps tiles (aerial, hybrid, and maps). Adjusting DPI and changing page sizes (e.g. plotting to your screen then to a page) is a breeze, since plotting is always based on keeping the bounding box visible.

Find the release on <a href="https://cran.r-project.org/package=rosm">CRAN</a> or the <a href="http://github.com/paleolimbot/rosm">GitHub repository</a>.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>951</wp:post_id>
		<wp:post_date><![CDATA[2015-10-20 15:40:07]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-10-20 22:40:07]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[rosm-0-1-2-release]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
		<category domain="post_tag" nicename="rosm"><![CDATA[rosm]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Prairie Coordinates 0.4 release		</title>
		<link>https://apps.fishandwhistle.net/archives/953</link>
		<pubDate>Mon, 02 Nov 2015 23:20:51 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=953</guid>
		<description></description>
		<content:encoded>
				<![CDATA[During the time I spent as a grad student (mostly not developing for Android), Google changed the way apps were designed and built (instead of using Eclipse, they now have their own <a href="http://developer.android.com/tools/studio/index.html">Android Studio</a>). Prairie Coordinates was the first app I ever built, and until this release still worked on everything Android 2.2 and up. Now that Android 6 has been released, it's about time for a reboot of the app using all the things Google has added since then (especially in the new <a href="https://developers.google.com/maps/documentation/android-api/">Google Maps API</a>). You can find Prairie Coordinates on the <a href="https://play.google.com/store/apps/details?id=ca.fwe.pcoordplus">Google Play Store</a>.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>953</wp:post_id>
		<wp:post_date><![CDATA[2015-11-02 16:20:51]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-11-02 23:20:51]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[prairie-coordinates-0-4-release]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="android"><![CDATA[android]]></category>
		<category domain="post_tag" nicename="prairie-coordinates"><![CDATA[prairie coordinates]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				EXIF in R		</title>
		<link>https://apps.fishandwhistle.net/archives/956</link>
		<pubDate>Sun, 13 Dec 2015 19:18:20 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=956</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
Recently I was tasked with organizing a large number of geotagged images extracted from several years of field data. The photos came from a GPS with a camera, but because there were tons of duplicate files, any GPS waypoints they were associated with were lost. Enter EXIF data, the format in which date/time, GPS, resolution, camera make/model, and a number of other fields are stored within image files. There is no package available for this, however [exiftool](http://www.sno.phy.queensu.ca/~phil/exiftool/), written by Phil Harvey, is a multi-platform command-line interface that extracts this data and outputs in a number of formats. Using the `system()` command in R, we can write a simple wrapper around the exiftool command that produces a nice `data.frame` with all the information about our image files.

First thing is first, you're going to need to install exiftool. It's available for Windows, Mac, and Unix-oid systems (although it's a little more complicated to [install on the unix-oid ones](http://www.sno.phy.queensu.ca/~phil/exiftool/install.html#Unix)). In Windows you'll end up with an `exiftool.exe` file that you should put in your RStudio Project directory (or working directory, if you don't use RStudio). If you can type `system("exiftool")` into your R console and not get any text saying "command not found", you're good to go.

The next thing you'll need is a photo with some EXIF data. Any photo taken by a digital camera has at least *some* kind of EXIF data, so this shouldn't be hard to find. Once you have one in your RStudio project (or working directory), try the following:

```R
system("exiftool my_file.jpg")
```

You should get something like this:

```
======== ./Garmin_USA.jpg
ExifTool Version Number         : 10.07
File Name                       : Garmin_USA.jpg
Directory                       : .
File Size                       : 49 kB
File Modification Date/Time     : 2015:11:21 14:16:21-04:00
File Access Date/Time           : 2015:12:13 14:02:36-04:00
File Inode Change Date/Time     : 2015:11:21 14:16:21-04:00
File Permissions                : rw-r--r--
File Type                       : JPEG
File Type Extension             : jpg
MIME Type                       : image/jpeg
JFIF Version                    : 1.01
Exif Byte Order                 : Little-endian (Intel, II)
Modify Date                     : 1990:01:01 08:00:00
GPS Version ID                  : 2.2.0.0
GPS Map Datum                   : WGS-84
GPS Latitude Ref                : North
GPS Longitude Ref               : West
GPS Altitude Ref                : Above Sea Level
Compression                     : Uncompressed
Photometric Interpretation      : RGB
Strip Offsets                   : 425
Samples Per Pixel               : 3
Rows Per Strip                  : 84
Strip Byte Counts               : 28224
X Resolution                    : 72
Y Resolution                    : 72
Resolution Unit                 : inches
Image Width                     : 475
Image Height                    : 163
Encoding Process                : Baseline DCT, Huffman coding
Bits Per Sample                 : 8
Color Components                : 3
Y Cb Cr Sub Sampling            : YCbCr4:2:0 (2 2)
GPS Altitude                    : 365 m Above Sea Level
GPS Latitude                    : 38 deg 51' 20.15" N
GPS Longitude                   : 94 deg 47' 56.41" W
GPS Position                    : 38 deg 51' 20.15" N, 94 deg 47' 56.41" W
Image Size                      : 475x163
Megapixels                      : 0.077
```

As you can see, all the information we need is here, but it's not in a format that is particularly conducive to parsing in R. Also, things like "GPS Latitude" are in a pretty unitelligible format (we'll probably want something like `-94.526` instead of `94 deg 47' 56.41" W` if we're going to do any processing in R). Luckily, the genious behind exiftool figured this out already...all you have to do is pass the `-n` parameter. Pass the `-csv` parameter and you've got the output in nice parsing form, ready for R to convert to a `data.frame`.

```R
system("exiftool -n -csv my_file.jpg")
```

Gives us:

```
SourceFile,APP14Flags0,APP14Flags1,BitsPerSample,ColorComponents,ColorTransform,Compression,DCTEncodeVersion,Directory,EncodingProcess,ExifByteOrder,ExifToolVersion,FileAccessDate,FileInodeChangeDate,FileModifyDate,FileName,FilePermissions,FileSize,FileType,FileTypeExtension,GPSAltitude,GPSAltitudeRef,GPSLatitude,GPSLatitudeRef,GPSLongitude,GPSLongitudeRef,GPSMapDatum,GPSPosition,GPSVersionID,ImageHeight,ImageSize,ImageWidth,JFIFVersion,Megapixels,MIMEType,ModifyDate,PhotometricInterpretation,Quality,ResolutionUnit,RowsPerStrip,SamplesPerPixel,StripByteCounts,StripOffsets,XResolution,YCbCrSubSampling,YResolution
./Garmin_Asia.jpg,,,8,3,,1,,.,0,II,10.07,2015:12:13 14:45:01-04:00,2015:11:21 14:16:21-04:00,2015:11:21 14:16:21-04:00,Garmin_Asia.jpg,644,84254,JPEG,JPG,319.9804698,0,25.0617996231694,N,121.640300536606,E,WGS-84,25.0617996231694 121.640300536606,2 2 0 0,409,800x409,800,1 1,0.3272,image/jpeg,1990:01:01 08:00:00,2,,2,84,3,28224,425,72,2 2,72
```

With that, we can use `read.csv()` to process the output. Since our output is not a file, we'll have wrap our string with `textConnection()` to make it accessible to `read.csv()`. With two lines of code, we can get a `data.frame` out of our EXIF data.

```R
output <- system("exiftool -n -csv my_file.jpg", intern=TRUE)
df <- read.csv(textConnection(output), stringsAsFactors = FALSE)
```

Note that we have to pass `intern=TRUE` to `system()` in order to obtain the string produced by `exiftool` (otherwise `system()` returns `0`). Passing `stringsAsFactors=FALSE` isn't necessary but you will get odd behaviour if all of your filenames are treated as factors and not strings.

Of course, this is much more useful distilled into a function:

```R
get.exif <- function(filename) {
  command <- paste("exiftool -n -csv", 
  				paste(shQuote(filename), collapse=" "))
  read.csv(textConnection(system(command, intern=TRUE)), 
  			stringsAsFactors = FALSE)
}
```

In case you're wondering, the `paste(shQuote(filename), collapse=" ")` allows you to pass a character vector as the `filename` argument (e.g. from `list.files()`), that will be separated by `" "` into a single string (the command would then look like `exiftool -n -csv file1.jpg file2.jpg`). Using `shQuotes()` lets there be spaces in the filenames (producing a command like this: `exiftool -n -csv 'file 1.jpg' 'file 2.jpg'`.

How might this get used in real life? As I alluded to earlier, my most recent project involved organizing a number of pictures by date/time using code something like this:

```R
library(lubridate)

#define exif function
get.exif <- function(filename) {
  command <- paste("exiftool -n -csv", 
  				paste(shQuote(filename), collapse=" "))
  read.csv(textConnection(system(command, intern=TRUE)), 
  			stringsAsFactors = FALSE)
}

#load exif data from my_directory
exifdata <- get.exif(list.files(path="my_directory"))

#set output directory
outdir <- "my_new_directory"

for(i in 1:nrow(exifdata)) {
  row <- exifdata[i, ]
  d <- ymd_hms(row$DateTimeOriginal)
  ext <- tools::file_ext(row$SourceFile) #maintain file extension
  newname <- file.path(outdir, 
  				sprintf("%04d-%02d-%02d %02d.%02d.%02d.%s",
                       year(d), month(d), day(d), hour(d), minute(d),
                       second(d), ext))
  file.copy(row$SourceFile, newname)
}
```

There's some pretty heavy usage of `sprintf()` and the [lubridate](https://cran.r-project.org/package=lubridate) package (`ymd_hms()`, `year()`, `month()`, `day()` etc.) that is a topic for another day, but you get the jist of it: working with EXIF data in R isn't too bad.
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>956</wp:post_id>
		<wp:post_date><![CDATA[2015-12-13 12:18:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-12-13 19:18:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[exif-in-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="exif"><![CDATA[EXIF]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				git-R-done: Effective project management in RStudio		</title>
		<link>https://apps.fishandwhistle.net/archives/961</link>
		<pubDate>Sun, 13 Dec 2015 21:34:10 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=961</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
[RStudio](https://www.rstudio.com/) makes it easy to write and execute R code in all kinds of ways: we can run commands on the console, we can run lines from our `.R` files, we can "source" (or run the entirety of) our `.R` files, we can import packages, create packages, analyze data, create data, and I'm sure I've only scratched the surface. We usually write our R scripts for the present...to produce the chart or map or table we need for our thesis or paper or boss. There's plenty of theories and even some scholarly articles regarding [best practices for scientific computing](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745). You probably don't have time to read all of them (I certainly didn't), but it's fairly intuitive that **writing code that's easy to understand and re-use in the future is a good idea**. There's more than a few ways to go about doing this, but for the purposes of this tutorial we're going to say the three things you should do to make things easy on yourself later are **avoid copy and paste**, **structure your files**, and **use git to backup and share your code**.

## Avoid copy & paste (or *modularizing* your code)
I understand better than anybody that when you sit down to write code the first thing you have to do is *just make it work*. While you're doing that, copy and paste all you want! Until you have something that works, there's nothing worth using in the future anyway. As an example we'll take some secchi depth data from a two lakes near Halifax, Nova Scotia and plot it.

```R
sddata <- read.delim("secchi_depth_data.txt")

# get first lake data
firstlakedata <- sddata[sddata$lake=="First Lake",]

# get second lake data
secondlakedata <- sddata[sddata$lake=="Second Lake",]

# plot first lake data
plot(firstlakedata$date, firstlakedata$sd, pch=18, xlab="Date", 
     ylab="Secchi Depth")
title("First Lake Secchi Depth")

# plot second lake data
plot(secondlakedata$date, secondlakedata$sd, pch=18, xlab="Date", 
     ylab="Secchi Depth")
title("Second Lake Secchi Depth")
```

You'll notice that in plotting the data for both lakes, **the code to plot the first lake is the same as the code to plot the second** (only the name of the lake is different). This bad for a few reasons: every time I change the code to plot the data for one lake I have to make a nearly identical change in some other line of code. Not only is this time consuming (imagine if you had 30 lakes), it's a great way to introduce errors into your project. The reason you can tell immediately that this was a bad move: **I had to copy and paste my code, then change it slightly**. Any time you run into this problem, make a function.

```R
sddata <- read.delim("secchi_depth_data.txt")

# define function to plot data
plot_secchi_depths <- function(lakename) {
  lakedata <- sddata[sddata$lake==lakename,]
  plot(lakedata$date, lakedata$sd, pch=18, xlab="Date", 
       ylab="Secchi Depth")
  title(paste(lakename, "Secchi Depth"))
}

# call function with both lake names
plot_secchi_depths("First Lake")
plot_secchi_depths("Second Lake")
```

With two lakes it doesn't make much of a difference, but it saves a whole lot of work the more times you have to copy and paste something. What if you wanted to change the title of each plot? It's a simple example, but you can see how we're working towards code that can easily be reused as opposed to code specific to your project.

Taking it a step further, we can *parameterize* the function, so we can customize each call we make to the function.

```R
sddata <- read.delim("secchi_depth_data.txt")

# define function to plot data
plot_secchi_depths <- function(lakename, pch=18, col="black") {
  lakedata <- sddata[sddata$lake==lakename,]
  plot(lakedata$date, lakedata$sd, pch=pch, col=col, xlab="Date", 
       ylab="Secchi Depth")
  title(paste(lakename, "Secchi Depth"))
}

# call function with both lake names
plot_secchi_depths("First Lake", col="green")
plot_secchi_depths("Second Lake", col="red")
```

If we wanted to make the function completely self-contained, we could also pass our original `data.frame` as an argument, so that not only could we use the function in this script, but we could use the function in *any* script that had a `sd` column and a `date` column.

```R
sddata <- read.delim("secchi_depth_data.txt")

# define function to plot data
plot_secchi_depths <- function(alldata, lakename, pch=18, col="black") {
  lakedata <- alldata[alldata$lake==lakename,]
  plot(lakedata$date, lakedata$sd, pch=pch, col=col, xlab="Date", 
       ylab="Secchi Depth")
  title(paste(lakename, "Secchi Depth"))
}

# call function with both lake names
plot_secchi_depths("First Lake", col="green")
plot_secchi_depths("Second Lake", col="red")
```

You can see now that **we've created code that we could literally drop straight into another project without changing a single line of code** (provided we've formatted the data in the same way). This is a very simple example, but I have many plotting functions that I use over and over again when working on similar projects. I almost never get away with leaving a function completely untouched (there's always *something* I didn't think of when I wrote the function the first time), but **because I took the time to split my code into re-usable parts, I saved tons of time**. You'll hear this also called *modularizing* your code (as in, splitting your code into *modules*).

## Structure your files

Right now we have two files: a file for our data (secchi_depth_data.txt) and a file that contains code to plot our data. These files would probably do just fine sitting anywhere on our computer, but each time we run the code we have to make sure that our *working directory* is in the right place so when we call `read.delim("secchi_depth_data.txt")`, R knows where to find `"secchi_depth_data.txt"`. If you downloaded the tutorial notes, you already know they're within an **RStudio Project**, which is the easiest way to never have to think about a working directory ever again.

I am the first to admit that I am extremely picky about the way I structure my files. That said, I'll show how I structure my projects and why, just as an example. The real key in structuring your RStudio projects is that you **understand what your code is doing**, you can **find your data/code/output quickly**, and that you make it **easy to share your code**. Here's what our small example project might look like:

![rstudio project file directory](/wp-content/uploads/2015/12/1_projectfiles.png)

I have a **data-raw** folder that contains the original data (in this case from two separate honours theses from several years ago). If you used any R code to clean the data, this is also a good place to put it. The **data** folder in our project contains any data that will be loaded by an R script, the **scripts** folder contains R code, and the **figures** folder contains the figures that were generated from **scripts**. Sometimes I have a **tables** folder if I use R to generate a table of values that I'll later use in a document, and sometimes I have a **functions** folder if I have a lot of functions. 

You'll notice there's a new file named `plot_secchi_depths.R`, where I've moved the `plot_secchi_depths()` function we defined earlier. For such a small function, it's probably overkill to do this, but the idea is to **split the project up into independently functioning parts**. `plot_secchi_depths.R` now looks like this:

```R
# define function to plot data
plot_secchi_depths <- function(alldata, lakename, pch=18, col="black") {
  lakedata <- alldata[alldata$lake==lakename,]
  plot(lakedata$date, lakedata$sd, pch=pch, col=col, xlab="Date", 
       ylab="Secchi Depth")
  title(paste(lakename, "Secchi Depth"))
}
```

And `flsl_secchi_plots.R` now looks like this:

```R
#load data
sddata <- read.delim("data/secchi_depth_data.txt")

#load plot_secchi_depths() from plot_secchi_depths.R
source("scripts/plot_secchi_depths.R")

# call function to make both plots, save to figures/
png("figures/first_lake_sd.png")
plot_secchi_depths(sddata, "First Lake")
dev.off()

png("figures/second_lake_sd.png")
plot_secchi_depths(sddata, "Second Lake")
dev.off()
```

You'll notice a few changes to the script. First, `secchi_depth_data.txt` is now `data/secchi_depth_data.txt`, because it's now in the `data/` directory in our project. Instead of defining `plot_secchi_depths()` in the file, we now `source("scripts/plot_secchi_depths.R")`. In the plotting section, we save our plots to files instead of just looking at them (sometimes I build this into my plotting functions so that if I pass a `filename=` argument it will save, and if I don't it will plot in RStudio). The script is slightly more readable, and it does one thing (produce two plots).

This is an obviously over-the-top example, but the idea is that I'm **separating raw data**, **input**, **re-usable functions**, **project-specific code**, and **output**. Choose whichever structure makes the most sense to you, but when it comes time to **share your code using git** it will make life a little bit easier.


## Use git to backup and share

The whole point of this tutorial is to write code that you can **share** and **use easily in the future**. Now that we have nice, clean *modularized* code, we can use **git** to *create a history of the changes* we make in our project, and **GitHub** to *backup these changes* and make it easy for others (and ourselves) to use the code we've spent hours and hours writing.

### Why use Git?

Have you ever saved versions of an R script like "myscript_version1.R", "myscript_version2.R", myscript_final.R" (and inevitably) "myscript_final_real_actually.R"? Before I discovered git I certainly did, because that history is an important history to keep...what worked in the past (in case something you try in the future doesn't). You can save as many times as you want using git without ever having confusing copies of your file. Some projects can have hundreds or thousands of files, making it unweildy to do anything else. The average R script writer isn't in this category, but it's still useful to have that history kept intact. Here's an example (not from R) of a file I worked on extensively for an Android app I wrote...you can see each line of code and why I changed them.

![github blame view](/wp-content/uploads/2015/12/2_blameview.png)

### Why use GitHub?

[GitHub](http://github.com/) is a place where git repositories live on the internet. The advantage of having them there is that anybody can access your source code by just sharing a link (instead of emailing a file), making code beyond easy to share and ensuring that if anything happens to your computer, your precious deveopment history is saved. Having lost hours and hours of code to computer malfunctions before, I can assure you that it's incredibly nice to have that history somewhere safe. GitHub also has tons of features for making website and forking projects, but for now all you really have to know is that GitHub backs up your code and all the changes you've ever saved (using git).

### Ok, I'll do it!

Great! Glad to hear it. The first thing you'll need is a [GitHub Account](https://github.com/join), if you don't already have one. The second is the [GitHub Desktop App](https://desktop.github.com/). If you can open up the GitHub Desktop app and get a window something like this, you're good to go (except you won't have any repositories).

![github blame view](/wp-content/uploads/2015/12/3_githubdesktopstart.png)

### Using Git

The basic idea behind git is that you **commit** your changes, then **push** or **sync** them with GitHub. This sounds confusing but you will get the hang of it...just keep saying to yourself: **work** on my project, **commit** my changes, **sync** them with GitHub...**work** on my project, **commit** my changes, **sync** them with GitHub. Each **commit** is a moment in time that is saved in your git history (called a **repository**), so whenever you make a change that's important you should **commit** that change (and then **sync** it with GitHub so that your changes are backed up online).

Next we have to create a git **respository** in our RStudio project directory. In case you're wondering, the repository actually lives inside a special `.git` folder inside your project directory, but you don't have to know that. To start using git with a project you've already created in RStudio, click the "+" on the top left of the GitHub Desktop App screen.

![github blame view](/wp-content/uploads/2015/12/4_desktopadd.png)

![github blame view](/wp-content/uploads/2015/12/5_afteradd.png)

You will now have a very confusing looking screen telling you that you have "Uncommited Changes" in your `.Rproj.user` folder. The `.Rproj.user` folder is a folder that RStudio generates automatically that we need to tell git to ignore. To do this, choose "Repository Settings..." under the "Repository" menu.

![github blame view](/wp-content/uploads/2015/12/6_reposettings.png)

Then we have to add a few lines under "Ignored Files" so that Git knows that we don't want to track changes to files that RStudio generates automatically. The three lines that you'll need in RStudio projects are `.Rproj.user`, `.Rdata`, and `.Rhistory` (note the leading `.` in each).

![github blame view](/wp-content/uploads/2015/12/7_gitignore.png)

Once you hit "Save", you should get a much more manageable looking window showing all the files you have in the repository. If there's any more you'd like to ignore (such as files that are generated by a script or large input files) you can right click the file and choose "Ignore" (the app will automatically add a line to your `.gitignore` file). In a few steps we'll be uploading this data to GitHub, so if there's any data that shouldn't be available to the world, you'll also want to "Ignore" those files as well. If you want to "Ignore" an entire foler, you'll have to do that in the "Repository Settings" dialog that we just used. Your screen should now look like this:

![github blame view](/wp-content/uploads/2015/12/8_afterignore.png)

Now you're ready to make your first **commit**! You'll notice the check mark on the left next to all your files. For your first commit, you'll want to leave all of these checked (any file that isn't checked will not be included in your **commit**). Type a short message like "first commit", in the "Summary" box, and click "Commit to master".

![github blame view](/wp-content/uploads/2015/12/9_firstcommit.png)

Your screen should now look like this. Now that we've made our first **commit**, we will **push**/**sync** our data to GitHub. Since we haven't created a place for our **repository** on GitHub yet, we need to click on **Publish**, at the top right of the screen.

![github blame view](/wp-content/uploads/2015/12/10_afterfirstcommit.png)

Type in a name and click "Publish". **Note that once you do this anybody will be able to see the files in your project**. It's unlikely anybody will care, but if you have code or data you'd like to keep private, you'll have to upgrade your GitHub account or "Ignore" the file in the GitHub Desktop app. This is mostly not a problem, generally having your development history is more valuable than anybody wanting to steal your data/code.

![github blame view](/wp-content/uploads/2015/12/11_publish.png)

Now that you've created a place for your **repository** on GitHub, your screen should look like this. Note that "Publish" has now become **Sync**.

![github blame view](/wp-content/uploads/2015/12/12_afterpublish.png)

It's worth checking out how your repository looks on the web now, since you can now access it through the GitHub website. Go to github.com or Right click your repository and choose "View On GitHub", and you should end up with something like this:

![github blame view](/wp-content/uploads/2015/12/13_githubweb.png)

If all your files are there you've done it right! Your precious code is now backed up for all eternity (well, as long as GitHub is a business, which will probably be a long time). Now that everything is setup, let's go through another round of **work** on your project, **commit** your changes, and **sync** to GitHub. Open up your project in RStudio and edit one of your files, then hop back to the GitHub Desktop App and go to the "Uncommitted Changes" tab. I added a comment in one of my files, and when I opened up the GitHub Desktop app it looked like this.

![github blame view](/wp-content/uploads/2015/12/14_updatefile.png)

Now we have to **commit** the changes we made. Type a message describing the changes you made (and do try to be descriptive, things like "update project" are not helpful when you're looking back through your history). Click "Commit to master" to commit your changes!

![github blame view](/wp-content/uploads/2015/12/15_commitfile.png)

Then click **Sync** to upload your changes to GitHub. Now your changes are backed up! I always do this when I do something coding that I really don't want to do again (I think I already mentioned the time I lost a whole day of coding and almost cried). Try the **work** on your project, **commit** your changes, and **sync** to GitHub cycle a few more times just to get the hang of it, once things are setup it's pretty easy.

We're almost done, but the one more thing I'd like to mention is that support for Git is built in to RStudio itself! This tutorial uses the GitHub app because it's very user friendly, but for projects that I work on primarily in RStudio it's very convenient to do my committing and pushing from within RStudio itself without having to bother with the GitHub Desktop app. For this, I'll refer you to [Hadley Wickam's tutorial](http://r-pkgs.had.co.nz/git.html) on the subject.

We've only scratched the surface with regard to what Git can do, but probably 95% of the time all I do with Git is **work**, **commmit**, and **sync** just to keep my code and data backed up somewhere that won't disappear if my computer dies. For all sorts of tutorials on how to use the app for collaboration with others and contributing to open source projects, check out the [GitHub help page](https://help.github.com/).
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>961</wp:post_id>
		<wp:post_date><![CDATA[2015-12-13 14:34:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-12-13 21:34:10]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[git-r-done-effective-project-management-in-rstudio-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="rstudio"><![CDATA[RStudio]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				{rosm} 0.1.3 release		</title>
		<link>https://apps.fishandwhistle.net/archives/964</link>
		<pubDate>Sun, 13 Dec 2015 21:51:35 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=964</guid>
		<description></description>
		<content:encoded>
				<![CDATA[We're excited to announce a new release of the <a href="https://cran.r-project.org/package=rosm">rosm</a> package! A feature request was posted to export the images created by rosm as a raster package object, opening up possibilities for reprojecting and writing to disk OSM tiles for use in your favourite GIS. Thanks to Timothée Giraud, maintainer of the <a href="https://cran.r-project.org/package=cartography">cartography</a> package for suggesting and writing the base code for this update. Ready to get started? The syntax is just like <code>osm.plot()</code> but with a few options like <code>projection</code> and <code>filename</code> that reproject/write your images using the raster package. Try this:
<div class="highlight highlight-source-r">
<pre>library(<span class="pl-smi">prettymapr</span>)
library(<span class="pl-smi">rosm</span>)
library(<span class="pl-smi">raster</span>)

<span class="pl-smi">ns</span> <span class="pl-k">&lt;-</span> searchbbox(<span class="pl-s"><span class="pl-pds">"</span>nova scotia<span class="pl-pds">"</span></span>, <span class="pl-v">source</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>google<span class="pl-pds">"</span></span>)
<span class="pl-smi">rasterobj</span> <span class="pl-k">&lt;-</span> osm.raster(<span class="pl-smi">ns</span>)
plotRGB(<span class="pl-smi">rasterobj</span>)</pre>
</div>
Up next in the update queue? Faster fusing of tiles enabling the creation of high-resolution geotiffs of your favourite OSM tile maps.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>964</wp:post_id>
		<wp:post_date><![CDATA[2015-12-13 14:51:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-12-13 21:51:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[rosm-release-0-1-3]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
		<category domain="post_tag" nicename="rosm"><![CDATA[rosm]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Canada Weather &amp; Radar 1.1		</title>
		<link>https://apps.fishandwhistle.net/archives/968</link>
		<pubDate>Thu, 24 Dec 2015 15:16:55 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=968</guid>
		<description></description>
		<content:encoded>
				<![CDATA[After over a year without an update, it was high time <a href="/mobile/canada-weather">Canada Weather & Radar</a> got a facelift. Users frequently emailed requesting that alert notifications be cancellable, for dark text with transparent background for the widget, for a direct link to Environment Canada's 24-Hour forecast for each location, and for a more modern looking user interface. After migrating the app to the new version of Android Studio from Eclipse, I took a few days during Christmas vacation to piece together the new version. With 130,000 users and 4000+ positive ratings, Canada Weather has become one of the top weather apps for Canadian weather! Check it out at the <a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather">Google Play Store</a>.

[gallery ids="969,970,971"]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>968</wp:post_id>
		<wp:post_date><![CDATA[2015-12-24 08:16:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-12-24 15:16:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[canada-weather-radar-1-1]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="android"><![CDATA[android]]></category>
		<category domain="post_tag" nicename="canada-weather"><![CDATA[canada weather]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				{rcanvec} 0.1.4 release		</title>
		<link>https://apps.fishandwhistle.net/archives/975</link>
		<pubDate>Thu, 14 Jan 2016 15:43:41 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=975</guid>
		<description></description>
		<content:encoded>
				<![CDATA[Thanks to a <a href="https://github.com/paleolimbot/rcanvec/issues/1">GitHub issue</a> raised by @HnyBadjr, I was able to fix a bug that didn't properly parse NTS strings containing the letter "E" (in case you're wondering, the algorithm to parse the NTS string starts at the beginning and stops parsing the series, which is a number, when the string can't be converted to a number any longer; since the letter "E" is completely valid in numbers, this approach had to be changed). Also apparent in testing was that anything other than a string vector generated errors, so coercing the input to character input was also added. Find the new version on <a href="https://cran.r-project.org/package=rcanvec">CRAN</a>!]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>975</wp:post_id>
		<wp:post_date><![CDATA[2016-01-14 08:43:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-01-14 15:43:41]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[rcanvec-0-1-4-release]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="rcanvec"><![CDATA[rcanvec]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Open Street Map Metadata in R		</title>
		<link>https://apps.fishandwhistle.net/archives/978</link>
		<pubDate>Sat, 23 Jan 2016 20:45:27 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=978</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
The [Open Street Map](http://www.openstreetmap.org/) project is an incredible resource for those interested in accessing worldwide geographic data, which is often as good or better than that provided by Google Maps. In addition to providing pre-rendered tiles that are acessible [on the web](http://www.openstreetmap.org/), [in R](https://cran.r-project.org/package=rosm), or [in QGIS](https://plugins.qgis.org/plugins/quick_map_services/), Open Street map provides access to the raw data used to create its maps. In R, this package takes the form of the [osmar](https://cran.r-project.org/package=osmar) package.

Recently I submitted a proposal on Upwork to compare GPS tracks collected from Android phones to Open Street Map data to see which tracks were from a highway and which were not. Before submitting, I tested the waters of Open Street Map data in R just to see what was possible. What I wanted to do was:

1. Download OSM data for a given bouding box
2. Extract highway data
3. Given a lat/lon, find the nearest highway

## The Packages

I always try to keep the packages used to a minimum, but here there's a few that are necessary to keep the amount of code to a minimum. The most important is the [osmar](https://cran.r-project.org/package=osmar) package, but I'll also use [prettymapr](https://cran.r-project.org/package=prettymapr) and [geosphere](https://cran.r-project.org/package=geosphere) for a few helpful functions.

```R
library(osmar) # (geosphere is inclued in osmar)
library(prettymapr)
```


## Getting OSM Data

To retreive OSM data via the Open Street Map API, all we need to do is call the `get_osm()` function with a bounding box as the argument. `osmar` has its own lingo for describing a bounding box, so you're best off using its own methods for creating one: `center_bbox(lon, lat, width, height)` (where `width` and `height` are in metres) or `corner_bbox()`. I personally can't be bothered to type in coordinates when there's an entire internet making it so I don't have to, so I'm going to use the `searchbbox` function from the `prettymapr` package and convert the results manually:

```R
wolfville <- searchbbox("wolfville, ns", 
                              source="google")
wolfville <- c(wv[1], wv[2], wv[3], wv[4])
names(wolfville) <- c("left", "bottom", 
                                "right", "top")
class(wolfville) <- "bbox"
```

After that, getting the data is easy:

```R
osmdata <- osmar::get_osm(wolfville)
```

The `osmdata` contains an `osmar` object, which has all the 'nodes' and 'ways' associated with the data. To get a 'flat' view of the data, try: `osmdata$nodes$attrs` or `osmdata$nodes$tags`, which are `data.frame` objects that store data associated with each node. Similarly, `osmdata$ways$attrs` and `osmdata$ways$tags` store information about each 'way'. You can also `plot_ways(osmdata)` and `plot_nodes(osmdata)` to see what they look like on a map. 

## Extract Highway Data

There's lots of data other than roads in our `osmdata` object, so we need to subset the data such that we only have data associated with the roads. There's an example of this in the [vignette published by osmar authors](https://journal.r-project.org/archive/2013-1/eugster-schlesinger.pdf), which we'll adapt to our purposes.

```R
hways_data <- subset(osmdata, 
                way_ids = find(osmdata, 
                  way(tags(k == "highway"))))
hways <- find(hways_data, way(tags(k == "name")))
hways <- find_down(osmdata, way(hways))
hways_data <- subset(osmdata, ids = hways)
```

I'm still not entirely sure what this does, but it uses the `subset` and `find` methods to get any 'way' (line) that has a tag 'highway' and tag 'name'. Then, it uses the `find_down()` function to get the ways *and* the 'nodes' associated with these 'ways'. Getting used to the 'node' and 'ways' syntax used by `osmar` takes a while, but the basic idea is that a 'way' is made up of several 'nodes'. The `find_up()` function will take a node id and find the `way` it is associated with (if there is one), while the `find_down()` function will take a way and find the nodes associated with it. Anytime you pass an ID to `find()`, `find_up()`, or `find_down()` you have to specify using `way()` or `node()` which type of feature you mean. Using the `tags()` function is a way to select nodes or ways that have a key (`k=`) and/or value (	`v=`). The exact syntax for this I'm not exactly sure of, but it's not explained particularly well in the manual so I thought I'd give the basics here.

The `osmar` package provides some functions to visualize nodes and ways, notably `plot_nodes()` and `plot_ways()`. If we call both on our data, we can get a preview of what we just did:

```R
plot_ways(hways_data)
plot_nodes(hways_data, pch=19, cex=0.1, add=T) 
```

![](/wp-content/uploads/2016/01/Rplot01.png)

Let's compare this to our original data by plotting the original data in gray.

```R
plot_ways(osmdata, col="gray")
plot_ways(hways_data, add=T)
```

![](/wp-content/uploads/2016/01/Rplot02.png)

## Finding the nearest highway

Now we have a complete inventory of all the roads in our dataset, so the next step is to build a function that, given a lat/lon point, finds the nearest 'node', and then use the `find_up()` function to find the 'way' associated with that 'node'. Then, we can search the 'tags' to find the name of that road. Intimidated? It's easier broken down into a few steps. 

Let's start with the 'nearest node' bit. The lat and lon of each node can be found in the `hways_data$nodes$attrs` data frame.

```R
hway_nodes_attrs <- hways_data$nodes$attrs
lons <- hway_nodes_attrs$lon
lats <- hway_nodes_attrs$lat
```

Now if we define our point, we can create a distance matrix using the `distm()` function in the package `geosphere` (which is already loaded for us by `osmar`). If you're looking to find a point without typing (did I mention how much I hate typing?), try `locator(n=1)`.

```R
point <- c(-64.37712, 45.0862)
distances <- distm(cbind(lons, lats), point)
```

Now we can use `which.min()` to find the row with the minimum distance, and use our `hway_nodes_attrs` data frame to find the ID of that node.

```R
row <- which.min(distances)
node_id <- hway_nodes_attrs$id[row]
```

Next we'll use the `find_up()` method to find the 'way' associated with that 'node'. It's worth noting that `find_up()` is fully vectorized, so if you put in a vector of node ids then you'll get a vector of way ids back (which you may want to do if you're looking for the closest n points instead of just a single node, for example).

```R
waydata <- find_up(hways_data, node(node_id))
waydata$way_ids # == 83765582, in this example
```

To get the 'tags' associated with that, we'll `subset()` the `hways_data` object according to `way_ids`.

```R
nearest_way <- subset(hways_data, 
                     way_ids = waydata$way_ids)
nearest_way_tags <- nearest_way$ways$tags
nearest_way_tags$v[nearest_way_tags$k=="name"]
```

In this case, our answer should be `Stirling Avenue`, which seems reasonable. Note that we can't `plot_ways(nearest_way)` because we didn't include any of the nodes when subsetting the data. For an example of this, look at where we subetted the `osmdata` object in the first step, where we had to use `find_down()` to get the 'nodes' associtated with the 'ways' that were tagged "highway". It's a complicated game, but a powerful one if you can wrap your mind around it.

A cool way to visualize this is to use the `locator()` function in a loop to do this a bunch of times (in this case you'll have to hit the `Esc` key to quit) to see if the algorithm works. Try this:

```R
plot_ways(hways_data)
plot_nodes(hways_data, pch=19, 
           cex=0.1, add=T)

while(TRUE) {
  point <- locator(n=1)
  if(is.null(point)) {
    break
  }
  
  distances <- distm(cbind(lons, lats), 
                     c(point$x, point$y))
  row <- which.min(distances)
  node_id <- hway_nodes_attrs$id[row]
  
  waydata <- find_up(hways_data, node(node_id))
  nearest_way <- subset(hways_data, 
                        way_ids = waydata$way_ids)
  
  nearest_way_tags <- nearest_way$ways$tags
  name <- nearest_way_tags$v[nearest_way_tags$k=="name"]
  print(as.character(name))
}
```

Cool eh? Remeber to hit `Esc` to stop (I didn't when I was first writing this).

## Wrapping it all up

Of course, what we really want is this whole bit distilled into a nice little function. You'll notice some modifications from above but the idea is the same: get the data, extract the highways, find the nearest node (in this case you can actually get any number of closest nodes that you want).

```R
extract_highways <- function(box, apisource=osmar::osmsource_api()) {
  if(class(box) == "matrix") {
    box <- c(box[1], box[2], box[3], box[4])
    names(box) <- c("left", "bottom", "right", "top")
    class(box) <- "bbox"
  }
  osmdata <- osmar::get_osm(box, source=apisource)
  hways_muc <- subset(osmdata, way_ids = osmar::find(osmdata, osmar::way(osmar::tags(k == "highway"))))
  hways <- osmar::find(hways_muc, osmar::way(osmar::tags(k == "name")))
  hways <- osmar::find_down(osmdata, osmar::way(hways))
  hways_muc <- subset(osmdata, ids = hways)
  #data.frame(hways_muc$nodes$attrs) to get lat/lons as data frame
  return(hways_muc)
}

nearest_highway <- function(osmarobj, lon, lat, num=1) {
  ats <- osmarobj$nodes$attrs
  ats$dists <- c(geosphere::distm(cbind(ats$lon, ats$lat), c(lon, lat)))
  ats <- ats[order(ats$dists,decreasing = FALSE),]
  node_id <- ats$id[1:num]
  dist <- ats$dists[1:num]
  return(list(info=osmar::find_up(d2, osmar::node(node_id)), dist=dist))
}
```

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>978</wp:post_id>
		<wp:post_date><![CDATA[2016-01-23 13:45:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-01-23 20:45:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[open-street-map-metadata-in-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="open-street-map"><![CDATA[open street map]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				JupyterQt - Concurrency in a basic PyQt Application		</title>
		<link>https://apps.fishandwhistle.net/archives/980</link>
		<pubDate>Sat, 23 Jan 2016 22:37:40 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=980</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
One thing that has always bothered me about Python is that there is no [RStudio](http://www.rstudio.com/) that exists as an IDE for scientific analysis in Python. There are attempts, such as [Spyder](https://pythonhosted.org/spyder/) and [Canopy](https://www.enthought.com/products/canopy/) (among others), but the closest you can get is [Jupyter](http://jupyter.org/), the latest iteration of IPython Notebook. I quite like Jupyter as a venue for analysis using Python, but I don't like that it's in a web browser. This is picky, I know, but my internet is my internet and my work is not, and I don't like them in the same place.

All this leads to my programming addiction, which I employed to write a simple wrapper around the `jupyter-notebook` console command using a `QWebView` from the [PyQt](https://riverbankcomputing.com/software/pyqt/intro) bindings for the [Qt framework](http://www.qt.io/qt-framework/). Its usefulness is suspect, but it was a good excuse to tackle a problem in Python that used `threading`, the `subprocess` module, and a couple of PyQt tricks of the trade. Follow step by step or check out the [raw Python file](/wp-content/uploads/2016/01/jupyterqt_simple.py_.zip).

First off, the `import` statements. I chose to import everything individually, but the standard `from PyQt5.QtCore import *` works just as well. Also note I happen to be using PyQt5 here, but I doubt anything except the import statements are different if you happen to be using PyQt4. Also worth noting here that I'm using Python 3, although off the top of my head I don't see anything that would trip up in Python 2.

```Python
from PyQt5.QtCore import pyqtSlot, QSettings, QTimer, QUrl, QDir
from PyQt5.QtWidgets import QMainWindow, QFileDialog, QMessageBox, QApplication
from PyQt5.QtWebKitWidgets import QWebView, QWebPage

import sys
import subprocess
import signal
import logging
import threading
```

The we setup the logging. I usually create a `log()` shorthand function out of convenience. We'll be using multiple threads here, so it's handy to have the `threadName` be logged as well.

```Python
logfileformat = '[%(levelname)s] (%(threadName)-10s) %(message)s'
logging.basicConfig(level=logging.DEBUG, format=logfileformat)

def log(message):
    logging.debug(message)
```

Next we'll start the server. Basically, we want to call the `jupyter-notebook` command and hold onto that process until the end of the program. We'll use `subprocess.Popen()` to do this, since this lets us hold onto the process (unlike `os.system()`). I condensed this into a function because you may want to change the values for some of these. The important parts here are the `bufsize=1`, which makes sure that the output is line-buffered (we'll want to read it and log it line-by-line and not all at once), and `stderr=subprocess.PIPE`, which gives us access to whatever the process writes to `stderr`. In this case, that all that the process writes to, but we could just as easily get access to `stdout`.

```Python
def startnotebook(notebook_executable="jupyter-notebook", port=8888, directory=QDir.homePath()):
    return subprocess.Popen([notebook_executable,
                            "--port=%s" % port, "--browser=n", "-y",
                            "--notebook-dir=%s" % directory], bufsize=1,
                            stderr=subprocess.PIPE)
```

Next we start the notebook process. We have to wait for the server to start before opening the webpage though, because we're not sure which address it will start on (even though you specify a port, if that port is taken already it will serve on another port). We'll loop, reading the output using `Popen.stderr.readline()` to read the output, looking for a web address on every line.

```Python
#start jupyter notebook and wait for line with the web address
log("Starting Jupyter notebook process")
notebookp = startnotebook()

log("Waiting for server to start...")
webaddr = None
while webaddr is None:
    line = str(notebookp.stderr.readline())
    log(line)
    if "http://" in line:
        start = line.find("http://")
        end = line.find("/", start+len("http://"))
        webaddr = line[start:end]
log("Server found at %s, migrating monitoring to listener thread" % webaddr)
```

Now we've gotten all we need out of the process from the main thread, but we'd like to continue logging the output in case something goes wrong. We'll do this using the `threading` module, defining a function and using it as the `target` for the `Thread` object. Note that it's also possible to subclass `QThread` for multithreading in Qt, but unless you have to communicate with your thread it's usually not worth it.

```Python
#pass monitoring over to child thread
def process_thread_pipe(process):
    while process.poll() is None: #while process is still alive
        log(str(process.stderr.readline()))

notebookmonitor = threading.Thread(name="Notebook Monitor", target=process_thread_pipe,
                                   args = (notebookp,))
notebookmonitor.start()
```

This thread will run until the process is no longer running (i.e. `process.poll()` stops returning `None`, dutifully logging the output using the `log()` function we defined earlier. Now we're ready to setup the application.

```Python
#setup application
log("Setting up GUI")
app = QApplication(sys.argv)
app.setApplicationName("JupyterQt")
app.setOrganizationDomain("fishandwhistle.net")

#setup webview
view = MainWindow(None, homepage=webaddr)

log("Starting Qt Event Loop")
```

Now everything is setup, so we start the event loop.

```Python
result = app.exec_()
```

"But wait!" I can hear you say, "What the eff is 'MainWindow' and where did it come from?". Don't worry, we'll get there. First, let's pretend the user closes the application (thus ending the event loop), and so execution in our main file resumes. Now we have to make sure to properly destroy our `jupyter-notebook` process. In the termal we do this by pressing `Ctrl-C` but since we can't do that, we'll have to send an interrupt signal instead. We do this using the `Popen.send_signal()` method. Because we passed `-y` to `jupyter-notebook`, it should automatically close when we send it the interrupt signal. Still, if we don't kill the process here, it will linger on until the computer shuts down, so we want to make sure it's properly taken care of.

```Python
log("Sending interrupt signal to jupyter-notebook")
notebookp.send_signal(signal.SIGINT)
try:
    log("Waiting for jupyter to exit...")
    notebookp.wait(10)
except subprocess.TimeoutExpired:
    log("control c timed out, killing")
    notebookp.kill()

log("Exited.")
sys.exit(result)
```

Ok! Now we're ready to take a look at the PyQt angle to all this. Basically, we need two classes: one to be the main window, and one to be the custom `QWebView` that takes care of rendering the page. Qt's `QWebView` is a little finicky and doesn't let anything create a new window (or close one) without you're (the programmer) express written consent.

Let's start with the `MainWindow`. Theoretically it's possible to do all of this without a `QMainWindow` subclass, but if you'd like to add menus and a status bar, then this makes things a little easier. The basic idea is that it's created with a target `homepage`, restores its geometry from `QSettings()`, then starts a single-shot timer to start loading once the event loop starts. The advantage of doing this is that it lets the window open quickly before something time-consuming (like loading a webpage) has to happen. Overriding the `closeEvent()` method lets us close all the other windows before closing that one. You could theoretically ask the user using `QMessageBox.information()` if they would really like to exit, then call `event.ignore()` to stop the user from exiting.

```Python
class MainWindow(QMainWindow):

    def __init__(self, parent=None, homepage=None):
        super(MainWindow, self).__init__(parent)
        self.homepage = homepage
        self.windows = []

        settings = QSettings()
        val = settings.value("net.fishandwhistle/JupyterQt/geometry", None)
        if val is not None:
            self.restoreGeometry(val)

        self.basewebview = CustomWebView(self, main=True)
        self.setCentralWidget(self.basewebview)
        QTimer.singleShot(0, self.initialload)

    @pyqtSlot()
    def initialload(self):
        if self.homepage:
            self.basewebview.load(QUrl(self.homepage))
        self.show()

    def closeEvent(self, event):
        if self.windows:
            for i in reversed(range(len(self.windows))):
                w = self.windows.pop(i)
                w.close()
            event.accept()
        else:
            event.accept()

        #save geometry
        settings = QSettings()
        settings.setValue("net.fishandwhistle/JupyterQt/geometry", self.saveGeometry())
```

We also have to define the `CustomWebView` used above. Qt requires we do this because it won't open a new window unless we override the `createWindow()` method. Similarly, it won't close unless we connect the `windowCloseRequested` signal with the `self.close` slot. **It is incredibly important that you disconnect any signal you connect unless both objects remain valid for the entire duration of the application**. Seriously, you get really weird things happening that are nearly impossible to debug if you forget to do this.

```Python
class CustomWebView(QWebView):

    def __init__(self, mainwindow, main=False):
        super(CustomWebView, self).__init__(None)
        self.parent = mainwindow
        self.main = main
        self.loadedPage = None
    
    @pyqtSlot(bool)
    def onpagechange(self, ok):
        log("on page change: %s, %s" % (self.url(), ok))
        if self.loadedPage is not None:
            log("disconnecting on close signal")
            self.loadedPage.windowCloseRequested.disconnect(self.close)
        self.loadedPage = self.page()
        log("connecting on close signal")
        self.loadedPage.windowCloseRequested.connect(self.close)

    def createWindow(self, windowtype):
        v = CustomWebView(self.parent)
        windows = self.parent.windows
        windows.append(v)
        v.show()
        return v

    def closeEvent(self, event):
        if self.loadedPage is not None:
            log("disconnecting on close signal")
            self.loadedPage.windowCloseRequested.disconnect(self.close)
        
        if not self.main:
            if self in self.parent.windows:
                self.parent.windows.remove(self)
            log("Window count: %s" % (len(self.parent.windows)+1))
        event.accept()
```

There you have it! You obviously have to put the UI elements above the start of the application, but other than that you'll have a fully functioning Jupyter frontend in no time. For a version with more bells and whistles, check out the [JupyterQt repository](https://github.com/paleolimbot/JupyterQt).

![](/wp-content/uploads/2016/01/jupyterqt_screenshot.png)
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>980</wp:post_id>
		<wp:post_date><![CDATA[2016-01-23 15:37:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-01-23 22:37:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[jupyterqt-concurrency-in-a-basic-pyqt-application]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="pyqt"><![CDATA[PyQt]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[Python]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>14076</wp:comment_id>
			<wp:comment_author><![CDATA[Matt Hall]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[matt@agilegeoscience.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.agilegeoscience.com/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[142.177.235.183]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-02-08 07:00:38]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-02-08 14:00:38]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This is very cunning, thanks for sharing!

Some people call <a href="https://github.com/yhat/rodeo" rel="nofollow">Rodeo</a> 'RStudio for Python'. I haven't used it myself, but it looks a bit nicer than Spyder and Canopy.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1454940038.96584606170654296875;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>14075</wp:comment_id>
			<wp:comment_author><![CDATA[Matt Hall]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[matt@agilegeoscience.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.agilegeoscience.com/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[142.177.235.183]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-02-08 07:00:07]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-02-08 14:00:07]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This is very cunning, thanks for sharing!

Some people call [Rodeo](https://github.com/yhat/rodeo) 'RStudio for Python'. I haven't used it myself, but it looks a bit nicer than Spyder and Canopy.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1454940007.5649158954620361328125;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>
				DFLite: The Quick &amp; Easy Python DataFrame		</title>
		<link>https://apps.fishandwhistle.net/archives/987</link>
		<pubDate>Thu, 10 Mar 2016 23:49:36 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=987</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
The data frame was a concept I first came across in R, where it is a fundemental component of data analysis. Never having done much data analysis in Python, I came across a situation where I needed a data frame but didn't know about the `pandas` implementation, so I went about writing my own `DataFrame` class. When I realized the `pandas` version existed, I immediately switched all my code over only to find that for my application, **the pandas DataFrame was over 2 times slower** than my lightweight `DataFrame`. So I spent some time making sure the interfaces were the same (at least in what I was doing) and made some nice 'this is what you would expect' modifications for personal use in the future.

In general, the `pandas` data frame performs quite well, especially with large datasets. I'm sure that this class has quite a few holes in it, but the idea of a lightweight `DataFrame` for Python is worth pursuing in the future. You can find the [source code for `dflite`](https://github.com/paleolimbot/dflite) on GitHub, including a copy of this notebook. Here's some general usage.

## Importing

Importing is easy, the only dependency is `numpy`.


```python
import dflite as df
```

## Creating a DataFrame

Usually all I want to do is create a `DataFrame` from a CSV file, but in code there's a couple of other ways to construct the class. The `DataFrame.from_records()` method is probably the most useful, creating a `DataFrame` from an iterable grouped by record (items coming out of a Postgres database via `psycopg2` are a good example). This is more or less equivalent to the `pandas` method of the same name. For now, we'll demo the class with the small CSV included in the directory.


```python
data = df.read_csv("test.csv")
data.head()
```




<table><tr><td></td><td><strong>Time (UTC)</strong></td><td><strong>Latitude</strong></td><td><strong>Longitude</strong></td></tr>
<tr><td><strong>0</strong></td><td>2016-03-02 17:50:18</td><td>45.10303743</td><td>-64.29103034</td></tr>
<tr><td><strong>1</strong></td><td>2016-03-02 17:50:19</td><td>45.10291441</td><td>-64.29095464</td></tr>
<tr><td><strong>2</strong></td><td>2016-03-02 17:50:20</td><td>45.10279595</td><td>-64.29089237</td></tr>
<tr><td><strong>3</strong></td><td>2016-03-02 17:50:21</td><td>45.1026838</td><td>-64.29084603</td></tr>
<tr><td><strong>4</strong></td><td>2016-03-02 17:50:22</td><td>45.10259138</td><td>-64.29080328</td></tr>
<tr><td><strong>5</strong></td><td>2016-03-02 17:50:23</td><td>45.10251977</td><td>-64.29080362</td></tr></table>



The `read_csv()` function is pretty much the same as the `pandas` version, at least for simple usage. You can also pass in a file-like object and a `driver=` parameter. Currently only `csv` files are supported, but they're the most common, so hey.

The `tail()` method works much the same as the `head()` method.


```python
data.tail(3)
```




<table><tr><td></td><td><strong>Time (UTC)</strong></td><td><strong>Latitude</strong></td><td><strong>Longitude</strong></td></tr>
<tr><td><strong>0</strong></td><td>2016-03-02 17:50:54</td><td>45.09937809</td><td>-64.29696471</td></tr>
<tr><td><strong>1</strong></td><td>2016-03-02 17:50:55</td><td>45.09924766</td><td>-64.2972626</td></tr>
<tr><td><strong>2</strong></td><td>2016-03-02 17:50:56</td><td>45.09911724</td><td>-64.29754859</td></tr></table>



Notice here how the indicies are 0, 1, and 2, where they *should* be the last few indicies of the `DataFrame`. The `pandas.DataFrame` supports (I would argue is slightly obsessed with) thd idea of **index**es for rows/columns. For rows I almost never have a reason to access them by anything other than an integer (certainly not a string), and because of this I don't bother with them. Since `tail()` is basically just checking the `DataFrame`, I didn't bother to re-number the rows (`tail()` is actually just shorthand for `data.iloc[(len(data)-nrows):len(data)])`, so it's really its own `DataFrame` object).

## Columns

Column names can be accessed and set just like the `pandas` version:


```python
data.columns
```




    ['Time (UTC)', 'Latitude', 'Longitude']




```python
data.columns = ("col1", "col2", "col3")
data.head()
```




<table><tr><td></td><td><strong>col1</strong></td><td><strong>col2</strong></td><td><strong>col3</strong></td></tr>
<tr><td><strong>0</strong></td><td>2016-03-02 17:50:18</td><td>45.10303743</td><td>-64.29103034</td></tr>
<tr><td><strong>1</strong></td><td>2016-03-02 17:50:19</td><td>45.10291441</td><td>-64.29095464</td></tr>
<tr><td><strong>2</strong></td><td>2016-03-02 17:50:20</td><td>45.10279595</td><td>-64.29089237</td></tr>
<tr><td><strong>3</strong></td><td>2016-03-02 17:50:21</td><td>45.1026838</td><td>-64.29084603</td></tr>
<tr><td><strong>4</strong></td><td>2016-03-02 17:50:22</td><td>45.10259138</td><td>-64.29080328</td></tr>
<tr><td><strong>5</strong></td><td>2016-03-02 17:50:23</td><td>45.10251977</td><td>-64.29080362</td></tr></table>



Column values can be added and removed in a similar way


```python
data["col1"]
```




    array(['2016-03-02 17:50:18', '2016-03-02 17:50:19', '2016-03-02 17:50:20',
           '2016-03-02 17:50:21', '2016-03-02 17:50:22', '2016-03-02 17:50:23',
           '2016-03-02 17:50:24', '2016-03-02 17:50:25', '2016-03-02 17:50:26',
           '2016-03-02 17:50:27', '2016-03-02 17:50:28', '2016-03-02 17:50:29',
           '2016-03-02 17:50:30', '2016-03-02 17:50:31', '2016-03-02 17:50:32',
           '2016-03-02 17:50:33', '2016-03-02 17:50:34', '2016-03-02 17:50:35',
           '2016-03-02 17:50:36', '2016-03-02 17:50:37', '2016-03-02 17:50:38',
           '2016-03-02 17:50:39', '2016-03-02 17:50:40', '2016-03-02 17:50:41',
           '2016-03-02 17:50:42', '2016-03-02 17:50:43', '2016-03-02 17:50:44',
           '2016-03-02 17:50:45', '2016-03-02 17:50:46', '2016-03-02 17:50:47',
           '2016-03-02 17:50:48', '2016-03-02 17:50:49', '2016-03-02 17:50:50',
           '2016-03-02 17:50:51', '2016-03-02 17:50:52', '2016-03-02 17:50:53',
           '2016-03-02 17:50:54', '2016-03-02 17:50:55', '2016-03-02 17:50:56'], 
          dtype='<U19')



Columns can also be accessed by index (this isn't possible in the `pandas` version, and I'm not quite sure why)


```python
data[0]
```




    array(['2016-03-02 17:50:18', '2016-03-02 17:50:19', '2016-03-02 17:50:20',
           '2016-03-02 17:50:21', '2016-03-02 17:50:22', '2016-03-02 17:50:23',
           '2016-03-02 17:50:24', '2016-03-02 17:50:25', '2016-03-02 17:50:26',
           '2016-03-02 17:50:27', '2016-03-02 17:50:28', '2016-03-02 17:50:29',
           '2016-03-02 17:50:30', '2016-03-02 17:50:31', '2016-03-02 17:50:32',
           '2016-03-02 17:50:33', '2016-03-02 17:50:34', '2016-03-02 17:50:35',
           '2016-03-02 17:50:36', '2016-03-02 17:50:37', '2016-03-02 17:50:38',
           '2016-03-02 17:50:39', '2016-03-02 17:50:40', '2016-03-02 17:50:41',
           '2016-03-02 17:50:42', '2016-03-02 17:50:43', '2016-03-02 17:50:44',
           '2016-03-02 17:50:45', '2016-03-02 17:50:46', '2016-03-02 17:50:47',
           '2016-03-02 17:50:48', '2016-03-02 17:50:49', '2016-03-02 17:50:50',
           '2016-03-02 17:50:51', '2016-03-02 17:50:52', '2016-03-02 17:50:53',
           '2016-03-02 17:50:54', '2016-03-02 17:50:55', '2016-03-02 17:50:56'], 
          dtype='<U19')




```python
data["newcol"] = 10
data.head()
```




<table><tr><td></td><td><strong>col1</strong></td><td><strong>col2</strong></td><td><strong>col3</strong></td><td><strong>newcol</strong></td></tr>
<tr><td><strong>0</strong></td><td>2016-03-02 17:50:18</td><td>45.10303743</td><td>-64.29103034</td><td>10</td></tr>
<tr><td><strong>1</strong></td><td>2016-03-02 17:50:19</td><td>45.10291441</td><td>-64.29095464</td><td>10</td></tr>
<tr><td><strong>2</strong></td><td>2016-03-02 17:50:20</td><td>45.10279595</td><td>-64.29089237</td><td>10</td></tr>
<tr><td><strong>3</strong></td><td>2016-03-02 17:50:21</td><td>45.1026838</td><td>-64.29084603</td><td>10</td></tr>
<tr><td><strong>4</strong></td><td>2016-03-02 17:50:22</td><td>45.10259138</td><td>-64.29080328</td><td>10</td></tr>
<tr><td><strong>5</strong></td><td>2016-03-02 17:50:23</td><td>45.10251977</td><td>-64.29080362</td><td>10</td></tr></table>




```python
data["newcol"] = data["newcol"] + 4
data.head()
```




<table><tr><td></td><td><strong>col1</strong></td><td><strong>col2</strong></td><td><strong>col3</strong></td><td><strong>newcol</strong></td></tr>
<tr><td><strong>0</strong></td><td>2016-03-02 17:50:18</td><td>45.10303743</td><td>-64.29103034</td><td>14</td></tr>
<tr><td><strong>1</strong></td><td>2016-03-02 17:50:19</td><td>45.10291441</td><td>-64.29095464</td><td>14</td></tr>
<tr><td><strong>2</strong></td><td>2016-03-02 17:50:20</td><td>45.10279595</td><td>-64.29089237</td><td>14</td></tr>
<tr><td><strong>3</strong></td><td>2016-03-02 17:50:21</td><td>45.1026838</td><td>-64.29084603</td><td>14</td></tr>
<tr><td><strong>4</strong></td><td>2016-03-02 17:50:22</td><td>45.10259138</td><td>-64.29080328</td><td>14</td></tr>
<tr><td><strong>5</strong></td><td>2016-03-02 17:50:23</td><td>45.10251977</td><td>-64.29080362</td><td>14</td></tr></table>




```python
del data["newcol"]
data.head()
```




<table><tr><td></td><td><strong>col1</strong></td><td><strong>col2</strong></td><td><strong>col3</strong></td></tr>
<tr><td><strong>0</strong></td><td>2016-03-02 17:50:18</td><td>45.10303743</td><td>-64.29103034</td></tr>
<tr><td><strong>1</strong></td><td>2016-03-02 17:50:19</td><td>45.10291441</td><td>-64.29095464</td></tr>
<tr><td><strong>2</strong></td><td>2016-03-02 17:50:20</td><td>45.10279595</td><td>-64.29089237</td></tr>
<tr><td><strong>3</strong></td><td>2016-03-02 17:50:21</td><td>45.1026838</td><td>-64.29084603</td></tr>
<tr><td><strong>4</strong></td><td>2016-03-02 17:50:22</td><td>45.10259138</td><td>-64.29080328</td></tr>
<tr><td><strong>5</strong></td><td>2016-03-02 17:50:23</td><td>45.10251977</td><td>-64.29080362</td></tr></table>



## Rows

In `pandas`, rows are accessed through the `iloc` attribute, so after considerable changing of code, so does mine. Here, `data.iloc[3]` will give the fourth row (as a `dict` ish object), and `data.iloc[3, :]` will give a `DataFrame` with only one row. The `pandas` version also has a `loc[]` option where names can be specified, but in this implementation `iloc` and `loc` are identical, and so you can pass more or less anything between the brackets and get a sensible result. 


```python
row = data.iloc[3]
row
```




<table><tr><td><strong>col1</strong></td><td><strong>col2</strong></td><td><strong>col3</strong></td></tr><tr><td>2016-03-02 17:50:21</td><td>45.1026838</td><td>-64.29084603</td></tr></table>



Of course, I've made the nice `_repr_html_()` method so it displas nicely but each row is actually a `_DFRow` object, which is a subclass of `dict` that keeps its values in order. This means you can index it by column name or by index.


```python
row["col1"]
```




    '2016-03-02 17:50:21'




```python
row[0]
```




    '2016-03-02 17:50:21'



Iterating through rows is done using the `itertuples()` method, which returns an iterator that iterates through the rows in the same way as the `pandas` version. Because `pandas` returns its row with the `0`th item as the row number (or row *index*, if you believe in that kind of thing), this method does as well.


```python
for row in data.head().itertuples():
    print(row[0], row["col2"], row[1])
```

    0 45.10303743 2016-03-02 17:50:18
    1 45.10291441 2016-03-02 17:50:19
    2 45.10279595 2016-03-02 17:50:20
    3 45.1026838 2016-03-02 17:50:21
    4 45.10259138 2016-03-02 17:50:22
    5 45.10251977 2016-03-02 17:50:23


## Subsetting

Each column is a NumPy `ndarray` object, so it can be indexed like any other `ndarray` object (i.e. by a `list` of desired rows, by an `ndarray` of logicals, by a single index, or by a `slice`). Some of this notation is available in the `iloc` method as well, which returns a single value (if two `int`s are passed), a `_DFRow` (if only a single integer is passed), or a subsetted `DataFrame` (if some combination of slices/ints/lists is passed). See the following examples:


```python
data.iloc[1:3]
```




<table><tr><td></td><td><strong>col1</strong></td><td><strong>col2</strong></td><td><strong>col3</strong></td></tr>
<tr><td><strong>0</strong></td><td>2016-03-02 17:50:19</td><td>45.10291441</td><td>-64.29095464</td></tr>
<tr><td><strong>1</strong></td><td>2016-03-02 17:50:20</td><td>45.10279595</td><td>-64.29089237</td></tr></table>




```python
data.iloc[1:5, 0:2]
```




<table><tr><td></td><td><strong>col1</strong></td><td><strong>col2</strong></td></tr>
<tr><td><strong>0</strong></td><td>2016-03-02 17:50:19</td><td>45.10291441</td></tr>
<tr><td><strong>1</strong></td><td>2016-03-02 17:50:20</td><td>45.10279595</td></tr>
<tr><td><strong>2</strong></td><td>2016-03-02 17:50:21</td><td>45.1026838</td></tr>
<tr><td><strong>3</strong></td><td>2016-03-02 17:50:22</td><td>45.10259138</td></tr></table>




```python
data.iloc[[2, 5, 5, 33], ("col1", "col3")]
```




<table><tr><td></td><td><strong>col1</strong></td><td><strong>col3</strong></td></tr>
<tr><td><strong>0</strong></td><td>2016-03-02 17:50:20</td><td>-64.29089237</td></tr>
<tr><td><strong>1</strong></td><td>2016-03-02 17:50:23</td><td>-64.29080362</td></tr>
<tr><td><strong>2</strong></td><td>2016-03-02 17:50:23</td><td>-64.29080362</td></tr>
<tr><td><strong>3</strong></td><td>2016-03-02 17:50:51</td><td>-64.29613321</td></tr></table>



Notice again how our original row number aren't preserved. You can work around this by making a column with your original row numbers. I get how this could be annoying, but including it was too complicated and wasn't necessary for what I was doing.


```python
data["original_rows"] = list(range(len(data)))
data.iloc[[2, 5, 5, 33], ("original_rows", "col1", "col3")]
```




<table><tr><td></td><td><strong>original_rows</strong></td><td><strong>col1</strong></td><td><strong>col3</strong></td></tr>
<tr><td><strong>0</strong></td><td>2</td><td>2016-03-02 17:50:20</td><td>-64.29089237</td></tr>
<tr><td><strong>1</strong></td><td>5</td><td>2016-03-02 17:50:23</td><td>-64.29080362</td></tr>
<tr><td><strong>2</strong></td><td>5</td><td>2016-03-02 17:50:23</td><td>-64.29080362</td></tr>
<tr><td><strong>3</strong></td><td>33</td><td>2016-03-02 17:50:51</td><td>-64.29613321</td></tr></table>



All of the nice indexing things we can do with NumPy are also available in the 'rows' part of the index:


```python
data.iloc[data["col2"] > 45.1022]
```




<table><tr><td></td><td><strong>col1</strong></td><td><strong>col2</strong></td><td><strong>col3</strong></td><td><strong>original_rows</strong></td></tr>
<tr><td><strong>0</strong></td><td>2016-03-02 17:50:18</td><td>45.10303743</td><td>-64.29103034</td><td>0</td></tr>
<tr><td><strong>1</strong></td><td>2016-03-02 17:50:19</td><td>45.10291441</td><td>-64.29095464</td><td>1</td></tr>
<tr><td><strong>2</strong></td><td>2016-03-02 17:50:20</td><td>45.10279595</td><td>-64.29089237</td><td>2</td></tr>
<tr><td><strong>3</strong></td><td>2016-03-02 17:50:21</td><td>45.1026838</td><td>-64.29084603</td><td>3</td></tr>
<tr><td><strong>4</strong></td><td>2016-03-02 17:50:22</td><td>45.10259138</td><td>-64.29080328</td><td>4</td></tr>
<tr><td><strong>5</strong></td><td>2016-03-02 17:50:23</td><td>45.10251977</td><td>-64.29080362</td><td>5</td></tr>
<tr><td><strong>6</strong></td><td>2016-03-02 17:50:24</td><td>45.10245523</td><td>-64.29083152</td><td>6</td></tr>
<tr><td><strong>7</strong></td><td>2016-03-02 17:50:25</td><td>45.10240112</td><td>-64.29086638</td><td>7</td></tr>
<tr><td><strong>8</strong></td><td>2016-03-02 17:50:26</td><td>45.10233343</td><td>-64.2909347</td><td>8</td></tr>
<tr><td><strong>9</strong></td><td>2016-03-02 17:50:27</td><td>45.10227411</td><td>-64.29102036</td><td>9</td></tr>
<tr><td><strong>10</strong></td><td>2016-03-02 17:50:28</td><td>45.1022154</td><td>-64.29111654</td><td>10</td></tr></table>



## Exporting

Writing the `DataFrame` to a CSV is probably the easiest way to export, although TSV is also supported. The `to_csv()` method works more or less like the `pandas` version, and can take a file-like object as well as a filename.

## Performance

As I mentioned earlier, running the `pandas.DataFrame` in production code that used quite a lot of `DataFrame`s was quite slow. I have a feeling that there's a lot of overhead involved with the convenience of multiple indexing and built-in plotting support that slows the class down when there isn't a need for it. There's also probably a lot of work to be done on this class that can add convenience without comprimising performance, but I'll leave that up to some folks with a bit more spare time than I do. Cheers!
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>987</wp:post_id>
		<wp:post_date><![CDATA[2016-03-10 19:49:36]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-03-10 23:49:36]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[dflite-the-quick-easy-python-dataframe]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="dflite"><![CDATA[dflite]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[Python]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				qosm: Open Street Map tiles in QGIS		</title>
		<link>https://apps.fishandwhistle.net/archives/1002</link>
		<pubDate>Fri, 11 Mar 2016 20:56:45 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1002</guid>
		<description></description>
		<content:encoded>
				<![CDATA[There was once a time in QGIS where getting Open Street Map basemaps was tricky. ArcGIS long since solved this problems with ready-made basemaps available with a few clicks, but in QGIS there was the <a href="https://plugins.qgis.org/plugins/openlayers_plugin/">OpenLayers plugin</a>, which was a rough go to use. In the first GIS short course I taught, this was the only option, so over Christmas break I decided to write something better. The result is <a href="https://plugins.qgis.org/plugins/qosm/">QOSM</a>, although since writing it I've discovered that the <a href="https://plugins.qgis.org/plugins/quick_map_services/">QuickMapServices plugin</a>, while much less searchable, is much more effective. Still the person in charge of the QGIS plugin repository let in the plugin saying it would help inspire improvement. 

[caption id="attachment_1003" align="none" width="491"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/03/Screenshot-from-2016-03-11-16-38-11.png" alt="Adding a QOSM Layer" width="491" height="322" class="size-full wp-image-1003" /> Adding a QOSM Layer[/caption]

Still, I think my plugin does a few things better than QMS, but QMS really does everything I'd ever wanted a basemap plugin to do. The main difference is the cacheing - QOSM is designed to store tiles indefinitely and store a lot of them (it even has an option to cache everything for a whole area), whereas QMS is designed to download tiles on the fly (although you can change the global cacheing expiry time in QGIS main preferences to get QMS to cache things for longer).

[caption id="attachment_1005" align="none" width="336"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/03/Screenshot-from-2016-03-11-16-51-09.png" alt="QOSM will happily download any number of tiles for a given area." width="336" height="247" class="size-full wp-image-1005" /> QOSM will happily download any number of tiles for a given area.[/caption]

The main benefit of writing QOSM was to get some insight into the <a href="http://docs.qgis.org/testing/en/docs/pyqgis_developer_cookbook/plugins.html">QGIS plugin structure</a>, which is quite powerful, if poorly documented. Plugins are written in Python (2), and allow easy creating of user interfaces using PyQt bindings for the Qt framework. The Python Console plugin allows users to easily run scripts that reference the QGIS Python interface, but plugins allow users to take control of the canavas, projects, loading layers, and even calling processing functions within the main user interface, and are distributed by the QGIS plugin repository. A <a href="http://docs.qgis.org/testing/en/docs/pyqgis_developer_cookbook/">manual of QGIS Python functionality</a> is available on the QGIS website, as well as the <a href="http://qgis.org/api/classQgisInterface.html">API reference</a> (the API reference is for the C++ classes, but calling these in Python works exactly the same in most cases).

So there you have it, it's a little anti-climatic, but if you need to load a hillshade layer or cache an entire province worth of tiles or aerial imagery, QOSM is what you're looking for.

[caption id="attachment_1004" align="none" width="600"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/03/Screenshot-from-2016-03-11-16-40-18-1024x627.png" alt="A QOSM Tile Layer in QGIS" width="600" height="367" class="size-large wp-image-1004" /> A QOSM Tile Layer in QGIS[/caption]

[caption id="attachment_1006" align="none" width="605"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/03/Screenshot-from-2016-03-11-16-43-43.png" alt="The QOSM Settings dialog." width="605" height="405" class="size-full wp-image-1006" /> The QOSM Settings dialog.[/caption]

]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1002</wp:post_id>
		<wp:post_date><![CDATA[2016-03-11 16:56:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-03-11 20:56:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[qosm-open-street-map-tiles-in-qgis]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="gis"><![CDATA[gis]]></category>
		<category domain="post_tag" nicename="pyqt"><![CDATA[PyQt]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[Python]]></category>
		<category domain="post_tag" nicename="qosm"><![CDATA[qosm]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>14198</wp:comment_id>
			<wp:comment_author><![CDATA[Noureddin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[nour72sy@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[46.58.224.130]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-04-03 05:20:16]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-04-03 08:20:16]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thank you for this plugin.
But I'm having trouble when using it.
every time I zoom in, I receive crash dumped error: minidump written to C
Sorry I'm a beginner to QGIS, would you please advise how to solve this issue?
Many thanks for your great efforts.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>14200</wp:comment_id>
			<wp:comment_author><![CDATA[dewey]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[apps@fishandwhistle.net]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[192.252.136.159]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-04-03 16:37:20]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-04-03 19:37:20]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Noureddin,

You should be using the [Quick Map Services](https://plugins.qgis.org/plugins/quick_map_services/) plugin! It's more stable and definitely more user friendly for beginners.

Cheers!

-dewey]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>14198</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
							</wp:comment>
					</item>
					<item>
		<title>
				The missing list comprehensions in R		</title>
		<link>https://apps.fishandwhistle.net/archives/1010</link>
		<pubDate>Sun, 13 Mar 2016 23:48:08 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1010</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
The first problem I had with making the move to R from Python was the lack of the **list comprehension**, which in python lets you easily create new vectors (well, lists, because it's Python) by evaluating a function. In R this is mostly taken care of because all the base functions (and most package functions) are vectorized, meaning they automatically operate on every item you pass into it and return a vector of the same (usually) length. Usually some combination of the `ifelse` function and vectorized operations gives the desired result, but sometimes it's not enough. Consider the following dataset:

``` r
cormatrix
```

    ##           As        Pb        Zn          V       Cr
    ## S         NA        NA        NA         NA       NA
    ## Cl 0.5978618        NA 0.5814264         NA       NA
    ## Ti        NA 0.7568323        NA         NA 0.815492
    ## Mn        NA        NA        NA         NA       NA
    ## Fe        NA        NA 0.5629760 -0.5589387       NA
    ## Co        NA        NA        NA         NA       NA
    ## As 1.0000000        NA 0.5897606         NA       NA
    ## Mo        NA        NA        NA         NA       NA
    ## Ag        NA        NA        NA  0.7963853       NA
    ## Sn        NA        NA        NA         NA       NA

Here we have a number of correlation coeficients between various metals, with the non-significant ones as `NA`. There's a few rows here that have all `NA` values that are unnecessary, and it would be a more useful table if we could remove them. In R language, we want to remove the rows where `all(is.na(cormatrix[i,]))` returns `TRUE` for all values of `i` in `1:nrow(cormatrix)`. In Python, the syntax for this would be something like this:

``` python
[all(isnan(row)) for row in cormatrix]
```

I run into this problem in R often, where I want to produce a vector but the input needs to be specified using indicies for some reason (usually because I'm trying to figure out something about multiple columns at a time). The base package contains the `apply()` function, as well as slightly more flexible approaches in the `foreach` and `plyr` packages. The syntax for all of these is a little clunky and I can never seem to remember it.

``` r
# from the foreach package
foreach(i=1:nrow(cormatrix), .combine=c) %do% all(is.na(cormatrix[i,]))
```

    ##  [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE

``` r
# from the plyr package
aaply(cormatrix, .margins=1, .expand = FALSE, .fun=function(row) all(is.na(row)))
```

    ##     1     2     3     4     5     6     7     8     9    10 
    ##  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE

``` r
# from the base package
apply(cormatrix, MARGIN=1, FUN=function(row) all(is.na(row)))
```

    ##     S    Cl    Ti    Mn    Fe    Co    As    Mo    Ag    Sn 
    ##  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE

I haven't played with the `Vectorize` function in R much, but there's a solution here as well.

``` r
f <- Vectorize(function(i) all(is.na(cormatrix[i,])))
f(1:nrow(cormatrix))
```

    ##  [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE

The general case here is a function such that typing something like `lc(all(is.na(cormatrix[i,])), i=1:nrow(cormatrix))` will produce the output vector we're looking for. It turns out that writing this function for the general case is a little tricky since we have to not evaluate the first argument until we've assigned the `i` value properly. Luckily, some R magic and the `foreach` package make this quite flexible.

``` r
lc <- function(expr, ...) {
  expr <- deparse(substitute(expr))
  foreach(..., .combine=c) %do% eval(parse(text=expr))
}

lc(all(is.na(cormatrix[i,])), i=1:nrow(cormatrix))
```

    ##  [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE

Performance wise, there's a good chance this is super slow. But the syntax is so neat that it's hard to resist using it in cases where one line of code would solve a complex subsetting operation like this one.

``` r
cormatrix[lc(!all(is.na(cormatrix[i,])), i=1:nrow(cormatrix)),]
```

    ##           As        Pb        Zn          V       Cr
    ## Cl 0.5978618        NA 0.5814264         NA       NA
    ## Ti        NA 0.7568323        NA         NA 0.815492
    ## Fe        NA        NA 0.5629760 -0.5589387       NA
    ## As 1.0000000        NA 0.5897606         NA       NA
    ## Ag        NA        NA        NA  0.7963853       NA

There are also solutions in the `plyr` and `base` package versions, but the `foreach` method is by far the most flexible since it doesn't depend on iterating over rows in a `data.frame` or `matrix`, and the `...` means we can pass in just about any iterable argument. For ease of copy-and-pasting, here's the general-use case (that doesn't require `library`ing `foreach`)

``` r
lc <- function(expr, ...) {
  `%do%` <- foreach::`%do%`
  expr <- deparse(substitute(expr))
  foreach::foreach(..., .combine=c) %do% eval(parse(text=expr))
}

lc(all(is.na(cormatrix[i,])), i=1:nrow(cormatrix))
```

    ##  [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1010</wp:post_id>
		<wp:post_date><![CDATA[2016-03-13 20:48:08]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-03-13 23:48:08]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[the-missing-list-comprehensions-in-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Mapping GPS Data to Open Street Map Roads		</title>
		<link>https://apps.fishandwhistle.net/archives/1076</link>
		<pubDate>Tue, 29 Mar 2016 18:24:27 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1076</guid>
		<description></description>
		<content:encoded>
				<![CDATA[The problem of matching GPS points collected from a phone or dedicated GPS device to actual defined roadways is becoming more prevalent as insurance companies move to sensor-based platforms for risk management. In a recent Upwork contract, I was tasked with this problem for an insurance company providing sensor-based risk management for insurance providers in China. Based on a <a href="http://research.microsoft.com/en-us/um/people/jckrumm/Publications%202009/map%20matching%20ACM%20GIS%20camera%20ready.pdf">research paper produced by programmers at Microsoft</a>, we used a Hidden Markov Model-like structure to assess the most likely path along road segments. The simple concept of emission probabilities based on distance from the segment and transition probabilities based on a comparison of driving distance and great-circle distance is easy in principle but difficult in practice given noisy GPS data and inconsistent road network data. After several weeks of full-time equivalent work, the project is now mostly completed with only a few outstanding issues. Overall the experience of working with a trans-continental team of programmers though Upwork was positive, although it has reminded me how much I value somebody sitting down in my office explaining what it is they're trying to accomplish.

[caption id="attachment_1077" align="none" width="600"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/03/Screen-Shot-2016-03-18-at-1.38.05-PM-1024x788.png" alt="A sample map matching result for a GPS track near Wolfville, Nova Scotia." width="600" height="462" class="size-large wp-image-1077" /> A sample map matching result for a GPS track near Wolfville, Nova Scotia.[/caption]

[caption id="attachment_1078" align="none" width="316"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/03/Screen-Shot-2016-03-29-at-2.22.58-PM.png" alt="The three distances forming the basis for emission and transition probabilities: great circle distance, distance to the segment, and driving distance." width="316" height="317" class="size-full wp-image-1078" /> The three distances forming the basis for emission and transition probabilities: great circle distance, distance to the segment, and driving distance.[/caption]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1076</wp:post_id>
		<wp:post_date><![CDATA[2016-03-29 15:24:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-03-29 18:24:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[mapping-gps-data-to-open-street-map-roads]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="general"><![CDATA[General]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Prairie Coordinates release 0.4.2		</title>
		<link>https://apps.fishandwhistle.net/archives/1081</link>
		<pubDate>Mon, 04 Apr 2016 16:39:43 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1081</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
After an email complaint about the app crashing when the "search by GPS" screen was loaded, I looked into some of the changes that were introduced with the Android 6.0 platform. The issue was a `SecurityException` that was thrown because in Android 6.0, certain permissions require the app to explicitly ask the user, including the GPS permission. This move appears to be in line with the iOS devices, although it would be nice to have some warning from Google when issues like this break backwards compatibility with existing apps since apps are almost all ratings-driven. Luckily my users continue to be great and keep telling me about problems instead of complaining about them!

Check out the new release on the [Google Play Store](https://play.google.com/store/apps/details?id=ca.fwe.pcoordplus)
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1081</wp:post_id>
		<wp:post_date><![CDATA[2016-04-04 13:39:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-04-04 16:39:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[prairie-coordinates-release-0-4-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="prairie-coordinates"><![CDATA[prairie coordinates]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Mapping R Package Updates		</title>
		<link>https://apps.fishandwhistle.net/archives/1112</link>
		<pubDate>Tue, 12 Apr 2016 21:15:14 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1112</guid>
		<description></description>
		<content:encoded>
				<![CDATA[The past few months haven't been particularly productive on the R package front (I've mostly been using R for plotting lakes data), but small updates here and there have led to a bunch of exciting new features in the <a href="http://paleolimbot.github.io/prettymapr">prettymapr</a>, <a href="http://paleolimbot.github.io/rosm">rosm</a>, and <a href="http://paleolimbot.github.io/rcanvec">rcanvec</a> packages. Things like <a href="https://github.com/paleolimbot/prettymapr#geocoding-tools">geocoding</a>, <a href="https://github.com/paleolimbot/prettymapr#bounding-box-tools">finding a bounding box containing multiple queries</a>, <a href="https://github.com/paleolimbot/rosm/issues/1">exporting OSM maps to raster file formats</a>, <a href="https://github.com/paleolimbot/rcanvec#exporting-canvec-data">exporting CanVec data for multiple NTS sheets</a> have made a much smoother user experience using the three packages for basemap data. In particular, I'm using the rcanvec export function to make a bunch of basemaps for publications at the moment, which was part of the stimulus for getting this done. In the meantime, GeoGratis did make using CanVec data significantly easier recently (they started properly naming their files), so their <a href="http://geogratis.gc.ca/site/eng/extraction">data extraction tool</a> is worth a look.

Also with this series of updates came updates to the documentation for <a href="http://paleolimbot.github.io/prettymapr">prettymapr</a>, <a href="http://paleolimbot.github.io/rosm">rosm</a>, and <a href="http://paleolimbot.github.io/rcanvec">rcanvec</a>, as well as an update to the <a href="http://apps.fishandwhistle.net/archives/924">Basemaps, eh? tutorial</a>. Enjoy!]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1112</wp:post_id>
		<wp:post_date><![CDATA[2016-04-12 18:15:14]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-04-12 21:15:14]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[mapping-r-package-updates]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="prettymapr"><![CDATA[prettymapr]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="rcanvec"><![CDATA[rcanvec]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
		<category domain="post_tag" nicename="rosm"><![CDATA[rosm]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Raspberry Pi Pure Python Infrared Remote Control		</title>
		<link>https://apps.fishandwhistle.net/archives/1115</link>
		<pubDate>Thu, 07 Jul 2016 20:56:39 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1115</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
Inspired by the ability to create scientific tech gadgets, I have two new toys on my desk these days: a Raspberry Pi and an Arduino. The simplicity of the Arduino is quite nice, but the ability to code the Raspberry Pi in Python (not to mention built-in Wi-Fi capability) drew me to do some experimenting with the infrared sensor. The problem I ran into almost immediately is that the out-of-the box solution to infrared remotes is lirc, which requires compilation and dedicating a pin specifically for the IR sensor for all time. I thought there must be a pure Python solution on the internets somewhere, but it appears nobody has tackled this one until now.

## The wiring setup

The wiring setup for this project used the [Sunfounder Raspberry Pi Sensor Kit](https://www.sunfounder.com/sensor-kit-v2-0-for-raspberry-pi-2-and-raspberry-pi-model-b.html), which has a few resistors built in. Based on some other posts involving an IR sensor and an Arduino, I think most IR sensors will function in a similar way. The IR Sensor has 3 pins: +3.3V, Signal, and Ground. The +3.3V pin gets attached to the +3.3V pin on the GPIO header, the Ground gets attached to one of the ground pins, and the Signal pin gets attached to one of the GPIO pins (in this example I use GPIO 18, or pin 11 on the header). The picture here is a bit fuzzy but my setup looked like this:

![the sensor](http://apps.fishandwhistle.net/wp-content/uploads/2016/07/Screen-Shot-2016-07-07-at-2.43.09-PM.png)

![the setup](http://apps.fishandwhistle.net/wp-content/uploads/2016/07/20160705_175502-e1467917497216-1024x768.jpg)

## How IR Remotes Work

If you're not going to use the out-of-the-box solution (lirc), you're going to need to delve into the world of raw IR data transfer. Between an [Arduino-related post](https://learn.adafruit.com/ir-sensor/ir-remote-signals) and [this YouTube Video](https://www.youtube.com/watch?v=aoPSX3wYSXo), it appears that IR remotes are basically using Morse code to transfer information between the remote and the receiver, with 0 indicated by a short pulse, and 1 indicated by a long pulse. How long are these pulses? It appears they are between 0.5 ms and 2 ms based on the [adafruit article](https://learn.adafruit.com/ir-sensor/ir-remote-signals), but is Python fast enough to measure this? It's time to find out:

```Python
import RPi.GPIO as GPIO
from time import time

# Numbers GPIOs by physical location
GPIO.setmode(GPIO.BOARD)
# set pin 11 as an input pin with default as LOW v
GPIO.setup(11, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)

# define a function to acquire data
def binary_acquire(pin, duration):
    # acquires data as quickly as possible
    t0 = time() # time is in seconds here
    results = []
    while (time() - t0) < duration:
        results.append(GPIO.input(pin))
    return results

print("Acquiring data for 1 second")
# acquire data for 1 second
results = binary_acquire(11, 1.0)
print("Done!")
print(",".join([str(result) for result in results])
GPIO.cleanup()
```

This bit of code will record as many possible values as it can from the signal output of the IR sensor, and print the results. These will be totally illegible, but if you put them into R, they look something like this (assuming you pressed a button on your remote during the 1 second interval you ran the script).

![sample IR data](http://apps.fishandwhistle.net/wp-content/uploads/2016/07/ir_data.png)

Lo and behold, we get a whole lot of long and short bursts, just as the [YouTube Video](https://www.youtube.com/watch?v=aoPSX3wYSXo) predicted. My Pi is able to acquire (and store to memory) around 160,000 values in a second, which isn't excellent but seems to do the trick for IR remotes anyway. If we zoom in to the section where the shorts and longs are, we can see the short/long difference a little more clearly.

![zoomed in IR dat](http://apps.fishandwhistle.net/wp-content/uploads/2016/07/Screen-Shot-2016-07-07-at-4.18.54-PM.png)

So how long are these short/long pulses? My X-axis here is marked in samples, but since this may vary from Pi to Pi, it's probably better to convert these into times like in the [adafruit article](https://learn.adafruit.com/ir-sensor/ir-remote-signals). Instead of measuring the time of every sample, I'm going to use the overall sample rate (`len(results)/duration`) to convert run-lengths into durations.

```Python
rate = len(data) / 1.0  # because we acquired data for 1 second
pulses = []
i_break = 0
# detect run lengths using the acquisition rate to turn the times in to microseconds
for i in range(1, len(data)):
    if (data[i] != data[i-1]) or (i == len(data)-1):
        pulses.append((data[i-1], int((i-i_break)/rate*1e6)))
        i_break = i
```

If we add this to our existing code and examine the results, it looks a little messy but we get a whole lot of long and short "1" values and pretty consistent "0" values. Based on the [adafruit article](https://learn.adafruit.com/ir-sensor/ir-remote-signals) and some data I played around with, short pulses are somewhere around 0.5 ms, and long pulses are somewhere around 1.2 ms. The variation in short/long pulses is probably because some combination of linux and Python are busy doing other things like running the OS or garbage collection and didn't manage to read a few of the values that we would have liked. Either way, the short pulses are always less than 1 ms, and the long pulses are always greater than 1 ms, which we can use to translate our pulse durations into binary code.

```Python
# decode ( < 1 ms "1" pulse is a 1, > 1 ms "1" pulse is a 1, longer than 2 ms pulse is something else)
# does not decode channel, which may be a piece of the information after the long 1 pulse in the middle
outbin = ""
for val, us in pulses:
    if val != 1:
        continue
    if outbin and us > 2000:
        break
    elif us < 1000:
        outbin += "0"
    elif 1000 < us < 2000:
        outbin += "1"
print(outbin)
```

In this example I disregard any pulse greater than 2 ms (2000 microseconds) until some numbers have been read, and use the longer than 2 ms pulse after all the numbers to terminate reading the long/short values. This ensures all the long/short pulses are next to eachother. Put into practice, we can use a couple more [RPi.GPIO tricks](https://sourceforge.net/p/raspberry-gpio-python/wiki/Inputs/) to listen for a change in the signal (`wait_for_edge()`) before recording our values.

```Python
import RPi.GPIO as GPIO
from time import time

def setup():
    GPIO.setmode(GPIO.BOARD)  # Numbers GPIOs by physical location
    GPIO.setup(11, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)


def binary_aquire(pin, duration):
    # aquires data as quickly as possible
    t0 = time()
    results = []
    while (time() - t0) < duration:
        results.append(GPIO.input(pin))
    return results


def on_ir_receive(pinNo, bouncetime=150):
    # when edge detect is called (which requires less CPU than constant
    # data acquisition), we acquire data as quickly as possible
    data = binary_aquire(pinNo, bouncetime/1000.0)
    if len(data) < bouncetime:
        return
    rate = len(data) / (bouncetime / 1000.0)
    pulses = []
    i_break = 0
    # detect run lengths using the acquisition rate to turn the times in to microseconds
    for i in range(1, len(data)):
        if (data[i] != data[i-1]) or (i == len(data)-1):
            pulses.append((data[i-1], int((i-i_break)/rate*1e6)))
            i_break = i
    # decode ( < 1 ms "1" pulse is a 1, > 1 ms "1" pulse is a 1, longer than 2 ms pulse is something else)
    # does not decode channel, which may be a piece of the information after the long 1 pulse in the middle
    outbin = ""
    for val, us in pulses:
        if val != 1:
            continue
        if outbin and us > 2000:
            break
        elif us < 1000:
            outbin += "0"
        elif 1000 < us < 2000:
            outbin += "1"
    try:
        return int(outbin, 2)
    except ValueError:
        # probably an empty code
        return None


def destroy():
    GPIO.cleanup()


if __name__ == "__main__":
    setup()
    try:
        print("Starting IR Listener")
        while True:
            print("Waiting for signal")
            GPIO.wait_for_edge(11, GPIO.FALLING)
            code = on_ir_receive(11)
            if code:
                print(str(hex(code)))
            else:
                print("Invalid code")
    except KeyboardInterrupt:
        pass
    except RuntimeError:
        # this gets thrown when control C gets pressed
        # because wait_for_edge doesn't properly pass this on
        pass
    print("Quitting")
    destroy()
```

Note that here I use the terminology `bouncetime`, which I mean to be the amount of time for which we should record. Based on some experimenting, it looks like it's usually around 150 ms. And there you go! You should get something like:

```
Starting IR Listener
Waiting for signal
0xffa25d
Waiting for signal
0xff629d
Waiting for signal
0xffe21d
^CQuitting
```

From some experimenting with the remote from the [Sunfounder Kit](https://www.sunfounder.com/sensor-kit-v2-0-for-raspberry-pi-2-and-raspberry-pi-model-b.html), I built a dictionary of codes for the remote.

```Python
CODES = {
    0xffa25d: "ON/OFF",
    0xff629d: "MODE",
    0xffe21d: "MUTE",
    0xff22dd: "PLAY/PAUSE",
    0xff02fd: "PREVIOUS",
    0xffc23d: "NEXT",
    0xffe01f: "EQ",
    0xffa857: "MINUS",
    0xff906f: "PLUS",
    0xff6897: "0",
    0xff9867: "SHUFFLE",
    0xffb04f: "U/SD",
    0xff30cf: "1",
    0xff18e7: "2",
    0xff7a85: "3",
    0xff10ef: "4",
    0xff38c7: "5",
    0xff5aa5: "6",
    0xff42bd: "7",
    0xff4ab5: "8",
    0xff52ad: "9",
}
```

Probably the most useful implementation of this would be to put it in a `Thread` of some type and listen in the background since the `wait_for_edge()` function blocks until it something changes on the pin that it monitors. Now to build a remote-control coffee machine for those early mornings...
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1115</wp:post_id>
		<wp:post_date><![CDATA[2016-07-07 17:56:39]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-07-07 20:56:39]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[raspberry-pi-pure-python-infrared-remote-control]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="gpio"><![CDATA[GPIO]]></category>
		<category domain="post_tag" nicename="infrared-remote"><![CDATA[infrared remote]]></category>
		<category domain="post_tag" nicename="making"><![CDATA[making]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[Python]]></category>
		<category domain="post_tag" nicename="raspberry-pi"><![CDATA[Raspberry Pi]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Spatial data in ggplot2		</title>
		<link>https://apps.fishandwhistle.net/archives/1123</link>
		<pubDate>Tue, 12 Jul 2016 17:22:41 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1123</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
I recently figured out how to use the ggplot2 framework to plot raster data, which led me to write a package extending ggplot2 for more convenient spatial data geometries. Ever wanted a `geom_spatial()` for those `SpatialPolygonsDataFrame`s and `RasterBrick`s you've got kicking around? Well...maybe you did or didn't, but the [ggspatial](https://github.com/paleolimbot/ggspatial) package is now here for your spatial data/ggplot-ing pleasure. It needs some work (and feedback) before it becomes a CRAN release, but in the meantime it should do the trick for most things spatial. Unfortunately, there is a bug when using `coord_fixed()` and `scales="free"` with facetting in ggplot, so facetting with different spatial extents is out. Luckily, that's not particularly common (probably more common is to vary the aesthetic with the same spatial extent which still isn't quite there yet in this package), so give it a shot!

``` r
install.packages("devtools") # if devtools isn't installed
devtools::install_github("paleolimbot/ggspatial")
library(ggspatial)
data(longlake_waterdf, longlake_depthdf)
ggplot() + geom_spatial(longlake_waterdf[2,], fill="lightblue") +
  geom_spatial(longlake_depthdf, aes(col=DEPTH.M), size=2) + 
  scale_color_gradient(high="black", low="#56B1F7") + coord_fixed()
```
[/markdown]
[caption id="attachment_1124" align="alignleft" width="672"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/07/unnamed-chunk-4-1.png" alt="All the ggplot goodness with your spatial data" width="672" height="480" class="size-full wp-image-1124" /> All the ggplot goodness with your spatial data[/caption]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1123</wp:post_id>
		<wp:post_date><![CDATA[2016-07-12 14:22:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-07-12 17:22:41]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[spatial-data-in-ggplot2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ggspatial"><![CDATA[ggspatial]]></category>
		<category domain="post_tag" nicename="gis"><![CDATA[gis]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Holes in ggplot polygons		</title>
		<link>https://apps.fishandwhistle.net/archives/1126</link>
		<pubDate>Wed, 13 Jul 2016 23:35:46 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1126</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
As part of my efforts to construct an [R package connecting ggplot and Spatial objects](http://github.com/paleolimbot/ggspatial), I came across an issue with ggplot involving holes in polygons. According to the collective knowledge of StackOverflow, it's possible to make this happen by putting the coordinates in the [correct clockwise/counterclockwise order](http://stackoverflow.com/questions/12033560/use-ggplot-to-plot-polygon-with-holes-in-a-city-map), [extending geom_polygon()](http://stackoverflow.com/questions/28036162/plotting-islands-in-ggplot2) somehow, or [other really complicated things](http://rstudio-pubs-static.s3.amazonaws.com/58401_1e87f6fe73e14bafbe1f3ce3df85c99e.html). But for my simple example, things didn't work, and [appears not to work for many examples](http://gis.stackexchange.com/questions/130482/plotting-islands-in-ggplot2) where spatial data is involved.

```r 
devtools::install_github("paleolimbot/ggspatial")
library(ggspatial)
data("longlake_waterdf")

spdf <- longlake_waterdf[is.na(longlake_waterdf$label),]
ggplot(spdf) + geom_polygon()
```
[/markdown]
[caption id="attachment_1127" align="alignleft" width="598"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/07/Rplot.png" alt="it&#039;s all wrong!" width="598" height="424" class="size-full wp-image-1127" /> it's all wrong![/caption]
[markdown]
Of course, `ggplot` has to turn the `SpatialPolygonsDataFrame` into a `data.frame` somehow, and it turns out that this happens using the `fortify()` function.

```r
df <- fortify(spdf)
head(df)
```

```
      long     lat order  hole piece id group
1 412583.6 5086360     1 FALSE     1  2   2.1
2 412585.6 5086360     2 FALSE     1  2   2.1
3 412588.5 5086356     3 FALSE     1  2   2.1
4 412588.4 5086350     4 FALSE     1  2   2.1
5 412585.2 5086343     5 FALSE     1  2   2.1
6 412583.1 5086335     6 FALSE     1  2   2.1
```

So more specifically, our call to `ggplot` is really more like this:

```r
ggplot(df, aes(x=long, y=lat)) + 
    geom_polygon(aes(group=id), fill="lightblue") + 
    geom_path(aes(group=group))
```
[/markdown]
<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/07/Rplot01.png" alt="Rplot01" width="598" height="424" class="alignleft size-full wp-image-1128" />
[markdown]
Here we can see that the 'fill' is working improperly, but if we use `group=group` as an aesthetic, the `geom_path` of the outline is correct.

It turns out that the secret is actually a bit of a workaround, suggested by [this answer]() on StackOverFlow. He didn't get credit for an answer, but got plenty of upvotes. Basically, if you come back to the same point after every hole, you fix the fill problem. So the solution is, insert a the first point before each piece of the polygon. Finding the first line of every hole is easy if it's just one feature (which our example is), so we'll start there.

```r
ringstarts <- which(!duplicated(df$group))
df[ringstarts, ]
```

```
        long     lat order  hole piece id group
1   412583.6 5086360     1 FALSE     1  2   2.1
320 412375.3 5086133   320  TRUE     2  2   2.2
431 412076.0 5085796   431  TRUE     3  2   2.3
502 412477.7 5086260   502  TRUE     4  2   2.4
581 412317.0 5085906   581  TRUE     5  2   2.5
676 412310.1 5086123   676  TRUE     6  2   2.6
714 412234.5 5085986   714  TRUE     7  2   2.7
```

Now if we manually insert the first row in front of each of the rings, we can see that our fill plots properly.

```r
df2 <- df[c(1:319, 
          1, 320:430, 1, 431:501, 1, 502:580, 
          1, 581:675, 1, 676:713, 1, 714:nrow(df)),]
ggplot(df, aes(x=long, y=lat)) + 
    geom_polygon(aes(group=id), fill="lightblue") + 
    geom_path(aes(group=group))
```
[/markdown]
<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/07/Rplot02.png" alt="Rplot02" width="598" height="424" class="alignleft size-full wp-image-1129" />
[markdown]
Programmatically coming up with this vector of row numbers took quite a bit of experimentation, but with a combination of `c()`, `lapply()`, and `do.call()` it looks like this does the trick:

```r
fixfeature <- function(df) {
  ringstarts <- which(!duplicated(df$group))
  if(length(ringstarts) < 2) {
    return(df)
  } else {
    ringstarts <- c(ringstarts, nrow(df))
    indicies <- c(1:(ringstarts[2]-1), do.call(c, lapply(2:(length(ringstarts)-1), function(x) {
        c(1, ringstarts[x]:(ringstarts[x+1]-1))
    })), nrow(df))
    return(df[indicies,])
  }
}
```

Because this only works with a single feature, we need to invoke `dplyr` to split our original `data.frame` up and apply the `fix_feature` function.

```r
library(dplyr)
custom_fortify <- function(x, ...) {
  df <- fortify(x, ...)
  df %>% group_by(id) %>% do(fixfeature(.))
}
```
This appears to work on any multi-part geometry, whether it involves a hole or not. The classic `wrld_simpl` dataset (from the `maptools` package) looks particularly bad when plotted without any type of conversion.

```r
library(maptools)
data(wrld_simpl)
wrld_df <- fortify(wrld_simpl)
ggplot(wrld_df, aes(x=long, y=lat)) + geom_polygon()
```
[/markdown]
<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/07/Rplot03.png" alt="Rplot03" width="598" height="424" class="alignleft size-full wp-image-1130" />
[markdown]
But if we use our `custom_fortify()` function, things look much prettier.

```r
wrld_df <- custom_fortify(wrld_simpl)
ggplot(wrld_df, aes(x=long, y=lat, group=id)) + geom_polygon()
```
[/markdown]
<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/07/Rplot04.png" alt="Rplot04" width="598" height="424" class="alignleft size-full wp-image-1131" />
[markdown]

Or if we want to add outlines, we have to use a slightly different aesthetic but it still works:

```r
ggplot(wrld_df, aes(x=long, y=lat, group=id)) + 
    geom_polygon() + geom_path(aes(group=group))
```
[/markdown]
<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/07/Rplot05.png" alt="Rplot05" width="598" height="424" class="alignleft size-full wp-image-1132" />
[markdown]
And of course, the whole point of this was to roll it into the [ggspatial](http://github.com/paleolimbot/ggspatial) package, so the easy way to go about this would be using `geom_spatial()` (but that would be cheating...).

```r
library(ggspatial) # devtools::install_github("paleolimbot/ggspatial") if you don't have it
data(longlake_waterdf)
ggplot() + geom_spatial(longlake_waterdf, aes(fill=label, col=area)) + 
    coord_fixed()
```
<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/07/unnamed-chunk-3-2.png" alt="unnamed-chunk-3-2" width="672" height="480" class="alignleft size-full wp-image-1133" />
[/markdown]

There you go! A generic solution (hopefully!) to all your holes-in-polygons needs.]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1126</wp:post_id>
		<wp:post_date><![CDATA[2016-07-13 20:35:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-07-13 23:35:46]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[holes-in-ggplot-polygons]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ggplot"><![CDATA[ggplot]]></category>
		<category domain="post_tag" nicename="ggspatial"><![CDATA[ggspatial]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				New article!		</title>
		<link>https://apps.fishandwhistle.net/archives/1140</link>
		<pubDate>Thu, 03 Nov 2016 18:24:27 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1140</guid>
		<description></description>
		<content:encoded>
				<![CDATA[It's been a nearly 9-month saga, but I'm pleased to announce that my first ever peer-reviewed paper has been published in the <a href="http://link.springer.com/journal/10933">Journal of Paleolimnology</a> as of October 2016. The paper ("<a href="http://link.springer.com/article/10.1007/s10933-016-9919-x">A geochemical perspective on the impact of development at Alta Lake, British Columbia, Canada</a>") is a result of my M.Sc. work on Alta Lake in Whistler, British Columbia. Hopefully the first of many to come!

<a href="http://link.springer.com/article/10.1007/s10933-016-9919-x"><img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/11/Screen-Shot-2016-11-03-at-2.22.36-PM-300x291.png" alt="screen-shot-2016-11-03-at-2-22-36-pm" width="300" height="291" class="alignleft size-medium wp-image-1141" /></a>]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1140</wp:post_id>
		<wp:post_date><![CDATA[2016-11-03 15:24:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-11-03 18:24:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[new-article]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="academia"><![CDATA[Academia]]></category>
		<category domain="post_tag" nicename="alta-lake"><![CDATA[Alta Lake]]></category>
		<category domain="post_tag" nicename="paleolimnology"><![CDATA[paleolimnology]]></category>
		<category domain="post_tag" nicename="publications"><![CDATA[publications]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Canada Weather 2.0		</title>
		<link>https://apps.fishandwhistle.net/archives/1146</link>
		<pubDate>Mon, 14 Nov 2016 00:12:27 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1146</guid>
		<description></description>
		<content:encoded>
				<![CDATA[I'm happy to announce that after a week-long struggle with the intricacies of the Android AppWidget framework, I've just released a new version of <a href="https://play.google.com/store/apps/details?id=ca.fwe.caweather">Canada Weather</a>. Over the past year requests have piled up involving more flexibility for the widget, better use of space on the widget, antiquated user interface, no option for sound on warning notifications, and many more that I didn't get the chance to implement in this update. In particular, the app features a shiny new user interface powered by the <a href="https://developer.android.com/topic/libraries/support-library/index.html">AppCompat framework</a>, which provides an API to make apps look and feel like the stock Google apps. It's been far too long since the last update, but I hope this keeps Canada Weather on the top of the Canadian Weather game!

[caption id="attachment_1147" align="alignleft" width="169"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/11/s2-169x300.png" alt="aahh..." width="169" height="300" class="size-medium wp-image-1147" /> aahh...[/caption]

[caption id="attachment_1148" align="alignleft" width="169"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2016/11/s1-169x300.png" alt="ooh!" width="169" height="300" class="size-medium wp-image-1148" /> ooh![/caption]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1146</wp:post_id>
		<wp:post_date><![CDATA[2016-11-13 20:12:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-11-14 00:12:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[canada-weather-2-0]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="android"><![CDATA[android]]></category>
		<category domain="post_tag" nicename="canada-weather"><![CDATA[canada weather]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				apply() yourself in R		</title>
		<link>https://apps.fishandwhistle.net/archives/1150</link>
		<pubDate>Mon, 14 Nov 2016 20:46:38 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1150</guid>
		<description></description>
		<content:encoded>
				<![CDATA[A few months ago I wrote what I thought was a quite useful post on list comprehensions in R, which, after working with numerous datasets since, I have realized is almost useless. In <a href="http://apps.fishandwhistle.net/archives/1010">the post</a>, I suggested a few ways to go about generating vectors of data using non-vectorized functions. Packages such as <a href="https://cran.r-project.org/package=foreach">foreach</a>, <a href="https://cran.r-project.org/package=plyr">plyr</a>, and <a href="https://cran.r-project.org/package=dplyr">dplyr</a> offer advanced solutions to tackle advanced manipulation and grouping. Most of the time, however, all I really want is to generate a vector of data based on some existing data structure. For sample data, we'll use a sample list of latitudes and longitudes of a few locations in Nova Scotia, Canada, and try to generate a list of distances of adjacent cities.

[markdown]
``` r
locations <- prettymapr::geocode(c("digby, NS", "middleton, NS", 
                                   "wolfville, NS", "windsor, NS", 
                                   "halifax, NS"))
locations <- locations[c("query", "lon", "lat")]
```

| query         |        lon|       lat|
|:--------------|----------:|---------:|
| digby, NS     |  -65.75857|  44.61940|
| middleton, NS |  -65.06807|  44.94243|
| wolfville, NS |  -64.36449|  45.09123|
| windsor, NS   |  -64.13637|  44.99051|
| halifax, NS   |  -63.57497|  44.64842|

For those of you who don't know, the cities listed here are along what could be a bus route from Digby to Halifax, and one could plausibly be interested in the distances between each. Finding the distance between two lat/lon points is quite easy using the [geosphere](https://cran.r-project.org/package=geosphere) package:

``` r
library(geosphere)
distGeo(c(-65.75857, 44.61940), c(-65.06807, 44.94243)) # about 65 km
```

    ## [1] 65385.73

However, in this situation, we need to compute the distance between the previous point and the current point, for which there is no vectorized function. It is possible to do this using a standard `for` loop, however it is usually best to avoid `for` loops in R as they are [horrendous for performance](http://leftcensored.skepsi.net/2011/08/21/the-performance-cost-of-a-for-loop-and-some-alternatives/). To avoid this, we need `sapply()`.

At heart, `sapply()` takes a vector (or list) input, applys a function to each item, and produces as simple an output as it can. In most cases this is a vector, but if the output has a `length` &gt; 1, results vary.

``` r
sapply(c("first item", "second item", "third item"), nchar)
```

    ##  first item second item  third item 
    ##          10          11          10

In this example, we apply `nchar` to each item individually, returning the result as a vector (the names above each item means that the vector is a **named** vector, which we can suppress by passing `USE.NAMES=FALSE`). In the case of `nchar` this is unnecessary, because `nchar` is already vectorized (i.e. passing in a vector of values results in the a vector the same length as output), but to do something more complicated, we need to specify a custom function.

``` r
sapply(c("first item", "second item", "third item"), function(item) {
  if(item == "first item") {
    return(1)
  } else if(item == "second item") {
    return(2)
  } else if(item == "third item") {
    return(3)
  }
}, USE.NAMES = FALSE)
```

    ## [1] 1 2 3

A third common use of `sapply()` is to use the indicies as well as the values within the funcion.

``` r
values <- c("first item", "second item", "third item")
sapply(1:length(values), function(index) {
  paste(rep(values[index], index), collapse="/")
})
```

    ## [1] "first item"                       "second item/second item"         
    ## [3] "third item/third item/third item"

In most cases, what you're trying to accomplish can be done using a vectorized function (the above example could use a few nested `ifelse` calls), but there's a few cases where this will not work:

-   A calculation involves values before/after the current value, or depends on the index of the value in addition to the value itself
-   A calculation involves multiple columns in a data frame, and the target function is not vectorized
-   It is necessary to construct a data structure more complicated than a vector (usually a `list`) from a vector.

Back to our list of Nova Scotian towns along a fictional bus line, the calculation we want to do falls into the first two categories. The first step is to generate a list of distances between adjascent points.

``` r
locations$distance <- c(0, sapply(2:nrow(locations), function(rownumber) {
  distGeo(c(locations$lon[rownumber-1], locations$lat[rownumber-1]), 
          c(locations$lon[rownumber], locations$lat[rownumber]))
}))
```

| query         |        lon|       lat|  distance|
|:--------------|----------:|---------:|---------:|
| digby, NS     |  -65.75857|  44.61940|      0.00|
| middleton, NS |  -65.06807|  44.94243|  65385.65|
| wolfville, NS |  -64.36449|  45.09123|  57871.46|
| windsor, NS   |  -64.13637|  44.99051|  21173.59|
| halifax, NS   |  -63.57497|  44.64842|  58453.64|

Often there is a calculation that depends heavily on on the index of the value and the value itself, for which use `sapply(1:nrow(some.data.frame), function(rownumber) ...)`. I find that I use this construct a few times a week during an average week of programming. While there is still no list comprehension in R (a Python construct), `sapply()` is as close as it gets.
[/markdown]
]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1150</wp:post_id>
		<wp:post_date><![CDATA[2016-11-14 16:46:38]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-11-14 20:46:38]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[apply-yourself-in-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="sapply"><![CDATA[sapply]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Using PySerial, PyNMEA2, and Raspberry Pi to log NMEA output		</title>
		<link>https://apps.fishandwhistle.net/archives/1155</link>
		<pubDate>Tue, 15 Nov 2016 20:10:45 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1155</guid>
		<description></description>
		<content:encoded>
				<![CDATA[A few months ago I needed to create detailed bathymetric maps for a water utility with which I was doing research. Our usual approach is to use an old Windows XP laptop with a serial port to log data from our Garmin GPS/Depth sounder unit. On the laptop, we used sofware called UnderSee Explorer (formerly Contour3D) which as far as I can tell, is now completely out of business. Usually the lakes we do research on are quite small, and the battery life of our laptops (around a few hours) is not an issue. However, the two lakes we needed to map were massive, and we needed a large amount of detail. All totalled we needed to collect bathymetric data for 5 days, which represented a significant challenge even with multiple batteries.

Enter the Raspberry Pi, which runs on 5 volts and can easily log serial data provided a suitable USB adapter. In the rain, the Raspberry Pi could easily be tucked away in a waterproof container, which was another issue we faced with using laptops in the field. In theory the Rapsberry Pi was an easy solution, all we needed was the Python code to make it happen. 

[markdown]
## PySerial

Luckily, the [PySerial](https://pypi.python.org/pypi/pyserial) library provides easy access to all matter of serial communication (Raspberry Pi and otherwise). The library is easily installed using `pip3` (`pip3 install pyserial`). In Python, the implementation is quite simple:


```python
import serial
with serial.Serial('/dev/tty.usbserial', baudrate=4800, timeout=1) as ser:
    # read 10 lines from the serial output
    for i in range(10):
        line = ser.readline().decode('ascii', errors='replace')
        print(line.strip())
```

    �K��r�bj�,*67
    $GPGSA,A,1,,,,,,,,,,,,,,,*1E
    $GPGSV,2,1,06,18,67,304,42,20,46,103,33,21,62,214,34,24,49,135,00*7C
    $GPGSV,2,2,06,28,03,033,00,32,,,27,,,,,,,,*42
    $GPRMC,195854,V,4425.8867,N,07543.5346,W,000.0,000.0,151116,,,N*7B
    $GPGGA,195855,4425.8867,N,07543.5346,W,0,00,,00000.0,M,-034.0,M,,*66
    $GPGSA,A,1,,,,,,,,,,,,,,,*1E
    $GPGSV,2,1,05,18,67,304,42,20,46,103,34,21,62,214,35,24,49,135,00*79
    $GPGSV,2,2,05,28,03,033,00,,,,,,,,,,,,*45
    $GPRMC,195855,V,4425.8867,N,07543.5346,W,000.0,000.0,151116,,,N*7A


There's a few very important things about this code that took some strategic handling. First, **use the `with` syntax when using serial ports**. You can also `open()` and `close()` `Serial` ports, but failure to do so can lead to unexpected behaviour when opening them again in the future, and handling all of the possible exception cases is a headache. The `with` syntax is much, much easier. Second, the `timeout=1` parameter is essential...it makes sure that when opening a serial port with no output (as you are bound to accidentally do at some point), the program doesn't block indefinitely when calling `readline()`. Third, you may notice that the output is complete jibberish for the first few lines. This is common, as it takes a few lines for the output to get warmed up. However, poorly written code can throw exceptions when trying to deal with this jibberish data, so adding the `errors='replace'` argument to `decode()` is a way to make sure that the `bytes` objects are defensively handled. Fourth, the serial port name. As I write this on a Mac, the serial port name for the first thing that gets plugged in is `/dev/tty.usbserial`. On linux/Raspberry Pi, this is slightly different. My solution is probably not perfect (in particular, I haven't tested it on Windows), but I combined a variety of online suggestions to come up with this:


```python
import sys
import glob
def _scan_ports():
    if sys.platform.startswith('win'):
        ports = ['COM%s' % (i + 1) for i in range(256)]
    elif sys.platform.startswith('linux') or sys.platform.startswith('cygwin'):
        # this excludes your current terminal "/dev/tty"
        patterns = ('/dev/tty[A-Za-z]*', '/dev/ttyUSB*')
        ports = [glob.glob(pattern) for pattern in patterns]
        ports = [item for sublist in ports for item in sublist]  # flatten
    elif sys.platform.startswith('darwin'):
        patterns = ('/dev/*serial*', '/dev/ttyUSB*', '/dev/ttyS*')
        ports = [glob.glob(pattern) for pattern in patterns]
        ports = [item for sublist in ports for item in sublist]  # flatten
    else:
        raise EnvironmentError('Unsupported platform')
    return ports

_scan_ports()
```




    ['/dev/cu.usbserial',
     '/dev/tty.usbserial']



## PyNMEA2

Finally, **what is all the output telling me?**. If you're familiar with NMEA output, this will be old hat to you, but if you're not you probably haven't wrapped your mind around how NMEA works. Essentially, every second or two, everything connected to the GPS system puts out some information, and that information is encoded in **sentences**, or what you see on each line. Each sentence encodes specific pieces of information separated by commas, which has a different meaning based on the first 5 (or so) characters. The exact specification is proprietary, but there is a [particularly good online reference](http://www.gpsinformation.org/dale/nmea.htm) if you're curious. Luckily, the [pynmea2](https://github.com/Knio/pynmea2) library takes care of much of this for us. For example, take a sample `$GPRMC` sentence:


```python
import pynmea2
nmea = '$GPRMC,164125,A,4425.8988,N,07543.5370,W,000.0,000.0,151116,,,A*67'
nmeaobj = pynmea2.parse(nmea)
['%s: %s' % (nmeaobj.fields[i][0], nmeaobj.data[i]) 
     for i in range(len(nmeaobj.fields))]
```




    ['Timestamp: 164125',
     'Status: A',
     'Latitude: 4425.8988',
     'Latitude Direction: N',
     'Longitude: 07543.5370',
     'Longitude Direction: W',
     'Speed Over Ground: 000.0',
     'True Course: 000.0',
     'Datestamp: 151116',
     'Magnetic Variation: ',
     'Magnetic Variation Direction: ']



This may remove some of the confusion surrounding what each field means, but it doesn't tell us some of the more important information (like, what exactly *is* the latitude/longitude based on `07543.5370`?) For this, `pynmea2` provides some additional helper attributes.


```python
nmeaobj.latitude
```




    44.431646666666666




```python
nmeaobj.longitude
```




    -75.72561666666667




```python
nmeaobj.datetime
```




    datetime.datetime(2016, 11, 15, 16, 41, 25)



## PySerial & PyNMEA2

Putting both of these pieces together, I put together a quick command-line interface program that scans all possible serial ports, logging the first valid NMEA output that it finds. This is important, beacuse without having a keyboard attached, the Rapsberry Pi can start the terminal application at boot, and will keep logging output even if errors occur (unplugging, replugging, power failure, etc.). My solution was as follows:


```python
import pynmea2, serial, os, time, sys, glob, datetime

def logfilename():
    now = datetime.datetime.now()
    return 'NMEA_%0.4d-%0.2d-%0.2d_%0.2d-%0.2d-%0.2d.nmea' % \
                (now.year, now.month, now.day,
                 now.hour, now.minute, now.second)

try:
    while True:
        ports = _scan_ports()
        if len(ports) == 0:
            sys.stderr.write('No ports found, waiting 10 seconds...press Ctrl-C to quit...\n')
            time.sleep(10)
            continue

        for port in ports:
            # try to open serial port
            sys.stderr.write('Trying port %s\n' % port)
            try:
                # try to read a line of data from the serial port and parse
                with serial.Serial(port, 4800, timeout=1) as ser:
                    # 'warm up' with reading some input
                    for i in range(10):
                        ser.readline()
                    # try to parse (will throw an exception if input is not valid NMEA)
                    pynmea2.parse(ser.readline().decode('ascii', errors='replace'))
                
                    # log data
                    outfname = logfilename()
                    sys.stderr.write('Logging data on %s to %s\n' % (port, outfname))
                    with open(outfname, 'wb') as f:
                        # loop will exit with Ctrl-C, which raises a
                        # KeyboardInterrupt
                        while True:
                            line = ser.readline()
                            print(line.decode('ascii', errors='replace').strip())
                            f.write(line)
                
            except Exception as e:
                sys.stderr.write('Error reading serial port %s: %s\n' % (type(e).__name__, e))
            except KeyboardInterrupt as e:
                sys.stderr.write('Ctrl-C pressed, exiting log of %s to %s\n' % (port, outfname))

        sys.stderr.write('Scanned all ports, waiting 10 seconds...press Ctrl-C to quit...\n')
        time.sleep(10)
except KeyboardInterrupt:
    sys.stderr.write('Ctrl-C pressed, exiting port scanner\n')
```

Note that everything in the file is written defensively...almost anything can happen while logging, and the program will continue to scan ports and log the NMEA output that it finds. The actual solution I used had a far more complicated loop that actually labelled each piece of output information (using the field labels provided by `pynmea2`, so that troubleshooting was a little easier. That solution is a little lengthy for this blog post, but it was incredibly useful in the field.

## Raspberry Pi Setup

The final step in using the Raspberry Pi to log NMEA data was to setup the Pi itself to log the data. The two main objectives here were to (1) have the Raspberry Pi open a terminal running our `scan_serial.py` file, (2) **never turn off the screen** (after all, what is the point of logging data all day for 5 days if you can't be sure the data ever logged?), and (3) make sure there is an on-screen keyboard available should things go wrong. It may seem that these are trivial Google search issues, but it turns out they are a little more involved (possibly because I have a Raspberry Pi 3, which is still relatively new).

After a few false starts, the solution that ended up working for me was to edit the `autostart` file in `/home/pi/.config/lxsession/LXDE-pi/autostart`. I added the line `@lxterminal --command /home/pi/onstart.sh`. Of course I could have added the exact command I was trying to run, but it is a little easier to wrap the command I was trying to run in a shell script (not to mention having it be easier to find again).

![screenshot3](/wp-content/uploads/2016/11/screenshot3.png)

![screenshot4](/wp-content/uploads/2016/11/screenshot4.png)

Then I had to create the `/home/pi/onstart.sh` file that contained the script that would run when the Pi booted up. I wrapped the command that launched our looping NMEA logger in an `echo` command (for no particular reason), a `cd "/home/pi"` (to make sure that log files ended up in the home folder and not somewhere else), and finally `sleep 5` (to catch any error messages that appeared as a result of the above code). The launch command was `python3 path/to/scan_serial.py`, or the code that appears above. Remember to `chmod +x` your `onstart.sh` script, or it may not run!

![screenshot3](/wp-content/uploads/2016/11/screenshot5.png)

![screenshot4](/wp-content/uploads/2016/11/screenshot6.png)

Keeping the screen on took a few more false starts, but of all the solutions a Google was able to find me, the only one that worked was changing the `xserver-command=` line in `/etc/lightdm/lightdm.conf`. It's hard to say what the line read previously, but in the end, the whole line read `xserver-command=X -s 0 dpms`.

![screenshot3](/wp-content/uploads/2016/11/screenshot1.png)

![screenshot4](/wp-content/uploads/2016/11/screenshot2.png)

## Battery Setup

The one final piece of the puzzle was reliably powering the Pi for a whole day on a 12-volt battery. I had thought this would be trivial, but it turns out the Pi needs at least 1.2A of output to run without shutting down (> 2.0 A is preferable). Many USB adapters only provide 500mA, and some USB cables (especially long ones) loose a significant amount of current between the adapter and the Pi. Looking at the USB adapter carefully is necessary when selecting one for use with the Pi.

![screenshot3](/wp-content/uploads/2016/11/photopower1.png)

![screenshot4](/wp-content/uploads/2016/11/photopower2.png)

For the on-screen keyboard I used matchbox-keyboard (`sudo apt-get install matchbox-keyboard`), and created a launcher script on the desktop.

## Extras

As a part of putting together NMEA data, I put together a list of 'talkers' (fairly sure they were taken from [here](http://www.gpsinformation.org/dale/nmea.htm) at some point) that may be useful in providing more meaningful output.


```python
_NMEA_TALKERS = {'AG': 'Autopilot(General)',
                 'AP': 'Autopilot(Magnetic)',
                 'CC': 'Programmed Calculator',
                 'CD': 'DSC (Digital Selective Calling)',
                 'CM': 'Memory Data',
                 'CS': 'Satellite Communications',
                 'CT': 'Radio-Telephone (MF/HF)',
                 'CV': 'Radio-Telephone (VHF)',
                 'CX': 'Scanning Receiver',
                 'DE': 'DECCA Navigation',
                 'DF': 'Direction Finder',
                 'DM': 'Magnetic Water Velocity Sensor',
                 'EC': 'ECDIS (Electronic Chart Display & Information System)',
                 'EP': 'EPIRB (Emergency Position Indicating Beacon)',
                 'ER': 'Engine Room Monitoring Systems',
                 'GP': 'GPS',
                 'HC': 'Magnetic Compass',
                 'HE': 'North Seeking Gyro',
                 'HN': 'Non-North Seeking Gyro',
                 'II': 'Integrated Instrumentation',
                 'IN': 'Integrated Navigation',
                 'LA': 'Loran A',
                 'LC': 'Loran C',
                 'MP': 'Microwave Positioning System',
                 'OM': 'OMEGA Navigation System',
                 'OS': 'Distress Alarm System',
                 'RA': 'RADAR and/or ARPA',
                 'SD': 'Depth Sounder',
                 'SN': 'Electronic Positioning System',
                 'SS': 'Scanning Sounder',
                 'TI': 'Turn Rate Indicator',
                 'TR': 'TRANSIT Navigation System',
                 'VD': 'Doppler Velocity Sensor',
                 'VW': 'Mechanical Water Velocity Sensor',
                 'WI': 'Weather Instruments',
                 'YC': 'Temperature Transducer',
                 'YD': 'Displacement Transducer',
                 'YF': 'Frequency Transducer',
                 'YL': 'Level Transducer',
                 'YP': 'Pressure Transducer',
                 'YR': 'Flow Rate Transducer',
                 'YT': 'Tachometer Transducer',
                 'YV': 'Volume Transducer',
                 'YX': 'Transducer',
                 'ZA': 'Atomic Clock Timekeeper',
                 'ZC': 'Chronometer Timekeeper',
                 'ZQ': 'Quartz Clock Timekeeper',
                 'ZV': 'Radio Update Timekeeper'}

_NMEA_MESSAGES__ = {'GNS': 'Fix data',
                    'DPT': 'Depth of Water',
                    'GST': 'GPS Pseudorange Noise Statistics',
                    'DTM': 'Datum Reference',
                    'GSV': 'Satellites in view',
                    'AAM': 'Waypoint Arrival Alarm',
                    'FSI': 'Frequency Set Information',
                    'VHW': 'Water speed and heading',
                    'GLC': 'Geographic Position, Loran-C',
                    'MSS': 'Beacon Receiver Status',
                    'PASHR': 'RT300 proprietary roll and pitch sentence',
                    'GSA': 'GPS DOP and active satellites',
                    'VDR': 'Set and Drift',
                    'MSK': 'Control for a Beacon Receiver',
                    'GBS': 'GPS Satellite Fault Detection',
                    'TPC': 'Trawl Position Cartesian Coordinates',
                    'HFB': 'Trawl Headrope to Footrope and Bottom',
                    'ZTG': 'UTC & Time to Destination Waypoint',
                    'MWV': 'Wind Speed and Angle',
                    'DCN': 'Decca Position',
                    'HSC': 'Heading Steering Command',
                    'PUBX 00': 'uBlox Lat/Long Position Data',
                    'PRWIZCH': 'Rockwell Channel Status',
                    'OLN': 'Omega Lane Numbers',
                    'RMB': 'Recommended Minimum Navigation Information',
                    'RMC': 'Recommended Minimum Navigation Information',
                    'RMA': 'Recommended Minimum Navigation Information',
                    'GGA': 'Global Positioning System Fix Data',
                    'TTM': 'Tracked Target Message',
                    'PGRME': 'Garmin Estimated Error',
                    'ROT': 'Rate Of Turn',
                    'OSD': 'Own Ship Data',
                    'VLW': 'Distance Traveled through Water',
                    'WPL': 'Waypoint Location',
                    'PUBX 01': 'uBlox UTM Position Data',
                    'RTE': 'Routes',
                    'GTD': 'Geographic Location in Time Differences',
                    'GRS': 'GPS Range Residuals',
                    'VTG': 'Track made good and Ground speed',
                    'WCV': 'Waypoint Closure Velocity',
                    'PMGNST': 'Magellan Status',
                    'STN': 'Multiple Data ID',
                    'MTW': 'Mean Temperature of Water',
                    'TRF': 'TRANSIT Fix Data',
                    'TDS': 'Trawl Door Spread Distance',
                    'XTE': 'Cross-Track Error, Measured',
                    'TPT': 'Trawl Position True',
                    'TPR': 'Trawl Position Relative Vessel',
                    'PUBX 03': 'uBlox Satellite Status',
                    'R00': 'Waypoints in active route',
                    'DBK': 'Depth Below Keel',
                    'ALM': 'GPS Almanac Data',
                    'TFI': 'Trawl Filling Indicator',
                    'PUBX 04': 'uBlox Time of Day and Clock Information',
                    'RSD': 'RADAR System Data',
                    'RPM': 'Revolutions',
                    'RSA': 'Rudder Sensor Angle',
                    'VWR': 'Relative Wind Speed and Angle',
                    'ITS': 'Trawl Door Spread 2 Distance',
                    'LCD': 'Loran-C Signal Data',
                    'SFI': 'Scanning Frequency Information',
                    'APB': 'Autopilot Sentence "B"',
                    'VBW': 'Dual Ground/Water Speed',
                    'DBS': 'Depth Below Surface',
                    'APA': 'Autopilot Sentence "A"',
                    'DBT': 'Depth below transducer',
                    'ZFO': 'UTC & Time from origin Waypoint'}
```

[/markdown]
]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1155</wp:post_id>
		<wp:post_date><![CDATA[2016-11-15 16:10:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-11-15 20:10:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[using-pyserial-pynmea2-and-raspberry-pi-to-log-nmea-output]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="nmea"><![CDATA[nmea]]></category>
		<category domain="post_tag" nicename="pynmea2"><![CDATA[pynmea2]]></category>
		<category domain="post_tag" nicename="pyserial"><![CDATA[pyserial]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[Python]]></category>
		<category domain="post_tag" nicename="raspberry-pi"><![CDATA[Raspberry Pi]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Plot.wp Wordpress Plugin Release		</title>
		<link>https://apps.fishandwhistle.net/archives/1168</link>
		<pubDate>Wed, 16 Nov 2016 19:33:05 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1168</guid>
		<description></description>
		<content:encoded>
				<![CDATA[Most of my development stays away from the realm of the web, but recently I've been stewing over a method of compiling paleolimnological data in some centralized repository such that it can be retrieved dynamically. I was also browsing in a local bookstore a few months ago and came across an interesting book entitled <a href="https://www.amazon.com/Building-Web-Apps-WordPress-Application/dp/1449364071/ref=sr_1_4?ie=UTF8&qid=1479323852&sr=8-4&keywords=wordpress+web+application+development">Building Web Apps with WordPress: WordPress as an Application Framework</a>. My involvement with Wordpress has mostly been as a user, but in browsing the book I can appreciate how Wordpress takes care of the data structures and frameworks that are the foundation of any web application. As an exercise, I developed a <a href="https://en-ca.wordpress.org/plugins/plotwp/">tiny Wordpress plugin</a> that uses the <a href="https://plot.ly/javascript/">Plotly.js plotting API</a> to embed plots in posts and pages. It's not particularly useful at the moment (currently embedding a plot requires typing out JSON, which if you've ever tried it, is not fun), but it serves as a useful outlet to delve into the web development scene, from which I've so far been absent.

[plotly]
{
  "data": [{
    "x": [1, 2, 3, 4],
    "y": [27, 28, 29, 50],
    "mode": "lines+markers",
    "type": "scatter"
  }],
  "layout": {
    "margin": {
      "t": 40, "r": 40, "b": 40, "l":40
    }
  }
}
[/plotly] 

(An indicator of how hard this is to use is probably the fact that I couldn't muster a more impressive example plot than this for its inaugural release post)]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1168</wp:post_id>
		<wp:post_date><![CDATA[2016-11-16 15:33:05]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-11-16 19:33:05]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[plot-wp-wordpress-plugin-release]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="php"><![CDATA[php]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
		<category domain="post_tag" nicename="web-development"><![CDATA[web development]]></category>
		<category domain="post_tag" nicename="wordpress-plugins"><![CDATA[wordpress plugins]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Processing sub-bottom profiling data in Python		</title>
		<link>https://apps.fishandwhistle.net/archives/1174</link>
		<pubDate>Thu, 17 Nov 2016 20:16:56 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1174</guid>
		<description></description>
		<content:encoded>
				<![CDATA[In addition to processing bathymetric data, my research on lakes requires sub-bottom profiling, or the collection of low-energy seismic data collected similarly to bathymetric data. The system we use is the SyQwest HydroBox, which, due to many hardware and software limitations, has been particularly frustrating. Still, when it does work properly, the HydroBox data has been invaluable to our lakes research. Unfortunately, the software supplied with the HydroBox is not particularly good at visualizing the collected data after collection.

[markdown]
![a look at the supplied interface](/wp-content/uploads/2016/11/hb_ss1.png)

As you can see, the only option for visualization following collection is to play back the file from the beginning (and if you change window settings the playback often resets unexpectedly). It's quite maddening to deal with, and in the past I've had to make figures by stitching together multiple screenshots in MS Paint. This assumes the boat travelled at constant velocity and requires some manual typing in of coordinates to figure out exactly where the section is geographically. Needless to say, absolutely un-ideal.

So what are the alternatives? The HydroBox is fully compatible with proprietary solutions such as [HyPack](http://www.hypack.com/new/), [HydroPro](http://construction.trimble.com/products-and-solutions/hydropro-software), and [SonarWiz](https://www.chesapeaketech.com/products/sonarwiz-sidescan/), but for a small fry like me these are expensive and have capabilities far beyond what I need. Plus, my sometimes out-of-control programming habit needs an outlet.

## Hacking the HydroBox

The HydroBox saves playback to a `.odc` file, in a format that looks much like NMEA (but is not quite). I think the output is in something called 'ODEC' format, but if the documentation of this format is on the internet, I haven't been able to find it (in fact, there are only two mentions of the PNTI sentence that makes up the format on the internet). To illustrate, here's some data I collected during my M.Sc. thesis ([get the data here](/wp-content/uploads/2016/11/20140711114326.odc)).

```python
with open('sample_odc/20140711101028.odc', 'rb') as f:
    for row in range(6):
        print(f.readline())
```

    b'$PNTI,101,!,00.11,00.49,0,*2F\r\n'
    b'$PNTI,103,X,0,0,1,0,0,1,0,H,*3D\r\n'
    b'$PNTI,105,W,0,20,0,29,98,1500,0,0,0,G,*37\r\n'
    b'$PNTI,171,07/11/14,10:10:28,0.00,*0E\r\n'
    b"$PNTI,111,H,1,00000,0,0020,0000,03296,\xff\xff\xff\xff\xff\xff\xff\xadW4D0\x14\x00\x00\x000\x01\x00 \x02\x00\x10\x000\x00\x00\x00\x00\x13\x00\x00!@\x12B\x00\x000\x00\x01 \x10\x120\x00\x00 \x02\x00\x00\x03!\x01\x021\x00\x00\x00\x00C\x00\x00DBFDs\x98\xaa\x8ahU\x87\xa9\x88hQv\x87\x99\x8au\x88\x99\x9a\xa9\x8a\x99xx\x01\x02@\xa7\x9aE\x98gWSF\x87\x88H\x00\x00\x00\x86Gf\x16\x00w\x06\x00\x86\x87x\x01 \x00u7#\x81x\x89vi\x03\x81\x89\x97\x88\x89\x98x\x89'\x01\x86%\x10\x98\x88\x89\x97\x88\x99\xa8\x99\xaa\x99\x8a\x06\x82x\x02d`fw\xa9\xb9\x9a&\x85y\x98\x98\x8a\xa9h%\x03U\x12\x84h\x99yTE\x96\x9a\xb8\x89F\xa8\x9a\xaaWc\xaa\x99\x9b\xa9\x8a5,*EA\r\n"
    b'$PNTI,151,07/11/14,17:10:28.17, 50.108116667,-122.981900000,181.1, 0.4,*04\r\n'

The first thing you'll notice is that we have to read the file in binary (hence the `'rb'`), because the file is not strictly (but contains some) text. The format for each line in the file is `$PNTI,...,...`, where the first `...` is some three-digit number that presumably tells us something about the data that follows (the second `...`). As far as I can tell from my examination of the data I've collected over the years, the types of sentences that are output are as follows:
<div>
<table class="dataframe" border="1">
<thead>
<tr style="text-align: right;">
<th>Type</th>
<th>Example Data</th>
<th>Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>$PNTI,101</td>
<td>b'$PNTI,101,!,00.12,00.38,0,*2A'</td>
<td>6</td>
</tr>
<tr>
<td>$PNTI,103</td>
<td>b'$PNTI,103,A,2,0,0,0,0,15,0,A,*1B'</td>
<td>12</td>
</tr>
<tr>
<td>$PNTI,105</td>
<td>b'$PNTI,105,C,1,120,0,6,48,4921,0,C,*2D'</td>
<td>122</td>
</tr>
<tr>
<td>$PNTI,111</td>
<td>b'$PNTI,111,C,1,04987,0,0120,0000,03296,\xa7\xcd\x9bW\x00\x00\x00\x00\x...</td>
<td>147384</td>
</tr>
<tr>
<td>$PNTI,151</td>
<td>b'$PNTI,151,10/12/05,14:28:23.64, 43.106417233,-76.234257950, 0.0, 0.0...</td>
<td>147353</td>
</tr>
<tr>
<td>$PNTI,152</td>
<td>b'$PNTI,152,11/09/16,22:26:55, 11/09/16 22:26:55,*19'</td>
<td>1</td>
</tr>
<tr>
<td>$PNTI,171</td>
<td>b'$PNTI,171,10/12/05,14:28:23,0.46,*0D'</td>
<td>6</td>
</tr>
</tbody>
</table>
</div>
In playback mode, the non-depth reflection data is displayed on the lefthand side in something that looks like this. Trying to link these two is currently my only method for determining the meaning of the data, althogh with some more experience in geophysics I'm sure I could do a little better. The following is my best guess as to what the output data means (note that there are quite a few question marks).

![the details](/wp-content/uploads/2016/11/hb_ss2.png)

### PNTI 171: Date/time at start of file

`$PNTI,171,10/12/05,14:28:23,0.46,*0D`

Easiest first, this looks like straight up date/time information in the format `PNTI,171,MM/DD/YY,HH:mm:ss,*FF`. My best guess is that the time is UTC, and this sentence appears to only exist at the beginning of ODC files. This can be parsed using Python's `datetime.datetime` object (`datetime.strptime(timestamp), '%m/%d/%y %H:%M:%S')`).

### PNTI 103: Settings at start of file

`$PNTI,103,A,2,0,0,0,0,15,0,A,*1B`

The PNTI 103 sentence is similar to the PNTI 105 sentence (below), but always appears at the beginning of files and sometimes (not often) at other places within the file.

### PNTI 105: Settings changed

`$PNTI,105,C,1,120,0,6,48,4921,0,C,*2D`

This sentence occurs when settings are changed during data collection. I'm unsure what the significance of the letters are, except that the last letter appears in the PNTI 111 sentences that follow, and the first one doesn't necessarily. The format is `PNTI,105,[A-Z],[0 or 1],RANGE,0,LF_GAIN,HF_GAIN,????,0,[A-Z],*2D`. The ???? is always 4 digits and for all of our data is 1500 (for some of the sample data it is &gt;4000 and variable).

### PNTI 152: Annotation

`$PNTI,152,11/09/16,22:26:55, 11/09/16 22:26:55,*19`

This sentence only came up once in my examination of the data, and I think it is what happens when you 'annotate'. The one example I have has an annotation that is just the date and time, but I think this is just the default text and was likely added by accident. The format is `PNTI,152,MM/DD/YY,HH:mm:ss,ANNOTATION TEXT,*FF`.

### PNTI 151: Lat/Lon Data

`$PNTI,151,10/12/05,14:28:23.64, 43.106417233,-76.234257950, 0.0, 0.0,*36`

This sentence carries the GPS data (which includes a mega accurate date/time to the hundredth of a second). Unlike NMEA lat/lon, the lat/lon data in this sentence are in decimal degrees. The format is `PNTI,151,MM/DD/YY,HH:mm:ss.ss, LATITUDE,LONGITUDE, ELEVATION, HDOP(?),*36`. The HDOP is a guess, but it is the only other piece of information I can think of that would be reported in this way (it is generally a decimal number less than 1.0). The datetime can be parsed using Python's `datetime.datetime` object (`datetime.strptime(time + date[:date.rfind('.')]), '%m/%d/%y %H:%M:%S')`). This truncates the hundredths of a second, but if you want them back you can use `.replace(microsecond=...)`.

### PNTI 111: Reflection data

`$PNTI,111,C,1,04987,0,0120,0000,03296,\xa7\xcd\x9bW\x00\x00\x00\x00\x00\x00\x10\x02\x00\x00\x00...`

This sentence carries the real data, and since most of that data is in binary form, it is the reason we cannot process the files as ASCII text (as we would other NMEA data). Because this is the bulk of the data, we will examine values from this sentence closely.
<div>
<table class="dataframe" border="1">
<thead>
<tr style="text-align: right;">
<th>Column</th>
<th>Best Guess</th>
<th>Values</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Sentence ID</td>
<td>$PNTI</td>
</tr>
<tr>
<td>2</td>
<td>Sentence ID</td>
<td>111</td>
</tr>
<tr>
<td>3</td>
<td>From previous PNTI 105?</td>
<td>D, E, F, G, I, J, K, L, M, N, Q, S, U, V, W, X, Y, Z</td>
</tr>
<tr>
<td>4</td>
<td>LF (1), HF(2)</td>
<td>1, 2</td>
</tr>
<tr>
<td>5</td>
<td>Depth * 100 (m)?</td>
<td>00000, 00150, 00151, 00152, 00153, 00155, 00156, 00157, 00158, 00160, 0...</td>
</tr>
<tr>
<td>6</td>
<td>Always 0</td>
<td>0</td>
</tr>
<tr>
<td>7</td>
<td>Range (metres)</td>
<td>0005, 0010, 0020, 0030, 0040, 0060, 0120</td>
</tr>
<tr>
<td>8</td>
<td>Unknown (mostly 0000)</td>
<td>000/, 0000, 0002, 0003, 0005, 0008, 0009, 0011, 0013, 1547, 1858, 1946,...</td>
</tr>
<tr>
<td>9</td>
<td>Always 03296</td>
<td>03296</td>
</tr>
<tr>
<td>10</td>
<td>Amplitude data</td>
<td>1, 182, 193, 200</td>
</tr>
</tbody>
</table>
</div>
The final column (column 10) carries amplitude response data, which is what is shown in the main area of the hydrobox playback window. The values shown above are lengths, which are usually 200 but it appears this can vary slightly. Each byte represents a value between 0 and 255 corresponding to the response amplitude. This data is transmitted approximately 10 times per second.

## A note on checksums

You will notice the `*XX` string at the end of every line...this is the NMEA checksum. For some reason, most data fails the NMEA checksum (perhaps I've written my checksum algorithm wrong, but I don't think I have). Therefore, it is necessary to use other means to verify the integrity of the data.

## Extracting the data

Given our newfound enlightenment on the `.odc` files output by the HydroBox software, we can extract the data. All we really need is the date/time, range, lat/lon, depth, and the amplitude data to make a pretty picture (and perhaps do some processing). ODC files can get rather large and so I would suggest that instead of loading everything into memory, it is better to write a generic handler function that gets called with the above fields that can be changed depending on the application. If using HF and LF, one could write two such functions, since the data from one probably shouldn't be mixed with the other.

```python
def handle_data(datetime, lon, lat, depth, depthrange, data):
    ## do something
    pass
```

Given appropriate handler functions, looping through the file can be done with the same code each time (I've also added a `startline` and `maxlines` argument in case reading a partial file is preferable). Note that I've added a check for the return value, so that the data handlers can tell the loop when to stop reading. This is useful if you've created some kind of finite data structure (like a NumPy array) to hold the data (you can `return False` to stop reading the file. Using `maxlines` is still a little safer (since data handlers aren't called when lines are malformed).


```python
def read_odc(filename, startline=1, checksum=False, maxlines=0, handle_hf=None, handle_lf=None):
    with open(filename, 'rb') as f:
        # these data are on different lines than the data itself,
        # so they need to be updated
        lon = float('nan')
        lat = float('nan')
        datetime = 'NA'
        
        linenum = 0
        for line in f:
            linenum += 1
            if linenum < startline:
                continue
            elif maxlines > 0 and (linenum-startline) > maxlines:
                break
            elif checksum and not nmea_checksum(line):
                continue
            
            if line.startswith(b'$PNTI,151'):
                try:
                    parts = line.decode('ascii').split(',')
                    lon = float(parts[5])
                    lat = float(parts[4])
                    datetime = ' '.join(parts[2:4])
                except UnicodeDecodeError:
                    # invalid bytes in string
                    pass
                except ValueError:
                    # string can't be parsed as a float
                    pass
                except IndexError:
                    # there aren't enough fields (probably malformed line)
                    pass
            elif line.startswith(b'$PNTI,111'):
                try:
                    # need to treat as a binary string
                    parts = line.split(b',')
                    depth = float(parts[4].decode('ascii')) / 100.0
                    depthrange = float(parts[6].decode('ascii'))
                    # convert to ints
                    data = [b for b in parts[9]]
                    freqtype = int(parts[3].decode('ascii'))
                except UnicodeDecodeError:
                    # invalid bytes in string
                    continue
                except ValueError:
                    # string can't be parsed as a float
                    continue
                except IndexError:
                    # there aren't enough fields (probably malformed line)
                    continue
                if freqtype == 1 and handle_lf is not None:
                    if handle_lf(datetime=datetime, lon=lon, lat=lat, depth=depth, 
                              depthrange=depthrange, data=data) is False:
                        break
                elif freqtype == 2 and handle_hf is not None:
                    if handle_hf(datetime=datetime, lon=lon, lat=lat, depth=depth, 
                              depthrange=depthrange, data=data) is False:
                        break

            
def handle_data_lf(datetime, lon, lat, depth, depthrange, data):
    print('LF Datetime: %s, Lon: %.5f, Lat: %.5f, Depth: %s, Depth Range: %s' %\
            (datetime, lon, lat, depth, depthrange))
    
def handle_data_hf(datetime, lon, lat, depth, depthrange, data):
    print('HF Datetime: %s, Lon: %.5f, Lat: %.5f, Depth: %s, Depth Range: %s' %\
            (datetime, lon, lat, depth, depthrange))
    
read_odc('sample_odc/20051012142823.odc', maxlines=20, handle_lf=handle_data_lf,
        handle_hf=handle_data_hf)
```

    LF Datetime: NA, Lon: nan, Lat: nan, Depth: 49.87, Depth Range: 120.0
    HF Datetime: 10/12/05 14:28:23.64, Lon: -76.23426, Lat: 43.10642, Depth: 49.31, Depth Range: 120.0
    LF Datetime: 10/12/05 14:28:23.64, Lon: -76.23426, Lat: 43.10642, Depth: 49.87, Depth Range: 120.0
    HF Datetime: 10/12/05 14:28:23.69, Lon: -76.23426, Lat: 43.10642, Depth: 49.31, Depth Range: 120.0
    LF Datetime: 10/12/05 14:28:23.75, Lon: -76.23426, Lat: 43.10642, Depth: 51.78, Depth Range: 120.0
    HF Datetime: 10/12/05 14:28:23.75, Lon: -76.23426, Lat: 43.10642, Depth: 49.27, Depth Range: 120.0
    LF Datetime: 10/12/05 14:28:23.80, Lon: -76.23426, Lat: 43.10642, Depth: 49.83, Depth Range: 120.0
    HF Datetime: 10/12/05 14:28:23.86, Lon: -76.23426, Lat: 43.10642, Depth: 49.23, Depth Range: 120.0


Of course, printing the output doesn't really help anybody, but writing to a CSV is a little bit more helpful. The problem still exists of dealing with the reflection data (200 elements long), which I've dealt with in the past by converting each integer (from 0 to 255) to a 2-character hex digit. This allows other programs (like R, or reading the file back in to Python) the ability to read the data more easily. Even looking at the hex string can be useful (non zeros tend to pop out quite nicely).

```python
with open('sample_odc/output_lf.csv', 'w') as out:
    out.write('datetime,lon,lat,depth,depthrange,data\n')
    def handle_data_lf(datetime, lon, lat, depth, depthrange, data):
        out.write(','.join((datetime, str(lon), str(lat), str(depth), str(depthrange),
                           ''.join(['%0.2X' % val for val in data]))) + '\n')
    read_odc('sample_odc/20051012142823.odc', handle_lf=handle_data_lf)

pd.read_csv('sample_odc/output_lf.csv').head()
```

<div>
<table class="dataframe" border="1">
<thead>
<tr style="text-align: right;">
<th>datetime</th>
<th>lon</th>
<th>lat</th>
<th>depth</th>
<th>depthrange</th>
<th>data</th>
</tr>
</thead>
<tbody>
<tr>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>49.87</td>
<td>120.0</td>
<td>A7CD9B5700000000000010020000000000000000000000...</td>
</tr>
<tr>
<td>10/12/05 14:28:23.64</td>
<td>-76.234264</td>
<td>43.106416</td>
<td>49.87</td>
<td>120.0</td>
<td>A7CD9B0700000000000000000000000000000000000000...</td>
</tr>
<tr>
<td>10/12/05 14:28:23.75</td>
<td>-76.234264</td>
<td>43.106416</td>
<td>51.78</td>
<td>120.0</td>
<td>A7CD9B6800000000000000000000000000000000000000...</td>
</tr>
<tr>
<td>10/12/05 14:28:23.80</td>
<td>-76.234264</td>
<td>43.106416</td>
<td>49.83</td>
<td>120.0</td>
<td>B7CD9B2700000000000000320000000000000000000000...</td>
</tr>
<tr>
<td>10/12/05 14:28:23.91</td>
<td>-76.234264</td>
<td>43.106416</td>
<td>50.36</td>
<td>120.0</td>
<td>A7CD9B6800210000000000010000000000000000000000...</td>
</tr>
</tbody>
</table>
</div>



For visuzliation, there are a few options. The easiest is the 'dumb' option, whereby the depthrange is ignored. I often do not change the depth range for this express purpose, but for other options you can see below.

**Note**: We are going to start loading the data into memory, so be very careful when reading an entire file, as the files can get really, really big. Make sure you try a small amount of data before going big!


```python
import numpy as np

alldata = []
def handle_data_lf(datetime, lon, lat, depth, depthrange, data):
    # because the data is not always exactly 200 elements long,
    # we need to manually ensure the output is 200 elements
    # or it won't load into numpy properly
    if len(data) == 200:
        alldata.append(data)
    elif len(data) > 200:
        alldata.append(data[:200])
    else:
        alldata.append(data + [0 for i in range(200-len(data))])

read_odc('sample_odc/20110627125717.odc', handle_lf=handle_data_lf)

np.array(alldata)
```




    array([[255, 255, 255, ...,   0,   0,   0],
           [255, 255, 255, ...,   0,   0,   0],
           [255, 255, 255, ...,  35,   0,   0],
           ..., 
           [255, 255, 255, ...,  68,  68,  51],
           [255, 255, 255, ...,  35,   1,   0],
           [255, 255, 255, ...,  85,  52,  51]])




```python
import matplotlib.pylab as plt
%matplotlib inline

plt.imshow(np.array(alldata).T, aspect='auto')
```




    <matplotlib.image.AxesImage at 0x11012cba8>




![png](/wp-content/uploads/2016/11/output_17_1.png)


The 'dumb' way works great, except when there is a change in the depth range, which makes the output inconsistent. There are probably a few approaches, but the first that comes to mind is getting the data in x/y/z, where the x is the ping number (or distance along a transect), y is the actual depth of the measurement, and z is the strength of the return.


```python
xs = []
ys = []
zs = []
pingnum = 0
def handle_data_lf(datetime, lon, lat, depth, depthrange, data):
    global pingnum
    
    if len(data) > 200:
        data = data[:200]
    elif len(data) < 200:
        data = data + [0 for i in range(200-len(data))]
    for i in range(200):
        xs.append(pingnum)
        ys.append(i*depthrange/200.0)
        zs.append(data[i])
    
    pingnum += 1
    if pingnum == 500:
        return False
read_odc('sample_odc/20051012142823.odc', handle_lf=handle_data_lf)

df = pd.DataFrame({'x':xs, 'y':ys, 'z':zs})
df.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
      <th>z</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.0</td>
      <td>167</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0.6</td>
      <td>205</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1.2</td>
      <td>155</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>1.8</td>
      <td>87</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>2.4</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



Unfortunately there is no easy way to plot this without interpolation, although if what is desired is a true transect using distance as the x-axis, this is probably the only way. A less involved method (ignoring distance along the transect) is to pick a depth range at the start, and resample each array as the file is read.


```python
plot_depthrange = 40.0
plot_npixels = 500
plot_res = plot_depthrange / plot_npixels
depthvals = np.arange(0, plot_depthrange, plot_res)
alldata = []

def handle_data_lf(datetime, lon, lat, depth, depthrange, data):
    if depthrange == 0:
        return
    if len(data) > 200:
        data = data[:200]
    elif len(data) < 200:
        data = data + [0 for i in range(200-len(data))]
    old_depthvals = np.arange(0, depthrange, depthrange/200.0)
    resampled = np.interp(depthvals, old_depthvals, data, right=0)
    alldata.append(resampled)
    

read_odc('sample_odc/20140711114326.odc', startline=29000,
         maxlines=6000, handle_lf=handle_data_lf)
plt.imshow(np.array(alldata).T, aspect='auto')
```




    <matplotlib.image.AxesImage at 0x1083e7080>




![png](/wp-content/uploads/2016/11/output_21_1.png)


## Closing comments

Processing data from the HydroBox isn't easy, and it can't be said that the HydroBox program does a much better job. At least the above code allows the user some flexibility on cropping, resampling, and parsing some of the data for the purposes of diagrams and further geoprocessing.

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1174</wp:post_id>
		<wp:post_date><![CDATA[2016-11-17 16:16:56]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-11-17 20:16:56]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[processing-sub-bottom-profiling-data-in-python]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="hydrobox"><![CDATA[HydroBox]]></category>
		<category domain="post_tag" nicename="nmea"><![CDATA[nmea]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[Python]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Bulk downloading of Environment Canada Climate data		</title>
		<link>https://apps.fishandwhistle.net/archives/1198</link>
		<pubDate>Tue, 06 Dec 2016 21:41:06 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1198</guid>
		<description></description>
		<content:encoded>
				<![CDATA[Fetching data from Environment Canada's archive has always been a bit of a chore. In the old days, it was necessary to download data one click at a time from the <a href="http://climate.weather.gc.ca/historical_data/search_historic_data_e.html">organization's search page</a>. To bulk download hourly data would require a lot of clicks and a good chance of making a mistake and having to start all over again. There are several R solutions online (posted by <a href="http://www.headwateranalytics.com/blog/web-scraping-environment-canada-weather-data">Headwater Analytics</a> and <a href="http://www.fromthebottomoftheheap.net/2015/01/14/harvesting-canadian-climate-data/">From the Bottom of the Heap</a> ), but both solutions are mostly single-purpose, and don't solve the additional problem of trying to find climate locations near you. In the <a href="https://cran.r-project.org/package=rclimateca">rclimateca package</a>, I attempt to solve both of these problems to produce filtered, plot-ready data from a single command.

[markdown]
Installation
------------

Rclimateca is available on CRAN and can be installed using `install.packages()`.

``` r
install.packages('rclimateca')
```

Finding climate stations
------------------------

We will start with finding sites near where you're interested in. Sometimes you will have a latitude and longitude, but most times you will have a town or address. Using the [prettymapr](https://cran.r-project.org/package=prettymapr) packages 'geocode' function, the `getClimateSites()` function looks up locations near you.

``` r
library(rclimateca)
getClimateSites("gatineau QC")
```

    ##                       Name Province Station ID Latitude (Decimal Degrees)
    ## 5648      OTTAWA CITY HALL  ONTARIO       4334                      45.43
    ## 5654  OTTAWA LA SALLE ACAD  ONTARIO       4339                      45.43
    ## 5656 OTTAWA LEMIEUX ISLAND  ONTARIO       4340                      45.42
    ## 5663         OTTAWA U OF O  ONTARIO       4346                      45.42
    ## 5662     OTTAWA STOLPORT A  ONTARIO       7684                      45.47
    ##      Longitude (Decimal Degrees) First Year Last Year
    ## 5648                      -75.70       1966      1975
    ## 5654                      -75.70       1954      1967
    ## 5656                      -75.73       1953      1979
    ## 5663                      -75.68       1954      1955
    ## 5662                      -75.65       1974      1976

If you also need data for a set of years, you can also pass a vector of years to further refine your data.

``` r
getClimateSites("gatineau QC", year=2014:2016)
```

    ##                   Name Province Station ID Latitude (Decimal Degrees)
    ## 7147           CHELSEA   QUEBEC       5585                      45.52
    ## 5646        OTTAWA CDA  ONTARIO       4333                      45.38
    ## 5647    OTTAWA CDA RCS  ONTARIO      30578                      45.38
    ## 7154 OTTAWA GATINEAU A   QUEBEC      50719                      45.52
    ## 7155 OTTAWA GATINEAU A   QUEBEC      53001                      45.52
    ##      Longitude (Decimal Degrees) First Year Last Year
    ## 7147                      -75.78       1927      2016
    ## 5646                      -75.72       1889      2016
    ## 5647                      -75.72       2000      2016
    ## 7154                      -75.56       2012      2016
    ## 7155                      -75.56       2014      2016

If you'd like to apply your own subsetting operation, the entire dataset is also available through this package (although it may be slightly out of date).

``` r
data("climateLocs2016")
names(climateLocs2016)
```

    ##  [1] "Name"                        "Province"                   
    ##  [3] "Climate ID"                  "Station ID"                 
    ##  [5] "WMO ID"                      "TC ID"                      
    ##  [7] "Latitude (Decimal Degrees)"  "Longitude (Decimal Degrees)"
    ##  [9] "Latitude"                    "Longitude"                  
    ## [11] "Elevation (m)"               "First Year"                 
    ## [13] "Last Year"                   "HLY First Year"             
    ## [15] "HLY Last Year"               "DLY First Year"             
    ## [17] "DLY Last Year"               "MLY First Year"             
    ## [19] "MLY Last Year"

Downloading data
----------------

Downloading data is accomplished using the `getClimateData()` function, or if you'd like something less fancy, the `getClimateDataRaw()` function. There is documentation in the package for both, but `getClimateData()` has all the bells and whistles, so I will go over its usage first. You will first need a `stationID` (or a vector of them) - in our case I'll use the one for Chelsea, QC, because I like [the ice cream there](http://www.lacigaleicecream.ca/).

``` r
df <- getClimateData(5585, timeframe="daily", year=2015)
str(df)
```

    ## 'data.frame':    365 obs. of  29 variables:
    ##  $ stationID                : num  5585 5585 5585 5585 5585 ...
    ##  $ Date/Time                : chr  "2015-01-01" "2015-01-02" "2015-01-03" "2015-01-04" ...
    ##  $ Year                     : int  2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 ...
    ##  $ Month                    : int  1 1 1 1 1 1 1 1 1 1 ...
    ##  $ Day                      : int  1 2 3 4 5 6 7 8 9 10 ...
    ##  $ Data Quality             : logi  NA NA NA NA NA NA ...
    ##  $ Max Temp (°C)            : num  NA NA NA NA -16.5 NA NA -10.5 -6.5 NA ...
    ##  $ Max Temp Flag            : chr  "M" "M" "M" "M" ...
    ##  $ Min Temp (°C)            : num  NA NA NA NA NA -26 -24.5 -31.5 -20.5 NA ...
    ##  $ Min Temp Flag            : chr  "M" "M" "M" "M" ...
    ##  $ Mean Temp (°C)           : num  NA NA NA NA NA NA NA -21 -13.5 NA ...
    ##  $ Mean Temp Flag           : chr  "M" "M" "M" "M" ...
    ##  $ Heat Deg Days (°C)       : num  NA NA NA NA NA NA NA 39 31.5 NA ...
    ##  $ Heat Deg Days Flag       : chr  "M" "M" "M" "M" ...
    ##  $ Cool Deg Days (°C)       : num  NA NA NA NA NA NA NA 0 0 NA ...
    ##  $ Cool Deg Days Flag       : chr  "M" "M" "M" "M" ...
    ##  $ Total Rain (mm)          : num  NA NA NA NA 0 0 0 0 0 0 ...
    ##  $ Total Rain Flag          : chr  "M" "M" "M" "M" ...
    ##  $ Total Snow (cm)          : num  NA NA NA NA 0.3 6.2 0.5 2.6 0 NA ...
    ##  $ Total Snow Flag          : chr  "M" "M" "M" "M" ...
    ##  $ Total Precip (mm)        : num  NA NA NA NA 0.3 6.2 0.5 2.6 0 NA ...
    ##  $ Total Precip Flag        : chr  "M" "M" "M" "M" ...
    ##  $ Snow on Grnd (cm)        : int  NA NA NA NA 14 14 18 16 17 17 ...
    ##  $ Snow on Grnd Flag        : chr  "M" "M" "M" "M" ...
    ##  $ Dir of Max Gust (10s deg): logi  NA NA NA NA NA NA ...
    ##  $ Dir of Max Gust Flag     : logi  NA NA NA NA NA NA ...
    ##  $ Spd of Max Gust (km/h)   : logi  NA NA NA NA NA NA ...
    ##  $ Spd of Max Gust Flag     : logi  NA NA NA NA NA NA ...
    ##  $ parsedDate               : POSIXct, format: "2015-01-01" "2015-01-02" ...

Boom! Data! The package can also melt the data for you (à la [reshape2](https://cran.r-project.org/package=reshape2)) so that you can easily use [ggplot](https://cran.r-project.org/package=ggplot2) to visualize.

``` r
library(ggplot2)
df <- getClimateData(5585, timeframe="daily", year=2015, format="long")
ggplot(df, aes(parsedDate, value)) + geom_line() + 
  facet_wrap(~param, scales="free_y")
```

![](/wp-content/uploads/2016/12/unnamed-chunk-6-1.png)<!-- -->

The function can accept a vector for most of the parameters, which it uses to either download multiple files or to trim the output, depending on the parameter. How to Chelsea, QC and Kentville, NS stack up during the month of November (Pretty similar, as it turns out...)?

``` r
df <- getClimateData(c(5585, 27141), timeframe="daily", year=2015, month=11, format="long")
ggplot(df, aes(parsedDate, value, col=factor(stationID))) + 
  geom_line() + facet_wrap(~param, scales="free_y")
```

![](/wp-content/uploads/2016/12/unnamed-chunk-7-1.png)<!-- -->

You will also notice that a little folder called `ec.cache` has popped up in your working directory, which contains the cached files that were downloaded from the Environment Canada site. You can disable this by passing `cache=NULL`, but I don't suggest it, since the cache will speed up running the code again (not to mention saving Environment Canada's servers) should you make a mistake the first time.

This function can download a whole lot of data, so it's worth doing a little math for yourself before you overwhelm your computer with data that it can't all load into memory. As an example, I tested this function by downloading daily data for every station in Nova Scotia between 1900 and 2016, which took 2 hours, nearly crashed my compute,r and resulted in a 1.3 **gigabyte** data.frame. You can do a few things (like ensure `checkdate=TRUE` and, if you're using `format="long"`, `rm.na=T`) to make your output a little smaller, or you can pass `ply=plyr::a_ply` to just cache the files so you only have to download them the once.

A little on how it works
------------------------

The code behind this package is [available on GitHub](http://github.com/paleolimbot/rclimateca), but it is fairly extensive and designed to tackle all of the corner cases that make writing a package so much more difficult than a script that runs once. Essentially, it's very close to the solution posted on [From the Bottom of the Heap](http://www.fromthebottomoftheheap.net/2015/01/14/harvesting-canadian-climate-data/) and in the [documentation itself](ftp://client_climate@ftp.tor.ec.gc.ca/Pub/Get_More_Data_Plus_de_donnees/Readme.txt). From the documentation:

    for year in `seq 1998 2008`;do for month in `seq 1 12`;do wget --content-disposition "http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&stationID=1706&Year=${year}&Month=${month}&Day=14&timeframe=1&submit= Download+Data" ;done;done

    WHERE; 
     year = change values in command line (`seq 1998 2008)
     month = change values in command line (`seq 1 12)
     format= [csv|xml]: the format output
     timeframe = 1: for hourly data 
     timeframe = 2: for daily data 
     timeframe = 3 for monthly data 
     Day: the value of the "day" variable is not used and can be an arbitrary value 
     For another station, change the value of the variable stationID
     For the data in XML format, change the value of the variable format to xml in the URL. 

This is, of course, the same as the `genURLS()` solution except for bash and wget instead of for R. This package uses the "format=csv" option, which produces a somewhat malformed CSV (there is quite a bit of header information). The "guts" of the read operation are as follows:

``` r
# download the file
library(httr)
url <- "http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&stationID=1706&Year=2008&Month=5&Day=14&timeframe=1&submit=Download+Data"
connect <- GET(url)
x <- content(connect, as="text", encoding="UTF-8")

# find the second emtpy line and start the read.csv after that line
xlines <- readLines(textConnection(x))
empty <- which(nchar(xlines) == 0)
empty <- empty[empty != length(xlines)]
# read the data frame
df <- read.csv(textConnection(x), 
               skip=empty[length(empty)], 
               stringsAsFactors = F, check.names = F)
```

I am sure is cringing as I read the entirity of each CSV file into memory, but they should be reminded that (a) R is terrible for memory usage anyway, and (b) the files are already "chunked up" by Environment Canada so that there is no chance of getting an unwieldy-sized file by accident.

Wrapping it up
--------------

That's it! Hopefully now you can all download unlimited quantities of data in pure bliss (as long as Environment Canada keeps its URLs consistent).
[/markdown]
]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1198</wp:post_id>
		<wp:post_date><![CDATA[2016-12-06 17:41:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-12-06 21:41:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[bulk-downloading-of-environment-canada-climate-data]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="environment-canada"><![CDATA[environment canada]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="rclimateca"><![CDATA[rclimateca]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Modeling sediment deformation using R &amp; RMarkdown		</title>
		<link>https://apps.fishandwhistle.net/archives/1225</link>
		<pubDate>Fri, 17 Feb 2017 15:29:10 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1225</guid>
		<description></description>
		<content:encoded>
				<![CDATA[Last summer I came home from a hike one day with way too much thesis work to do (my submission date was, I think, close to a week away). As per usual, I stopped by the <a href="http://kcirvingcentre.acadiau.ca/welcome.html">K.C. Irving Centre</a> to catch up on work, and promptly sat down and started writing what would become my <a href="http://link.springer.com/article/10.1007%2Fs10933-017-9947-1">latest publication in the Journal of Paleolimnology</a>. The article used R to apply mathematical functions describing how sediment is deformed when cored, and for the first time in my short career of academic writing, I wrote the paper in <a href="http://rmarkdown.rstudio.com/">RMarkdown</a>. At a certain point in the process of finalizing the publication it was necessary to switch to Word, but for the most part, the writing and coding was written in an interleaved fashion that allowed tweaking various parameters and, in seconds, viewing the resulting publication as a PDF. I have since taken to preparing most of my draft reports (and assignments that require coding) in this way.

[caption id="attachment_1226" align="alignleft" width="300"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2017/02/10933_2017_9947_Fig2_HTML-300x231.gif" alt="" width="300" height="231" class="size-medium wp-image-1226" /> This figure was, by far, the most difficult to prepare, even if it was the only one that wasn't done in R.[/caption]
]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1225</wp:post_id>
		<wp:post_date><![CDATA[2017-02-17 11:29:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-02-17 15:29:10]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[modeling-sediment-deformation-using-r-rmarkdown]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="academia"><![CDATA[Academia]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Programmer-friendly names come to rclimateca!		</title>
		<link>https://apps.fishandwhistle.net/archives/1229</link>
		<pubDate>Mon, 27 Feb 2017 23:57:02 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1229</guid>
		<description></description>
		<content:encoded>
				<![CDATA[The most frustrating thing about working with the previous version of the <a href="https://cran.r-project.org/package=rclimateca">rclimteca package</a> is the column headers given by Environment Canada. In R it is possible to refer to these columns using the backticks or the double bracket, but to save on typing I introduced a predictable column renaming function that removes the hard-to-type and upper-case characters so that column names that were previously something like "Max Temp (°C)" or "Latitude (Decimal Degrees)" become "maxtemp" and "latitude". The implementation of the <a href="https://cran.r-project.org/package=testthat">testthat package</a> for automated testing highlighted a number of bugs that were discovered and fixed, in addition to integration with the new <a href="https://cran.r-project.org/package=mudata">mudata package</a> (more to come on that in a few weeks!). Give it a shot!

[markdown]
```r
# get nice names for climate data (wide or long)
df <- getClimateData(5585, timeframe="daily", year=2015, nicenames=TRUE)
# get nice names for climate sites
sites <- getClimateSites("gatineau QC", year=2014:2016, nicenames=TRUE)
```
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1229</wp:post_id>
		<wp:post_date><![CDATA[2017-02-27 19:57:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-02-27 23:57:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[programmer-friendly-names-come-to-rclimateca]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="rclimateca"><![CDATA[rclimateca]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				The Mineralogical Paragram		</title>
		<link>https://apps.fishandwhistle.net/archives/1239</link>
		<pubDate>Sun, 19 Mar 2017 23:26:15 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1239</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]

Matt Hall from [Agile Geoscience](https://agilescientific.com/) recently wrote a [post](https://agilescientific.com/blog/2017/3/16/the-quick-green-forsterite) on the problem of finding the shortest possible pangram (sentence containing all letters in the alphabet) using only [mineral names](https://github.com/softwareunderground/undersampledradio/blob/master/data/IMA_mineral_names.txt). The post goes into the details on the [set cover problem](https://en.wikipedia.org/wiki/Set_cover_problem), of which assembling a pangram from a list of minerals is one example. Matt's best solution, "quartz kvanefjeldite abswurmbachite pyroxmangite", contained 45 characters and four mineral names, and its timing coincided with a weekend where my other options were to proofread a 50-page report or do my taxes. Trying to beat 45 characters seemed using weighted random sampling seemed like a much better use of a Saturday afternoon.

One approach (see [Agile's notebook](https://github.com/softwareunderground/undersampledradio/blob/master/Mineral_name_pangrams.ipynb)) would be to iterate through all possible combinations of mineral names until a pangram is found. This assumes the number of mineral names needed to form the shortest pangram is known, and that the search can be performed in such an order that the shortest combinations float to the top of the list, since iterating through every combination of 4 mineral names is a decades-long endeavour (and longer if 5 names are required).

Another approach would be to assemble pangram randomly on a name-by-name basis, keeping only the shortest. One way to do this might be to pick minerals that are the most probable to result in a short name that covers the whole alphabet. This might mean picking the first mineral name that is short, doesn't repeat letters, and contains less frequent letters (such as 'quartz'). I've done this in R (with packages dplyr and ggplot2), since my random sampling in Python is a bit rusty. First, we need to load the data.

``` r
# load the mineral names, letters of the alphabet
minnames <- readLines("mineralnames.txt")
chars <- strsplit("abcdefghijklmnopqrstuvwxyz", "")[[1]]
```

Second, we need to define a 'coverage' function that returns the number of unique letters based on the input.

``` r
alphacoverage <- function(chars) {
  # strip anything that isn't a-z from the lowercase input
  splits <- strsplit(gsub("[^a-z]", "", tolower(chars)), "")
  vapply(splits, function(x) length(unique(x)), integer(1))
}

alphacoverage("the Quick brown fox jumps over the lazy doG.")
```

    ## [1] 26

To use weighted random sampling, we need to quantify a couple of parameters that could be useful for probability weighting.

``` r
# create a data frame with information about each mineral
minerals <- data.frame(name=minnames, 
                       coverage=alphacoverage(minnames), 
                       stringsAsFactors = FALSE)

# create a matrix of letter coverage
lettercoverage <- vapply(chars, 
                         function(char) grepl(char, minnames), 
                         logical(3912))
colnames(lettercoverage) <- chars
rownames(lettercoverage) <- minnames

# calculate a letter 'score', with the rarest letters the highest
lettersums <- colSums(lettercoverage)
letterscore <- max(lettersums) / lettersums

# plot letter scores
data.frame(letter=chars, score=letterscore) %>%
  ggplot(aes(letter, score)) + geom_bar(stat="identity")
```
[/markdown]

[caption id="attachment_1240" align="alignnone" width="672"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2017/03/unnamed-chunk-3-1.png" alt="" width="672" height="480" class="size-full wp-image-1240" /> New game: mineralogical scrabble. Fun for the whole family! Your friends will all love you.[/caption]

[markdown]

For quantifying good words to include, the idea of "word score", "coverage density", and "score density" (or word score for unique letters divided by string length) might be useful in weighting random sampling:

``` r
# coverage density: coverage / length
minerals$coverage_density <- minerals$coverage / nchar(minerals$name)
# calculate scores for minerals: unique letters times the 'usefulness' of the letters
minerals$score <- vapply(minnames, function(name) {
  sum(letterscore[unique(strsplit(gsub("[^a-z]", 
                                       "", tolower(name)), 
                                  "")[[1]])])
}, numeric(1))
# calculate score density for minerals: score / length of word
minerals$score_density <- minerals$score / nchar(minerals$name)
head(arrange(minerals, desc(score_density)))
```

| name        |  coverage|  coverage\_density|   score|  score\_density|
|:------------|---------:|------------------:|-------:|---------------:|
| quartz      |         6|               1.00|  121.42|           20.24|
| naquite     |         7|               1.00|  105.38|           15.05|
| taseqite    |         6|               0.75|  101.83|           12.73|
| queitite    |         5|               0.62|  101.69|           12.71|
| quijarroite |         9|               0.82|  139.29|           12.66|
| qusongite   |         9|               1.00|  113.88|           12.65|

Unsurprisingly, 'quartz' tops the list by far (has a q and a z and repeats no letters). A function to describe the dissimilarity in letter coverage between two mineral names might also be useful:

``` r
name_dissimilarity <- function(name1, name2) {
  letters1 <- strsplit(name1, "")[[1]]
  letters2 <- strsplit(name2, "")[[1]]
  sum(xor(chars %in% letters1, chars %in% letters2))
}
# the length of the symmetric difference:
# "r", "z", "n", "i", "e"
name_dissimilarity("quartz", "naquite") 
```

    ## [1] 5

Finally, we need a function to assemble an arbitrary number of mineral names to form a pangram. The first word we'll choose based on the "score density" we calculated above, and after that, pick each mineral name (up to 20) based on the mineral names that cover the greatest number of missing letters, randomly sampling ties based on the length of the mineral name.

``` r
assemble_pangram <- function(maxwords=20, seed=NULL) {
  if(is.null(seed)) {
    # randomly pick the first word weighted by score density
    name <- minnames[sample(length(minnames), size=1, 
                            prob=minerals$score_density)]
  } else {
    # use the seed as the first name(s)
    name <- seed
  }
  
  words <- length(name)
  while(words < maxwords) {
    # only use mineral names that aren't already included
    minnames2 <- minnames[!(minnames %in% name)]
    # calculate coverage dissimilarity
    diffs <- mapply(name_dissimilarity, paste(name, collapse=""),
                    minnames2)
    # select only those names with maximum dissimilarity
    minnames2 <- minnames2[diffs == max(diffs)]
    # add new name to the list, randomly picking ties weighted by 2/n chars
    chars <- nchar(minnames2)
    name <- c(name, minnames2[sample(length(minnames2), size=1, 
                                     prob=2/(chars/max(chars)))])
    # if it covers all 26 letters, return the names
    if(alphacoverage(paste(name, collapse = " ")) == 26) return(name)
    words <- words + 1
  }
  # if nothing after maxwords, return NULL
  NULL
}
assemble_paragram()
```

    ## [1] "grechishchevite" "hexamolybdenum"  "witzkeite"       "jeppeite"       
    ## [5] "tin"             "hafnon"          "ice"             "queitite"

The result, of course, isn't always short, but *is* always a pangram. Sample enough times (40000, for the purposes of this post), and some short names should start to pop up (I know, using `plyr` to loop and the superassignment operator to modify isn't the best form, but it displays a helpful progress bar...).

``` r
set.seed(1500) # for replicability
shortest <- character(0)
plyr::a_ply(1:40000, 1, .fun=function(i) { # about 8 hours
  result <- paste(assemble_pangram(), collapse=" ")
  shortest <- shortest[!is.na(shortest)]
  if(!is.null(result) && !(result %in% shortest)) {
    # keep the 100 best at all times
    shortest <- c(result, shortest[!is.na(shortest)])
    shortest <<- shortest[order(nchar(gsub(" ", "", shortest)), 
                                na.last = TRUE)][1:100]
  }
}, .progress = "time")
# write results to disk
write(shortest, "panagram_psample.txt")
# display results
shortest[1:10]
```

    ##  [1] "johnwalkite gypsum quartz fedotovite ice blixite" 
    ##  [2] "arhbarite gypsum kvanefjeldite wilcoxite quartz"  
    ##  [3] "makovickyite sulphur xifengite jedwabite quartz"  
    ##  [4] "jeppeite hexamolybdenum wicksite fivegite quartz" 
    ##  [5] "kvanefjeldite gypsum schorl tewite blixite quartz"
    ##  [6] "pyroxmangite fukuchilite jedwabite sveite quartz" 
    ##  [7] "fukuchilite pyroxmangite jedwabite ivsite quartz" 
    ##  [8] "fukuchilite pyroxmangite jedwabite sveite quartz" 
    ##  [9] "wicksite hexamolybdenum fivegite quartz jeppeite" 
    ## [10] "jeppeite hexamolybdenum wicksite quartz fivegite"

It looks as though the [best I can get in 8 hours](/wp-content/uploads/2017/03/panagram_psample.txt) (overnight) is 43 characters, which is a tie between the first three listed above. The solution "makovickyite sulphur xifengite jedwabite quartz" also popped up in a previous trial run of the above, which suggests this list is fairly stable. Interestingly, even though quartz and gypsum show up in nearly all the solutions, using them as the first one (or two) mineral name(s) instead of randomly selecting it inhibits a random search of the solution space, since fewer options are considered for random selection. That said, many names show up more frequently than others in the top 100, and they aren't the same as the order of the "score density" used to weight the sampling of the first mineral name.

``` r
names <- unlist(strsplit(shortest, " "), use.names = FALSE)
namesdf <- data.frame(name=names, stringsAsFactors = FALSE) %>%
  group_by(name) %>%
  summarise(count=length(name)) %>%
  filter(count > 2) %>%
  arrange(desc(count))

# arrange names
namesdf$name <- factor(namesdf$name, levels=rev(namesdf$name))

ggplot(namesdf, aes(name, count)) + 
  geom_bar(stat="identity") +
  coord_flip()
```

![](/wp-content/uploads/2017/03/unnamed-chunk-10-1.png)

A few ideas for improving the search:

* Removing minerals with duplicated lettersets might speed things up. A cursory examination of `paste(sort(unique(x)), collapse="")` from the output of `strsplit()` suggests that this would reduce the number of minerals to sift through each step from 3912 to 3187. 
* Learn from the past: use mineral names common in previously short pangram to inform subsequent attempts.
* Expand random search of names for names other than the first. Once the first name is picked, there is not much room for random search, since the next name is the name that adds the most letters to the result. There may be a better metric to weight than purely name dissimilarity.

Perhaps when I'm done my Ph.D. thesis, proofreading all the reports, and finished my taxes, some time will pop up to solve this pressing issue once and for all.
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1239</wp:post_id>
		<wp:post_date><![CDATA[2017-03-19 20:26:15]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-03-19 23:26:15]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[the-mineralogical-paragram]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="general"><![CDATA[General]]></category>
		<category domain="post_tag" nicename="geology"><![CDATA[geology]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Line-by-line, row-by-row...		</title>
		<link>https://apps.fishandwhistle.net/archives/1258</link>
		<pubDate>Mon, 27 Mar 2017 18:19:48 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1258</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]

The [NatChem database](https://www.ec.gc.ca/natchem/default.asp?lang=En&n=90EDB4BC-1) from [Environment Canada](https://www.ec.gc.ca/) contains the best long-term atmospheric monitoring data that exists for Canada, similar to the National Atmospheric Deposition Program (NADP) in the US. Unlike the NADP, the distribution format associated with NatChem data is a hideous export format that looks like it was used by SAS at one point.

``` r
readLines("natchem_sample.CSV", n=6)
```

    ## [1] "*DATA EXCHANGE STANDARD VERSION,NATCHEM PRECIP 2003/01/17 (1.01),,,,,,,,,"                                                                                  
    ## [2] "*COMMENT,For information on this file please see the NAtChem web site http://www.ec.gc.ca/natchem/"                                                         
    ## [3] "*COMMENT,Network descriptions and links to home pages are provided on the NAtChem web site."                                                                
    ## [4] "*COMMENT,Data Originators or NAtChem Staff may occasionally revise data files. Users should download the latest version of each file before using the data."
    ## [5] "*COMMENT,The flags and comments provided with the original data have been mapped to the NAtChem standard flags."                                            
    ## [6] "*COMMENT,Please contact NAtChem if you want DES files with the original network flags and comments."

If an easy way to import this data to R exists, I can't find it. In particular, `read.csv()` produces ugly results, and using Excel to remove extraneous information is both time-consuming and results in significant data loss (since an Excel user is unlikely to keep other tables that may exist other than the data itself). So instead, with an eye towards including a driver for this data type in the [rclimateca package](https://cran.r-project.org/package=readr), I decided to look into line-by-line parsing in R.

### Reading files line-by-line in R

While other languages (Python in particular) make it easy to iterate through files by line, this strategy has never been popular in R. There is, in large part, no need to read a file line-by-line, in large part due to the excellent `read.csv()` and `read.delim()` functions (plus the [readr](https://cran.r-project.org/package=readr) package for even more). An exception to this is data distributed in non-standard formats, such as CSVs with lots of header information. The NatChem format probably contains the most header information that I have ever seen in a CSV (even compared with Environment Canada's historical climate data distribution format, which has a large number of extraneous rows containing location, parameter, and flag information), which means to automatically parse these files we need the line-by-line strategy.

For small files, it's possible to use `readLines()` with the filename as the first argument, then iterate through each line using a `for` loop (`readLines()` returns a character vector with one element per line). This strategy gets more difficult when the files get large, and is especially inefficient if most of the information in the file will be disregarded anyway. Still, it's incredibly easy, and worth demonstrating.

``` r
lines <- readLines("natchem_sample.CSV")
head(lines)
```

    ## [1] "*DATA EXCHANGE STANDARD VERSION,NATCHEM PRECIP 2003/01/17 (1.01),,,,,,,,,"                                                                                  
    ## [2] "*COMMENT,For information on this file please see the NAtChem web site http://www.ec.gc.ca/natchem/"                                                         
    ## [3] "*COMMENT,Network descriptions and links to home pages are provided on the NAtChem web site."                                                                
    ## [4] "*COMMENT,Data Originators or NAtChem Staff may occasionally revise data files. Users should download the latest version of each file before using the data."
    ## [5] "*COMMENT,The flags and comments provided with the original data have been mapped to the NAtChem standard flags."                                            
    ## [6] "*COMMENT,Please contact NAtChem if you want DES files with the original network flags and comments."

The more "Python-esque" method is also simple, but not intuitive to R users because it involves an object with a mutable state. Most objects in R are copied whenever they are modified, or more specifically, calling a function with an object as an argument almost never results in that object being modified in the calling environment. This is fairly useful in most cases, but reading a file is not one of those cases. Enter one of the few natively mutable objects in R: the "file connection".

``` r
connection <- file("natchem_sample.CSV")
open(connection)
for(i in 1:6) {
  print(readLines(connection, n=1))
}
```

    ## [1] "*DATA EXCHANGE STANDARD VERSION,NATCHEM PRECIP 2003/01/17 (1.01),,,,,,,,,"
    ## [1] "*COMMENT,For information on this file please see the NAtChem web site http://www.ec.gc.ca/natchem/"
    ## [1] "*COMMENT,Network descriptions and links to home pages are provided on the NAtChem web site."
    ## [1] "*COMMENT,Data Originators or NAtChem Staff may occasionally revise data files. Users should download the latest version of each file before using the data."
    ## [1] "*COMMENT,The flags and comments provided with the original data have been mapped to the NAtChem standard flags."
    ## [1] "*COMMENT,Please contact NAtChem if you want DES files with the original network flags and comments."

``` r
close(connection)
```

The strategy is fairly simple: open a "file connection" to the file, and call `readLines()` to read a line and advance the pointer. **Always `close()` the connection when you are finished!** If you don't, R will yell at you at some point telling you that you never closed the connection. Notice that the `connection` object is modified when passed to `open()`, `readLines()` and `close()`, whereas in "normal" R code you would have to do something like `connection <- open(connection)` to acheive a similar result. Language details aside, this approach avoids the possible danger of loading a huge file into memory by accident (although if the file does not have any line endings, this is still a problem).

So far the above strategy works if we know we have to read a fixed number of lines, but in most cases the number of lines in the file is unknown. After some experimentation, it seems that `readLines()` will return `character(0)` when there are no more lines in the file (not to be confused with an empty string `""`, which is an empty line). We can use this in conjunction with a `while` loop to loop through every line in the file.

``` r
connection <- file("natchem_sample.CSV")
open(connection)
line <- readLines(connection, n=1)
while(length(line) > 0) {
  # do something with each line
  # ...
  # read a new line
  line <- readLines(connection, n=1)
}
close(connection)
```

### Processing the lines

So far we can iterate through lines, but what's the point? In this case, we need to exctract specific information about the contents of the files. To this end, there are two (among others) useful functions: `scan()` and `grepl()`. The `scan()` function was built to parse delimited text, which covers most of the things that are read in a line-by-line strategy (such as NatChem).

``` r
connection <- file("natchem_sample.CSV")
open(connection)
first_line <- readLines(connection, n=1)
line1_elements <- scan(text=first_line, what = character(1), sep=",")
close(connection)

line1_elements
```

    ##  [1] "*DATA EXCHANGE STANDARD VERSION"  "NATCHEM PRECIP 2003/01/17 (1.01)"
    ##  [3] ""                                 ""                                
    ##  [5] ""                                 ""                                
    ##  [7] ""                                 ""                                
    ##  [9] ""                                 ""                                
    ## [11] ""

There are a number of arguments to `scan()` that control what it's looking for and how it will find it, although the only ones I've personally used are `quiet=TRUE` (which suppresses the message that reports how many items have been read) and `strip.white=TRUE` (which strips whitespace). Many of these are also options in `read.csv()`, although if you're making use of them it's a good sign that you should probably be using `read.csv()` or something similar to read your data.

The second thing that is useful is searching for specific text in a line (for example, a line that may suggest that table information exists in a specific loction). In the NatChem format, there are lines that read `*TABLE BEGINS` and `*TABLE ENDS` at the beginning and end of table data, respectively. Thus, we can use `grepl()` to search for lines that contain these strings to delineate where tables begin and end.

``` r
connection <- file("natchem_sample.CSV")

table_start <- integer(0)
table_end <- integer(0)

open(connection)
line <- readLines(connection, n=1)
line_number <- 1L
while(length(line) > 0) {
  # check for table begin/end
  if(grepl("*TABLE BEGINS", line, fixed = TRUE)) {
    table_start <- c(table_start, line_number)
  } else if(grepl("*TABLE ENDS", line, fixed = TRUE)) {
    table_end <- c(table_end, line_number)
  }
  
  # read a new line (and increment line number)
  line <- readLines(connection, n=1)
  line_number <- line_number + 1
}
close(connection)

rbind(table_start, table_end)
```

    ##             [,1] [,2] [,3] [,4] [,5]
    ## table_start   35   60   75   98  120
    ## table_end     51   65   85  101  189

It's worth noting that `grepl()` takes its arguments in reversed order to what you would expect based on other vectorized R functions (that is, `grepl("what you're looking for", where_to_look))`. By default, the search term in `grepl()` is a regular expression, which you can disable by using `fixed=TRUE`. Unless you actually want to use a [regular expression](https://en.wikipedia.org/wiki/Regular_expression), it's best to use `fixed=TRUE` since many common characters are reserved in regular expressions and may produce unexpected results. For a more consistent implementation of regular expression matching, see the [stringr](https://cran.r-project.org/package=stringr) package.

### Extracting the tables

Just extracting the start and end lines of the tables isn't particularly useful without the table name/column information data, which is also provided in NatChem files. These lines look like `*TABLE NAME,...` and `*TABLE COLUMN NAME,...,...,...`, respectively. Using these data, we can read a full specification of each table into a `list`.

``` r
connection <- file("natchem_sample.CSV")

# these will be updated as the file is read
table_start <- NA
table_end <- NA
table_name <- NA
table_columns <- NA

# this will contain the output
tables <- list()

open(connection)
line <- readLines(connection, n=1)
line_number <- 1L
while(length(line) > 0) {
  # check for table begin/end
  if(grepl("*TABLE BEGINS", line, fixed = TRUE)) {
    # update the table start line number
    table_start <- line_number
    
  } else if(grepl("*TABLE ENDS", line, fixed = TRUE)) {
    # update the table end line number
    table_end <- line_number
    
    # on table end, update 'tables' with a full specification
    tables[[length(tables)+1]] <- list(name=table_name, 
                                       columns=table_columns,
                                       start=table_start, 
                                       end=table_end)
    
  } else if(grepl("*TABLE NAME,", line, fixed = TRUE)) {
    # update the table name
    line_fields <- scan(text=line, sep=",", 
                        what = character(1), quiet = TRUE)
    table_name <- line_fields[2]
    
  } else if(grepl("*TABLE COLUMN NAME,", line, fixed = TRUE)) {
    # update the column names
    line_fields <- scan(text=line, sep=",", 
                        what = character(1), quiet = TRUE)
    table_columns <- line_fields[2:length(line_fields)]
  }
  
  # read a new line (and increment line number)
  line <- readLines(connection, n=1)
  line_number <- line_number + 1
}
close(connection)

tables[1]
```

    ## [[1]]
    ## [[1]]$name
    ## [1] "Network comments: office"
    ## 
    ## [[1]]$columns
    ##  [1] "Comment codes: office" "Description"          
    ##  [3] ""                      ""                     
    ##  [5] ""                      ""                     
    ##  [7] ""                      ""                     
    ##  [9] ""                      ""                     
    ## 
    ## [[1]]$start
    ## [1] 35
    ## 
    ## [[1]]$end
    ## [1] 51

Of course, there's still much room for improvement, but that's the gist of it. Overall, reading files line-by-line isn't usually what you want, but in my case of reading hundreds of NatChem files at a time, it is the most robust solution in a language that wasn't really built to handle it. The syntax needed to update variables without a fixed length is lacking in R, which makes writing code like the above awkward (in general it is terrible practice to use `c()` to update a vector, but if the output isn't of known length, there aren't any easy alternatives).

### Vectorized Solutions

There are some vectorized solutions to the above examples, that may be appropriate if the parsing is simple. For example, reading the line numbers of `*BEGIN TABLE` and `*END TABLE` is easy to do using `readLines()` and `grep()` (which is essentially `which(grepl(...))`).

``` r
lines <- readLines("natchem_sample.CSV")
rbind(grep("*TABLE BEGINS", lines, fixed = TRUE), 
      grep("*TABLE ENDS", lines, fixed = TRUE))
```

    ##      [,1] [,2] [,3] [,4] [,5]
    ## [1,]   35   60   75   98  120
    ## [2,]   51   65   85  101  189

Extracting more information gets more complicated, especially if some information is omitted. The above example breaks down very quickly if anything is malformed about the file, since the parsing will fail with an unhelpful error message if for some reason there is no `*TABLE ENDS` after table data.

### Performance

Normally looping in R is thought of as slow, but in this case, it's not as slow as you might expect. If we use `vapply()` to create a stripped-down version of the native `readLines()` and benchmark, it looks like the native `readLines()` is a little faster, but not enough to be noticable unless the files start to get huge (in which case a line-by-line strategy may end up faster due to reduced memory allocation). With variable-length output (i.e. using `c()` to create variable length output vectors), it's likely that looping will get progressively slower with longer files. Still, if there are only a few lines that need extracting, it may be worth it.

``` r
readLines_R <- function(csvfile, n=6) {
  connection <- file(csvfile)
  open(connection)
  lines <- vapply(1:n, function(i) readLines(connection, n=1), 
                  character(1))
  close(connection)
  return(lines)
}

library(microbenchmark)
microbenchmark(readLines("natchem_sample.CSV", n=6), 
               readLines_R("natchem_sample.CSV", n=6))
```

    ## Unit: microseconds
    ##                                      expr     min      lq      mean
    ##    readLines("natchem_sample.CSV", n = 6)  85.383  86.772  90.44636
    ##  readLines_R("natchem_sample.CSV", n = 6) 113.100 115.400 120.09061
    ##    median      uq     max neval
    ##   87.6335  89.716 180.586   100
    ##  116.5595 121.016 162.676   100

### Parsing NatChem

The full parsing of the NatChem format will be available soon though the [rclimateca](https://cran.r-project.org/package=rclimateca) package, along with (hopefully) an extractor for HYDAT hydrological data (which is, as you may have guessed, in a completely different format).

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1258</wp:post_id>
		<wp:post_date><![CDATA[2017-03-27 15:19:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-03-27 18:19:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[line-by-line-row-by-row]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="parsing"><![CDATA[parsing]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="text-files"><![CDATA[text files]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Major ggspatial updates		</title>
		<link>https://apps.fishandwhistle.net/archives/1260</link>
		<pubDate>Wed, 29 Mar 2017 00:41:37 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1260</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
I took the day to update [ggspatial](https://github.com/paleolimbot/ggspatial) an old R project that was never published to CRAN. The idea is to create [ggplot](https://github.com/tidyverse/ggplot2) `layer()` calls from [sp](https://github.com/edzer/sp) Spatial* objects using a consistent interface. Last year I wrote a blog post about how that might work, and while the usage hasn't changed much, the implementation is now much, much, much, (much!) cleaner, and a few things have been removed that were never really going to work out. One of those is the `geom_osm()` function, which needs some serious soul searching given better options (like [ggmap](https://github.com/dkahle/ggmap) and [rosm](https://github.com/paleolimbot/ggspatial)).

In the meantime, you can do some pretty seriously cool work with vector layers, including aesthetic mapping, facets, multiple projections, and more! Check it out!

``` r
ggspatial(longlake_waterdf, fill="lightblue") +
   geom_spatial(longlake_marshdf, fill="grey", alpha=0.5) +
   geom_spatial(longlake_streamsdf, col="lightblue") +
  coord_map()
```

![](/wp-content/uploads/2017/03/unnamed-chunk-7-1.png)

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1260</wp:post_id>
		<wp:post_date><![CDATA[2017-03-28 21:41:37]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-03-29 00:41:37]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[major-ggspatial-updates]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ggplot"><![CDATA[ggplot]]></category>
		<category domain="post_tag" nicename="ggspatial"><![CDATA[ggspatial]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Mapping package updates		</title>
		<link>https://apps.fishandwhistle.net/archives/1263</link>
		<pubDate>Sun, 02 Apr 2017 00:13:19 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1263</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
I've been in a bit of a programming haze for the last few days after discovering that the code I wrote a year ago was not working very well in the [rosm](https://github.com/paleolimbot/rosm) and [prettymapr](https://github.com/paleolimbot/prettymapr) packages. In addition, a project I had worked on a few months ago needed some work, and for some reason I decided that the last few days were the days to get things done. The READMEs for [rosm](https://github.com/paleolimbot/rosm) and [prettymapr](https://github.com/paleolimbot/prettymapr), and [ggspatial](https://github.com/paleolimbot/ggspatial) give all the details, but most of it can be summed up in the following few-liner:

```r
library(prettymapr)
library(ggspatial)

places <- geocode(c("halifax, ns", "moncton, nb", "yarmouth, ns", "wolfville, ns"))
ggplot(places, aes(lon, lat, shape = query)) + 
  geom_osm() + geom_point() + 
  coord_quickmap()
```
[/markdown]

<img src="http://apps.fishandwhistle.net/wp-content/uploads/2017/04/Rplot001.png" alt="" width="480" height="480" class="alignnone size-full wp-image-1264" />

It turns out extending geometries and stats in ggplot is actually quite complex, but it seems this particular iteration of ggspatial does the trick. Now back to the real world...]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1263</wp:post_id>
		<wp:post_date><![CDATA[2017-04-01 21:13:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-04-02 00:13:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[mapping-package-updates]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ggspatial"><![CDATA[ggspatial]]></category>
		<category domain="post_tag" nicename="gis"><![CDATA[gis]]></category>
		<category domain="post_tag" nicename="prettymapr"><![CDATA[prettymapr]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
		<category domain="post_tag" nicename="rosm"><![CDATA[rosm]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				An Alternative R Wrapper for PHREEQC Geochemical Modelling		</title>
		<link>https://apps.fishandwhistle.net/archives/1268</link>
		<pubDate>Mon, 01 May 2017 21:57:45 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1268</guid>
		<description></description>
		<content:encoded>
				<![CDATA[[markdown]
Recently I was introduced to [PHREEQC](https://wwwbrr.cr.usgs.gov/projects/GWC_coupled/phreeqc/), which is a program that models chemical interactions between rocks and water (among many, many other things). It is an open-source project by the USGS, and has a number of powerful features, including modeling equilibrium concentrations of elements according to various input parameters (e.g. temperature, pH). My task for the day (a few weeks ago) was to see if changing partial pressure of CO<sub>2</sub> could theoretically have any effect on Alkalinity in surface water. PHREEQC was the solution.

Installing and running PHREEQC is not trivial. There are binary distributions for Mac and Windows, but the method of running the program is somewhat archane: the user is required to provide an input file that looks something like this (this is a modified version of [Example 2](https://wwwbrr.cr.usgs.gov/projects/GWC_coupled/phreeqc/phreeqc3-html/phreeqc3-57.htm#50524175_28577) in the [Version 3 User's Guide](https://wwwbrr.cr.usgs.gov/projects/GWC_coupled/phreeqc/phreeqc3-html/phreeqc3.htm):

    TITLE Example 2.--Temperature dependence of solubility
                      of gypsum and anhydrite
    SOLUTION 1 Pure water
            pH      7.0
            temp    25.0                
    EQUILIBRIUM_PHASES 1
            Gypsum          0.0     1.0
            Anhydrite       0.0     1.0
    REACTION_TEMPERATURE 1
            25.0 75.0 in 51 steps
    SELECTED_OUTPUT
            -file   ex2.sel
            -temperature
            -si     anhydrite  gypsum
    END

The above code will calculate the saturation indicies of Gypsum and Anhydrite as they change with temperature (between 25 and 75 degrees), and save the results to a delimited file called `ex2.sel`. It takes some sleuthing in the documentation (which is quite good and very complete) to figure that out. Somebody has also made a [package for R](https://cran.r-project.org/package=phreeqc) that will install, run, and parse the results of PHREEQC to an R object.

``` r
library(phreeqc)
phrLoadDatabaseString(phreeqc.dat)
phrRunString(ex2)
so <- phrGetSelectedOutput()
head(so$n1)
```

|  sim| state   |  soln|  dist\_x|  time|  step|        pH|        pe|  temp.C.|  si\_anhydrite|  si\_gypsum|
|----:|:--------|-----:|--------:|-----:|-----:|---------:|---------:|--------:|--------------:|-----------:|
|    1| i\_soln |     1|       NA|    NA|    NA|  7.000000|   4.00000|       25|             NA|          NA|
|    1| react   |     1|       NA|     0|     1|  7.066148|  10.74441|       25|     -0.3030109|           0|
|    1| react   |     1|       NA|     0|     2|  7.052481|  10.67606|       26|     -0.2922587|           0|
|    1| react   |     1|       NA|     0|     3|  7.038914|  10.60756|       27|     -0.2815457|           0|
|    1| react   |     1|       NA|     0|     4|  7.025453|  10.53874|       28|     -0.2708716|           0|
|    1| react   |     1|       NA|     0|     5|  7.012103|  10.47159|       29|     -0.2602361|           0|

Here, `ex2` is a character vector (one element per line) of the input file, which is included in the package as data. Similarly, `phreeqc.dat` is a database that contains speciation and geochemical reaction information. There are a number of these databases included, which, being new to PHREEQC, I don't fully grasp other than that they contain different species, elements, and reactions.

What I would like to type is quite a bit simpler than creating a character vector with a preformed input file. What if creating a solution could be as easy as `solution(pH=7)`? I spent an afternoon with this, and ended up with a wrapper package that is called (preliminarily) [easyphreeqc](https://github.com/paleolimbot/easyphreeqc).

``` r
library(easyphreeqc)

# generate input
input <- solution(pH = 7, temp = 25) +
  equilibrium_phases(Gypsum = c(0, 1), Anhydrite = c(0, 1)) +
  reaction_temperature(low = 25, high = 75, steps = 51) +
  selected_output(temperature = TRUE, si = c('anhydrite', 'gypsum'))

# run phreeqc() with a character vector as input
output <- phreeqc(input)

# output is the first selected output as a data frame
head(output)
```

|  sim| state   |  soln|  dist\_x|  time|  step|        pH|        pe|  temp.C.|  si\_anhydrite|  si\_gypsum|
|----:|:--------|-----:|--------:|-----:|-----:|---------:|---------:|--------:|--------------:|-----------:|
|    1| i\_soln |     1|       NA|    NA|    NA|  7.000000|   4.00000|       25|             NA|          NA|
|    1| react   |     1|       NA|     0|     1|  7.066148|  10.74441|       25|     -0.3030109|           0|
|    1| react   |     1|       NA|     0|     2|  7.052481|  10.67606|       26|     -0.2922587|           0|
|    1| react   |     1|       NA|     0|     3|  7.038914|  10.60756|       27|     -0.2815457|           0|
|    1| react   |     1|       NA|     0|     4|  7.025453|  10.53874|       28|     -0.2708716|           0|
|    1| react   |     1|       NA|     0|     5|  7.012103|  10.47159|       29|     -0.2602361|           0|

The idea is to turn each section of the input file into a function, so that specifying it is more "R-like". If you have used [ggplot2](https://cran.r-project.org/package=ggplot2), you'll notice the syntax is very similar. Of course, you can use regular old `c()` to combine the output of `solution()` and related functions (they just return, for now, the equivalent character vector that you would read from a file specifying the same solution).

With simplification comes the loss of flexibility, but many of the components of PHREEQC that require that flexibility are features that are more convenient to implement in R (e.g. vectorization of input values such as temperature, or plotting output). Realistically, functions like `solution()` should have formal R arguments, rather than just pasting the input together line-by-line. In any case, it's unlikely I'll get back to this side-project until after field season, but if you are interested in contributing, the [GitHub repo](https://github.com/paleolimbot/easyphreeqc) has the source code available. You can install the package using `devtools::install_github('paleolimbot/easyphreeqc')`.
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1268</wp:post_id>
		<wp:post_date><![CDATA[2017-05-01 18:57:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-05-01 21:57:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[an-alternative-r-wrapper-for-phreeqc-geochemical-modelling]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="phreeqc"><![CDATA[PHREEQC]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				10,000 Years of Metal Dynamics in the Tantramar Marshes		</title>
		<link>https://apps.fishandwhistle.net/archives/1272</link>
		<pubDate>Sat, 27 May 2017 02:01:57 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1272</guid>
		<description></description>
		<content:encoded>
				<![CDATA[Almost six years ago, my honours thesis (<a href="http://openarchive.acadiau.ca/cdm/singleitem/collection/HTheses/id/719/rec/3">Using paleolimnological methods to track late holocene environmental change at Long Lake, New Brunswick – Nova Scotia border region, Canada</a>) and my friend Hilary's <a href="http://openarchive.acadiau.ca/cdm/singleitem/collection/Theses/id/645/rec/80">master's thesis</a> were completed, but it was only this year that we finally got around to submitting the combined manuscript to <a href="http://facetsjournal.com/">FACETS</a>, a new (ish) open access journal from Canadian Science Publishing. Hilary's thesis describes (among other things) a peak in mercury deposition at Long Lake, near Amherst, Nova Scotia, approximately 5,000 years ago. These data were combined with the more recent record from my honours thesis, and we are proud to announce that the resulting paper (<a href="http://facetsjournal.com/article/facets-2017-0004/">A paleolimnological archive of metal sequestration and release in the Cumberland Basin Marshes, Atlantic Canada</a>) was published on Tuesday. Special thanks to co-authors Ian, Mark, and Nelson, all of whom had a key role in prodding us to complete the manuscript.

[caption id="attachment_1273" align="alignnone" width="1020"]<img src="http://apps.fishandwhistle.net/wp-content/uploads/2017/05/facets-2017-0004-f3-standard.png" alt="" width="1020" height="694" class="size-full wp-image-1273" /> This diagram was redone at least four times, and probably represents the last time I will attempt to create plot graphics outside of R.[/caption]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1272</wp:post_id>
		<wp:post_date><![CDATA[2017-05-26 23:01:57]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-05-27 02:01:57]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[10000-years-of-metal-dynamics-in-the-tantramar-marshes]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="academia"><![CDATA[Academia]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Environment Canada HYDAT data in R		</title>
		<link>https://apps.fishandwhistle.net/archives/1279</link>
		<pubDate>Fri, 21 Jul 2017 14:54:24 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1279</guid>
		<description></description>
		<content:encoded>
				<![CDATA[The <a href="http://www.ec.gc.ca/rhc-wsc/default.asp?lang=en&n=9018B5EC-1">HYDAT database</a> contains over 1 GB of hydrological observation data collected by <a href="http://ec.gc.ca/">Environment Canada</a>. The database is freely available from the Environment Canada website, however extracting data can be difficult. Recently a student in our lab was working with this data, so I adapted some previously shotty code that cleaned up the somewhat difficult to navigate database into a <a href="https://github.com/paleolimbot/hydatr">nice unit-tested R package</a>. Not doing much work in the hydrology sector myself, I'd love some feedback on what is useful and not useful about the package! Some basic usage is demonstrated below.

[markdown]
Installation
------------

You can install hydatr from github with:

``` r
# install.packages("devtools")
devtools::install_github("paleolimbot/hydatr")
```

Finding Hydro Stations
----------------------

Find hydro sites:

``` r
hydat_find_stations("lower sackville, NS", year = 1999:2012)
#> # A tibble: 10 x 8
#>    STATION_NUMBER dist_from_query_km
#>             <chr>              <dbl>
#>  1        01EJ004          0.9861109
#>  2        01EJ001          5.1509276
#>  3        01DG003          8.8762890
#>  4        01DJ005         74.3590516
#>  5        01EF001         80.2094515
#>  6        01DC005        107.6006889
#>  7        01DP004        107.6301656
#>  8        01DL001        108.5261888
#>  9        01EE005        113.1515549
#> 10        01DC007        116.6305481
#> # ... with 6 more variables: STATION_NAME <chr>, FIRST_YEAR <int>,
#> #   LAST_YEAR <int>, LONGITUDE <dbl>, LATITUDE <dbl>,
#> #   DRAINAGE_AREA_GROSS <dbl>
```

Get detailed information about one hydro site:

``` r
as.list(hydat_station_info("01EJ004"))
#> $STATION_NUMBER
#> [1] "01EJ004"
#> 
#> $STATION_NAME
#> [1] "LITTLE SACKVILLE RIVER AT MIDDLE SACKVILLE"
#> 
#> $PROV_TERR_STATE_LOC
#> [1] "NS"
#> 
#> $LATITUDE
#> [1] 44.76447
#> 
#> $LONGITUDE
#> [1] -63.6875
#> 
#> $DRAINAGE_AREA_GROSS
#> [1] 13.1
#> 
#> $DRAINAGE_AREA_EFFECT
#> [1] NA
#> 
#> $STATUS_EN_HYD
#> [1] "Active"
#> 
#> $STATUS_EN_SED
#> [1] "Discontinued"
#> 
#> $REGIONAL_OFFICE_NAME_EN
#> [1] "DARTMOUTH"
#> 
#> $AGENCY_EN_CONTRIBUTOR
#> [1] "NOVA SCOTIA DEPARTMENT OF ENVIRONMENT"
#> 
#> $AGENCY_EN_OPERATOR
#> [1] "WATER SURVEY OF CANADA (DOE) (CANADA)"
#> 
#> $RHBN
#> [1] 0
#> 
#> $REAL_TIME
#> [1] 1
#> 
#> $DATUM_ID
#> [1] 10
#> 
#> $FIRST_YEAR
#> [1] 1980
#> 
#> $LAST_YEAR
#> [1] 2015
```

Get detailed information about all the hydro sites:

``` r
hydat_station_info()
#> # A tibble: 1,000 x 17
#>    STATION_NUMBER                                           STATION_NAME
#>             <chr>                                                  <chr>
#>  1        01AA002        DAAQUAM (RIVIERE) EN AVAL DE LA RIVIERE SHIDGEL
#>  2        01AD001      MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA
#>  3        01AD002                          SAINT JOHN RIVER AT FORT KENT
#>  4        01AD003            ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE
#>  5        01AD004                         SAINT JOHN RIVER AT EDMUNDSTON
#>  6        01AD005           MADAWASKA (RIVIERE) AU RESERVOIR TEMISCOUATA
#>  7        01AD008                         LONG (LAC) PRES DE LES ETROITS
#>  8        01AD009                           CABANO (RIVIERE) AU LAC LONG
#>  9        01AD012 SAINT-FRANCOIS (RIVIERE) EN AVAL DU LAC SAINT-FRANCOIS
#> 10        01AD013 SAINT-FRANCOIS (RIVIERE) EN AVAL DU LAC SAINT-FRANCOIS
#> # ... with 990 more rows, and 15 more variables:
#> #   PROV_TERR_STATE_LOC <chr>, LATITUDE <dbl>, LONGITUDE <dbl>,
#> #   DRAINAGE_AREA_GROSS <dbl>, DRAINAGE_AREA_EFFECT <dbl>,
#> #   STATUS_EN_HYD <chr>, STATUS_EN_SED <chr>,
#> #   REGIONAL_OFFICE_NAME_EN <chr>, AGENCY_EN_CONTRIBUTOR <chr>,
#> #   AGENCY_EN_OPERATOR <chr>, RHBN <int>, REAL_TIME <int>, DATUM_ID <int>,
#> #   FIRST_YEAR <int>, LAST_YEAR <int>
```

Extracting Data
---------------

The following methods extract data from the database given the correct station number:

``` r
hydat_flow_monthly("01AD001")
#> # A tibble: 880 x 9
#>    STATION_NUMBER                                      STATION_NAME  YEAR
#>             <chr>                                             <chr> <int>
#>  1        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1918
#>  2        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1918
#>  3        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1918
#>  4        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1918
#>  5        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#>  6        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#>  7        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#>  8        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#>  9        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#> 10        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#> # ... with 870 more rows, and 6 more variables: MONTH <int>, DATE <date>,
#> #   MONTHLY_MEAN <dbl>, MONTHLY_TOTAL <dbl>, DAILY_MIN <dbl>,
#> #   DAILY_MAX <dbl>
hydat_flow_daily("01AD001")
#> # A tibble: 26,785 x 8
#>    STATION_NUMBER                                      STATION_NAME  YEAR
#>             <chr>                                             <chr> <int>
#>  1        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1918
#>  2        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1918
#>  3        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1918
#>  4        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1918
#>  5        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#>  6        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#>  7        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#>  8        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#>  9        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#> 10        01AD001 MADAWASKA (RIVIER) EN AVAL DU BARRAGE TEMISCOUATA  1919
#> # ... with 26,775 more rows, and 5 more variables: MONTH <int>, DAY <int>,
#> #   DATE <date>, FLOW <dbl>, FLOW_SYMBOL <chr>
hydat_level_monthly("01AD003")
#> # A tibble: 60 x 9
#>    STATION_NUMBER                                STATION_NAME  YEAR MONTH
#>             <chr>                                       <chr> <int> <int>
#>  1        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     1
#>  2        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     2
#>  3        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     3
#>  4        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     4
#>  5        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     5
#>  6        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     6
#>  7        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     7
#>  8        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     8
#>  9        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     9
#> 10        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011    10
#> # ... with 50 more rows, and 5 more variables: DATE <date>,
#> #   MONTHLY_MEAN <dbl>, MONTHLY_TOTAL <dbl>, DAILY_MIN <dbl>,
#> #   DAILY_MAX <dbl>
hydat_level_daily("01AD003")
#> # A tibble: 1,826 x 8
#>    STATION_NUMBER                                STATION_NAME  YEAR MONTH
#>             <chr>                                       <chr> <int> <int>
#>  1        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     1
#>  2        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     2
#>  3        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     3
#>  4        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     4
#>  5        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     5
#>  6        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     6
#>  7        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     7
#>  8        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     8
#>  9        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011     9
#> 10        01AD003 ST. FRANCIS RIVER AT OUTLET OF GLASIER LAKE  2011    10
#> # ... with 1,816 more rows, and 4 more variables: DAY <int>, DATE <date>,
#> #   LEVEL <dbl>, LEVEL_SYMBOL <chr>
hydat_sed_monthly("01AF006")
#> # A tibble: 26 x 9
#>    STATION_NUMBER                           STATION_NAME  YEAR MONTH
#>             <chr>                                  <chr> <int> <int>
#>  1        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     4
#>  2        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     5
#>  3        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     6
#>  4        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     7
#>  5        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     8
#>  6        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     9
#>  7        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1972     4
#>  8        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1972     5
#>  9        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1972     6
#> 10        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1972     7
#> # ... with 16 more rows, and 5 more variables: DATE <date>,
#> #   MONTHLY_MEAN <dbl>, MONTHLY_TOTAL <dbl>, DAILY_MIN <dbl>,
#> #   DAILY_MAX <dbl>
hydat_sed_daily("01AF006")
#> # A tibble: 794 x 7
#>    STATION_NUMBER                           STATION_NAME  YEAR MONTH   DAY
#>             <chr>                                  <chr> <int> <int> <int>
#>  1        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     4     1
#>  2        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     5     1
#>  3        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     6     1
#>  4        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     7     1
#>  5        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     8     1
#>  6        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1971     9     1
#>  7        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1972     4     1
#>  8        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1972     5     1
#>  9        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1972     6     1
#> 10        01AF006 BLACK BROOK NEAR ST-ANDRE-DE-MADAWASKA  1972     7     1
#> # ... with 784 more rows, and 2 more variables: DATE <date>, LOAD <dbl>
```

Advanced Functionality
----------------------

The package is built on top of `dplyr` and `RSQLite`, and also provides low-level access to the HYDAT database through these methods.

``` r
hydat_get_db() # gets the db that was loaded in hydat_load()
#> src:  sqlite 3.11.1 [/Library/Frameworks/R.framework/Versions/3.3/Resources/library/hydatr/Hydat_sqlite3_99999999.db]
#> tbls: AGENCY_LIST, ANNUAL_INSTANT_PEAKS, ANNUAL_STATISTICS,
#>   CONCENTRATION_SYMBOLS, DATA_SYMBOLS, DATA_TYPES, DATUM_LIST, DLY_FLOWS,
#>   DLY_LEVELS, MEASUREMENT_CODES, OPERATION_CODES, PEAK_CODES,
#>   PRECISION_CODES, REGIONAL_OFFICE_LIST, SAMPLE_REMARK_CODES,
#>   SED_DATA_TYPES, SED_DLY_LOADS, SED_DLY_SUSCON, SED_SAMPLES,
#>   SED_SAMPLES_PSD, SED_VERTICAL_LOCATION, SED_VERTICAL_SYMBOLS,
#>   sqlite_stat1, STATIONS, STN_DATA_COLLECTION, STN_DATA_RANGE,
#>   STN_DATUM_CONVERSION, STN_DATUM_UNRELATED, STN_OPERATION_SCHEDULE,
#>   STN_REGULATION, STN_REMARK_CODES, STN_REMARKS, STN_STATUS_CODES, VERSION
```

``` r
hydat_tbl("ANNUAL_INSTANT_PEAKS") # gets the db that was loaded in hydat_load()
#> # Source:   table<ANNUAL_INSTANT_PEAKS> [?? x 12]
#> # Database: sqlite 3.11.1
#> #   [/Library/Frameworks/R.framework/Versions/3.3/Resources/library/hydatr/Hydat_sqlite3_99999999.db]
#>    STATION_NUMBER DATA_TYPE  YEAR PEAK_CODE PRECISION_CODE MONTH   DAY
#>             <chr>     <chr> <int>     <chr>          <int> <int> <int>
#>  1        01AD002         Q  1940         H             NA     5     5
#>  2        01AD002         Q  1950         H             NA     4    23
#>  3        01AD002         Q  1960         H             NA     5     8
#>  4        01AD002         Q  1970         H             NA     5     3
#>  5        01AD002         Q  1980         H             NA     4    16
#>  6        01AD003         Q  1960         H             NA     5    12
#>  7        01AD003         Q  1970         H             NA     5     4
#>  8        01AD003         Q  1980         H             NA     4    18
#>  9        01AD004         Q  1970         H             NA     5     3
#> 10        01AD004         H  1980         H              8    12    13
#> # ... with more rows, and 5 more variables: HOUR <int>, MINUTE <int>,
#> #   TIME_ZONE <chr>, PEAK <dbl>, SYMBOL <chr>
```

Because the package is built on `dplyr`, using `dplyr` functions is a particularly good way to get the most out of the database.

``` r
library(dplyr)
peaks <- hydat_tbl("ANNUAL_INSTANT_PEAKS") %>%
  left_join(hydat_tbl("PEAK_CODES")) %>%
  collect() %>%
  mutate(DATE = lubridate::ymd(paste(YEAR, MONTH, DAY))) %>%
  select(STATION_NUMBER, DATE, PEAK_CODE = PEAK_EN, PEAK)
#> Warning: 7 failed to parse.
peaks
#> # A tibble: 1,000 x 4
#>    STATION_NUMBER       DATE PEAK_CODE     PEAK
#>             <chr>     <date>     <chr>    <dbl>
#>  1        01AD002 1940-05-05   Maximum 2460.000
#>  2        01AD002 1950-04-23   Maximum 1890.000
#>  3        01AD002 1960-05-08   Maximum 2580.000
#>  4        01AD002 1970-05-03   Maximum 2690.000
#>  5        01AD002 1980-04-16   Maximum 1590.000
#>  6        01AD003 1960-05-12   Maximum  203.000
#>  7        01AD003 1970-05-04   Maximum  251.000
#>  8        01AD003 1980-04-18   Maximum  125.000
#>  9        01AD004 1970-05-03   Maximum 2810.000
#> 10        01AD004 1980-12-13   Maximum  138.457
#> # ... with 990 more rows
```
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1279</wp:post_id>
		<wp:post_date><![CDATA[2017-07-21 11:54:24]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-07-21 14:54:24]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[environment-canada-hydat-data-in-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="hydatr-r-hydrology"><![CDATA[hydatr R hydrology]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Dynamic Forms &amp; Formsets in Django		</title>
		<link>https://apps.fishandwhistle.net/archives/1283</link>
		<pubDate>Sun, 06 Aug 2017 20:17:02 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1283</guid>
		<description></description>
		<content:encoded>
				<![CDATA[In working on a <a href="https://www.djangoproject.com/">Django</a> web app recently, I ran across the problem of creating programatically-created forms. Creating regular forms in Django is a <a href="https://docs.djangoproject.com/en/1.11/topics/forms/#building-a-form-in-django">piece of cake</a>:

[markdown]
```python
from django import forms
class NameForm(forms.Form):
    your_name = forms.CharField(label='Your name', max_length=100)
```

Creating many forms is [equally easy](https://docs.djangoproject.com/en/1.11/topics/forms/formsets/), once the initial form class has been defined:

```python
from django.forms import formset_factory
NameFormSet = formset_factory(NameForm)
```

The ease of Django is magical, but what if the form fields need to be programmatically assigned? In my case, I'd like to provide a data entry template (user created) that leverages Django's powerful form and field libraries, but because form fields are assigned at the class level, they [cannot be assigned dynamically](https://stackoverflow.com/questions/68645/static-class-variables-in-python).

Luckily, form fields can be changed after a `Form` has been instantiated (see [this example](https://jacobian.org/writing/dynamic-form-generation/) from Jacob Kaplan-Moss), such that one only needs to modify the constructor of the custom `Form` class to make a dynamically-generated form class:

```python
class DataViewForm(forms.Form):
    def __init__(self, data_view, *args, **kwargs):
        super(DataViewForm, self).__init__(*args, **kwargs)
        # here data_view is the object that contains custom form information
        for item in data_view.column_spec:
            self.fields[item] = data_view.fields[item]
```

Formset objects are a little trickier, because they are created by the `formset_factory()` function, which takes the form class as its first argument and returning a `type` object that is used in a similar way as a `Form`. Upon inspecting the [source code of the formeset module](https://docs.djangoproject.com/en/1.11/_modules/django/forms/formsets/), it appears that the original `Form` class is used only as a callable to instantiate `Form` objects. This means that we can pass any callable that returns a `Form` instance to `formset_factory()`, not limited to `type` objects.

```python
# define a function that returns a callable that generates a proper form based on a
# data_view object
def form_class_factory(data_view):
    def new_form(*args, **kwargs):
        return DataViewForm(data_view, *args, **kwargs)
    return new_form

# pass the function returned from form_class_factory to formset_factory() 
# as a stand-in for a Form class
custom_form_class = form_class_factory(data_view_instance)
DataViewInstanceFormset = formset_factory(custom_form_class)
```

I'm not an expert in Django or web development, but it seems that the above solution works reasonably well to create forms when the format for the form must be passed programmatically.
[/markdown]

]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1283</wp:post_id>
		<wp:post_date><![CDATA[2017-08-06 17:17:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-06 20:17:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[dynamic-forms-formsets-in-django]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="django"><![CDATA[Django]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[Python]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Using the tidyverse to wrangle core data		</title>
		<link>https://apps.fishandwhistle.net/archives/1287</link>
		<pubDate>Thu, 24 Aug 2017 18:31:51 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1287</guid>
		<description></description>
		<content:encoded>
				<![CDATA[The paleolimnological data I work with most days is voluminous and difficult to wrangle. There are a lot of cores, a lot of variables, and a lot of parameters thanks to the multi-element analysis of the X-Ray Fluorescence spectrometer we've used recently on our sediment samples. However, since the advent of the tidyverse, this job has gotten a lot easier! I've been preparing some material to help students at the <a href="http://centreforwaterresourcesstudies.dal.ca/">Centre for Water Resources Studies</a> at Dalhousie and the <a href="http://paleoenvironment.acadiau.ca/home.html">Paleoenvironmental Research Group</a> at Acadia handle what are quickly becoming big data projects.

[markdown]
## Prerequisites

This tutorial uses the tidyverse, which can be installed using:

```r
install.packages("tidyverse")
library(tidyverse)
```

The tutorial also uses some sample data, which is a small subset of data from two cores taken several years ago. You can load the sample data by running the below code:

```r
pocmaj_raw <- tribble(
  ~sample_id, ~Ca, ~Ti, ~V,  
  "poc15-2 0",  1036, 1337, 29,
  "poc15-2 0", 1951, 2427, 31,
  "poc15-2 0", 1879, 2350, 39,
  "poc15-2 1", 1488, 2016, 36,
  "poc15-2 2", 2416, 3270, 79,
  "poc15-2 3", 2253, 3197, 79,
  "poc15-2 4", 2372, 3536, 87,
  "poc15-2 5", 2621, 3850, 86,
  "poc15-2 5", 2785, 3939, 95,
  "poc15-2 5", 2500, 3881, 80,
  "maj15-1 0", 1623, 2104, 73,
  "maj15-1 0", 1624, 2174, 73,
  "maj15-1 0", 2407, 2831, 89,
  "maj15-1 1", 1418, 2409, 70,
  "maj15-1 2", 1550, 2376, 70,
  "maj15-1 3", 1448, 2485, 64,
  "maj15-1 4", 1247, 2414, 57,
  "maj15-1 5", 1463, 1869, 78,
  "maj15-1 5", 1269, 1834, 71,
  "maj15-1 5", 1505, 1989, 94
)
```

### Wrangling the sample ID column

Often the first challenge in dealing with data straight from a lab is converting the `sample_id` column into a `core` and `depth` column, which are the two columns needed to properly plot the data. For this, we will use the `separate()` function within the tidyverse (for more advanced manipulation, see the `extract()` function).

``` r
pocmaj_clean <- pocmaj_raw %>%
  separate(sample_id, into = c("core", "depth"), sep = " ")

head(pocmaj_clean)
#> # A tibble: 6 x 5
#>      core depth    Ca    Ti     V
#>     <chr> <chr> <int> <int> <int>
#> 1 poc15-2     0  1036  1337    29
#> 2 poc15-2     0  1951  2427    31
#> 3 poc15-2     0  1879  2350    39
#> 4 poc15-2     1  1488  2016    36
#> 5 poc15-2     2  2416  3270    79
#> 6 poc15-2     3  2253  3197    79
```

The `separate()` function takes a data.frame and three arguments: the column containing the values to separate, the names of the output columns, and the separator to use. This is technically a [regular expression](https://en.wikipedia.org/wiki/Regular_expression), which will only matter if you need to split on a string that contains special characters such as `+\[]()?*.{}`. Usually this isn't a problem, but if it is you can escape the string with a double backslash like this: `sep = "\\+"`. You can also keep your original `sample_id` column by passing `remove = FALSE`.

There is a good chance that some of your `sample_id` values will be misspelled for some reason or another. A simple way to fix these values is using `if_else()`, which can be used to replace specific values in a column.

``` r
pocmaj_raw %>%
  mutate(sample_id = if_else(sample_id == "poc15-2 1", 
                             "the correct value", sample_id)) %>%
  head()
#> # A tibble: 6 x 4
#>           sample_id    Ca    Ti     V
#>               <chr> <int> <int> <int>
#> 1         poc15-2 0  1036  1337    29
#> 2         poc15-2 0  1951  2427    31
#> 3         poc15-2 0  1879  2350    39
#> 4 the correct value  1488  2016    36
#> 5         poc15-2 2  2416  3270    79
#> 6         poc15-2 3  2253  3197    79
```

For more advanced manipulation, use the `stringr` package, which provides the function `str_replace()` (among others) that can perform search and replace queries along the column.

The next step is to convert depth values into numbers (they are currently text!). For this we will use `mutate()` and `as.numeric()`:

``` r
pocmaj_clean <- pocmaj_raw %>%
  separate(sample_id, into = c("core", "depth"), sep = " ") %>%
  mutate(depth = as.numeric(depth))

head(pocmaj_clean)
#> # A tibble: 6 x 5
#>      core depth    Ca    Ti     V
#>     <chr> <dbl> <int> <int> <int>
#> 1 poc15-2     0  1036  1337    29
#> 2 poc15-2     0  1951  2427    31
#> 3 poc15-2     0  1879  2350    39
#> 4 poc15-2     1  1488  2016    36
#> 5 poc15-2     2  2416  3270    79
#> 6 poc15-2     3  2253  3197    79
```

### Parameter-long data

Occasionally, data in parameter-wide form (like the above) is useful, but to summarise replicates for a whole bunch of parameters and plot all the parameters at once, we need the data in parameter-long form. This form is more difficult to understand, but easier to work with! To convert the data to parameter-long form, we can use the `gather()` function.

``` r
pocmaj_long <- pocmaj_clean %>%
  gather(Ca:V, key = "param", value = "value")

head(pocmaj_long)
#> # A tibble: 6 x 4
#>      core depth param value
#>     <chr> <dbl> <chr> <int>
#> 1 poc15-2     0    Ca  1036
#> 2 poc15-2     0    Ca  1951
#> 3 poc15-2     0    Ca  1879
#> 4 poc15-2     1    Ca  1488
#> 5 poc15-2     2    Ca  2416
#> 6 poc15-2     3    Ca  2253
```

The `gather()` function takes a data frame plus three arguments: the columns to gather, the `key` column (in which the column names are placed), and the `value` column (in which the values corresponding to each row/key combination are drawn). The columns not mentioned act as identifying variables that identify unique rows, which means that columns that contain measured values will cause problems! These can be removed using something like `select(-ends_with("_error"))`, or something similar. If you don't quite understand this step, bear with me, because it makes plotting and summarising a whole lot easier!

### Summarising replicates

The final step before plotting is to summarise replicate values. For this, we will use `group_by()` and `summarise()`.

``` r
pocmaj_long_summarised <- pocmaj_long %>%
  group_by(core, depth, param) %>%
  summarise(mean_value = mean(value), sd_value = sd(value), n = n())

head(pocmaj_long_summarised)
#> # A tibble: 6 x 6
#> # Groups:   core, depth [2]
#>      core depth param mean_value   sd_value     n
#>     <chr> <dbl> <chr>      <dbl>      <dbl> <int>
#> 1 maj15-1     0    Ca 1884.66667 452.354212     3
#> 2 maj15-1     0    Ti 2369.66667 401.056521     3
#> 3 maj15-1     0     V   78.33333   9.237604     3
#> 4 maj15-1     1    Ca 1418.00000         NA     1
#> 5 maj15-1     1    Ti 2409.00000         NA     1
#> 6 maj15-1     1     V   70.00000         NA     1
```

Using `group_by()` then `summarise()` is common: `group_by()` specifies the columns whose unique combinations we are interested in. The values in these columns will identify unique rows in the output, which in our case are represented by `core`, `depth`, and `param`. The `summarise()` function takes arguments in the form of `output_column_name = expression`, where `expression` is an R expression (like `mean(value)`)) where column names can be used like variables. Using `mean()` and `sd()` is a good start, but `min()` and `max()` are also useful, as well as passing `na.rm = TRUE` if `NA` values exist in the `value` column.

### Converting parameter-long data to parameter-wide format

Some plotting functions and almost all ordination functions require data in parameter-wide. For this, we can use the opposite of `gather()`: the `spread()` function.

``` r
pocmaj_wide_summarised <- pocmaj_long_summarised %>%
  select(core, depth, param, mean_value) %>%
  spread(key = param, value = mean_value)

head(pocmaj_wide_summarised)
#> # A tibble: 6 x 5
#> # Groups:   core, depth [6]
#>      core depth       Ca       Ti        V
#>     <chr> <dbl>    <dbl>    <dbl>    <dbl>
#> 1 maj15-1     0 1884.667 2369.667 78.33333
#> 2 maj15-1     1 1418.000 2409.000 70.00000
#> 3 maj15-1     2 1550.000 2376.000 70.00000
#> 4 maj15-1     3 1448.000 2485.000 64.00000
#> 5 maj15-1     4 1247.000 2414.000 57.00000
#> 6 maj15-1     5 1412.333 1897.333 81.00000
```

Plotting Paleo Data
-------------------

There are several plotting libraries for paleo data, particularly for species composition data. These include [analogue](https://github.com/gavinsimpson/analogue) (the `Stratiplot()` function) and [rioja](https://cran.r-project.org/web/packages/rioja/index.html) (the `strat.plot()` function). For non-species data, the `ggplot2` library works quite well, provided data are in parameter-long form.

``` r
ggplot(pocmaj_long_summarised, aes(y = depth, x = mean_value, colour = core)) +
  geom_errorbarh(aes(xmax = mean_value + sd_value, xmin = mean_value - sd_value),
                 height = 0.1) +
  geom_point() +
  geom_path() +
  facet_wrap(~param, scales = "free_x") +
  scale_y_reverse()
#> Warning: Removed 24 rows containing missing values (geom_errorbarh).
```

![](/wp-content/uploads/2017/08/README-unnamed-chunk-12-1.png)

The `ggplot` library is quite intimidating at first, but it provides much flexibility and is worth the effort to [learn](http://r4ds.had.co.nz/data-visualisation.html). The above plot is constructed using a few lines, which I will describe one at a time.

``` r
ggplot(pocmaj_long_summarised, aes(y = depth, x = mean_value, col = core)) +
```

The `ggplot()` function creates a plot using its first argument as the primary `data` source. In our case, this is the `pocmaj_long_summarised`. Within `ggplot()`, we specify the default **aesthetics**, which is a mapping between the columns in `data` and the information that `ggplot` needs to construct a plot. Generally, paleo diagrams have the depth on the y-axis, and the parameter value on the x-axis. If more than one value exists in `core` (or this column may not exist of the data is only for one core), we can use `colour = core` to plot each core using a different color.

``` r
  geom_point() +
  geom_path() +
```

These `geom_*` functions don't need any arguments because they inherit the default aesthetics specified in `ggplot()`. We use `geom_path()` instead of `geom_line()` because `geom_line()` sorts its values by x value, which in our case doesn't make any sense!

``` r
geom_errorbarh(aes(xmax = mean_value + sd_value, xmin = mean_value - sd_value),
                 height = 0.1) +
```

Including error information is important when constructing paleolimnological diagrams (when uncertainty information is available), which is why we include the fairly long call to `geom_errorbarh()`. Unlike `geom_point()` and `geom_path()`, error bars require more information than `x`, `y`, and `colour`. Instead, we need to specify additional aesthetics (`xmin` and `xmax`), and how these should be calculated given `data` (in our case, `xmax = mean_value + sd_value, xmin = mean_value - sd_value`). Finally, the `height` of the error bars needs to be adjusted or they look quite huge.

``` r
facet_wrap(~param, scales = "free_x") +
```

The `facet_wrap()` specification is how `ggplot` creates many graphs using a single data input. The values in the specified column are used to create panels, with one panel per value. In our case, the `param` column is how we would like to separate our plots (this is usually the case). By default, `facet_wrap()` will keep all axes aligned, but because each parameter has a different range of values, we need that axis to automatically adjust based on its content. This is why we specify `scales = "free_x"`.

``` r
  scale_y_reverse()
```

Finally, we need to reverse the y-axis for the traditional strat diagram look with zero depth at the top of the diagram. If ages are used, this can be omitted.

### Tips and tricks

There are as many variations on strat diagrams as there are cores in the world, and `ggplot` can't produce all of them. A few things are still possible with slight modification!

#### Dual Y axis (ages and depths)

Having a dual y axis is possible, but requires a function transforming depth to age. In this example I'll use a simple function assuming a constant sedimentation rate, but in reality this function will probably use the `approx()` function given known age/depth values from <sup>210</sup>Pb or other dating method.

``` r
depth_to_age <- function(depth, sed_rate = 0.1, core_year = 2017) {
  # sed_rate here is in cm/year
  core_year - 1 / sed_rate * depth
}
depth_to_age(0:10)
#>  [1] 2017 2007 1997 1987 1977 1967 1957 1947 1937 1927 1917
```

Given this function, it can be passed to the `trans` argument of `sec_axis` to create a secondary Y axis.

``` r
ggplot(pocmaj_long_summarised, aes(y = depth, x = mean_value, col = core)) +
  geom_path() +
  facet_wrap(~param, scales = "free_x") +
  scale_y_reverse(sec.axis = sec_axis(trans = ~depth_to_age(.), name = "age"))
```

![](/wp-content/uploads/2017/08/README-unnamed-chunk-19-1.png)

The details of creating a secondary axis can be found in `?sec_axis`. Obviously this doesn't make sense for multiple cores, but works well for multiple parameters on a single core.

#### Zonation Lines

Zone lines can be added using `geom_hline()`, which creates a horizontal line superimposed over the plot.

``` r
ggplot(pocmaj_long_summarised, aes(y = depth, x = mean_value, col = core)) +
  geom_path() +
  geom_hline(yintercept = c(1.8, 4.2), linetype = 2, alpha = 0.5) +
  facet_wrap(~param, scales = "free_x") +
  scale_y_reverse()
```

![](/wp-content/uploads/2017/08/README-unnamed-chunk-20-1.png)

#### "Everything vs. everything"

Making a series of biplots is often useful, especially when dealing with XRF data. This is easy with data in its original, wide, format, but ggplot needs data in parameter-long form to make use of facets. This paired, long-form data can be created using a self `full_join()` using the non-parameter ID columns:

``` r
long_pairs <- full_join(pocmaj_long_summarised,
                        pocmaj_long_summarised,
                        by = c("core", "depth"))

head(long_pairs)
#> # A tibble: 6 x 10
#> # Groups:   core, depth [1]
#>      core depth param.x mean_value.x sd_value.x   n.x param.y mean_value.y
#>     <chr> <dbl>   <chr>        <dbl>      <dbl> <int>   <chr>        <dbl>
#> 1 maj15-1     0      Ca     1884.667   452.3542     3      Ca   1884.66667
#> 2 maj15-1     0      Ca     1884.667   452.3542     3      Ti   2369.66667
#> 3 maj15-1     0      Ca     1884.667   452.3542     3       V     78.33333
#> 4 maj15-1     0      Ti     2369.667   401.0565     3      Ca   1884.66667
#> 5 maj15-1     0      Ti     2369.667   401.0565     3      Ti   2369.66667
#> 6 maj15-1     0      Ti     2369.667   401.0565     3       V     78.33333
#> # ... with 2 more variables: sd_value.y <dbl>, n.y <int>
```

Creating a plot using this is quite straightforward (note that in this form, error bars can also be included using `geom_errorbar()` and `geom_errorbarh()`):

``` r
ggplot(long_pairs, aes(x = mean_value.x, y = mean_value.y, col = core)) +
  geom_point() +
  facet_grid(param.y ~ param.x, scales = "free")
```

![](/wp-content/uploads/2017/08/README-unnamed-chunk-22-1.png)

This data format has the added advantage of being able to test all the correlations for significance:

``` r
long_pairs %>%
  group_by(param.x, param.y) %>%
  summarise(test = list(cor.test(mean_value.x, mean_value.y))) %>%
  mutate(test = map(test, broom::glance)) %>%
  unnest()
#> # A tibble: 9 x 10
#> # Groups:   param.x [3]
#>   param.x param.y  estimate    statistic      p.value parameter   conf.low
#>     <chr>   <chr>     <dbl>        <dbl>        <dbl>     <int>      <dbl>
#> 1      Ca      Ca 1.0000000          Inf 0.000000e+00        10 1.00000000
#> 2      Ca      Ti 0.9025993 6.630408e+00 5.850911e-05        10 0.68194992
#> 3      Ca       V 0.5913529 2.318939e+00 4.285015e-02        10 0.02641645
#> 4      Ti      Ca 0.9025993 6.630408e+00 5.850911e-05        10 0.68194992
#> 5      Ti      Ti 1.0000000 2.122169e+08 1.328317e-79        10 1.00000000
#> 6      Ti       V 0.6502911 2.706912e+00 2.205017e-02        10 0.12187280
#> 7       V      Ca 0.5913529 2.318939e+00 4.285015e-02        10 0.02641645
#> 8       V      Ti 0.6502911 2.706912e+00 2.205017e-02        10 0.12187280
#> 9       V       V 1.0000000          Inf 0.000000e+00        10 1.00000000
#> # ... with 3 more variables: conf.high <dbl>, method <fctr>,
#> #   alternative <fctr>
```

Summary
-------

The tidyverse offers a great number of possibilities with respect to core data, only a few of which I describe here. In general, the functions in the tidyverse allow for parameter-long data to be manipulated more easily, which allows for a greater amount of metadata and uncertainty information to be kept alongside the data until it is not needed. For more, see the extensive [tidyverse documentation](http://www.tidyverse.org/articles/) and companion book, [R for Data Science](http://r4ds.had.co.nz/).

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1287</wp:post_id>
		<wp:post_date><![CDATA[2017-08-24 15:31:51]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-24 18:31:51]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[using-the-tidyverse-to-wrangle-core-data]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="paleolimnology"><![CDATA[paleolimnology]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				R data structures for Chemical Modeling		</title>
		<link>https://apps.fishandwhistle.net/archives/1298</link>
		<pubDate>Fri, 25 Aug 2017 16:17:51 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1298</guid>
		<description></description>
		<content:encoded>
				<![CDATA[In the past few months I've done some work on <a href="http://apps.fishandwhistle.net/archives/1268">PHREEQC modeling in R</a>, as well as a whole lot of XRF data work that required converting what seemed like an ungodly number of molecular concentrations (e.g. Al2O3) into elemental concentrations (Al). Both of these highlighted a need for chemical data structures in R such that user input to <a href="https://github.com/paleolimbot/easyphreeqc">easyphreeqc</a> can be properly validated and chemical calculations can be made reproducible easily. The result is <a href="https://github.com/paleolimbot/chemr">chemr</a>, which will hopefully integrate soon with other packages of the chemical persuasion.

[markdown]
Installation
------------

You can install chemr from github with:

``` r
# install.packages("devtools")
devtools::install_github("paleolimbot/chemr")
```

If you can load the package, everything worked!

``` r
library(chemr)
```

The periodic tibble
-------------------

The periodic tibble is [Wikipedia's version of the periodic table](https://en.wikipedia.org/wiki/List_of_chemical_elements) in data.frame (tibble) form. You can access these data in a few ways:

``` r
is_element("Hg")
#> [1] TRUE
```

``` r
elmass("Hg")
#>       Hg 
#> 200.5923
```

``` r
elz("Hg")
#> Hg 
#> 80
```

``` r
elsymbol(80)
#> [1] "Hg"
```

You can also access the entire periodic tibble by typing `data(pt)`.

``` r
data(pt)
pt
#> # A tibble: 118 x 7
#>        z symbol      name group period      mass   valence
#>    <int>  <chr>     <chr> <int>  <int>     <dbl>    <list>
#>  1     1      H  Hydrogen     1      1  1.008000 <int [3]>
#>  2     2     He    Helium    18      1  4.002602 <int [1]>
#>  3     3     Li   Lithium     1      2  6.940000 <int [2]>
#>  4     4     Be Beryllium     2      2  9.012183 <int [3]>
#>  5     5      B     Boron    13      2 10.810000 <int [6]>
#>  6     6      C    Carbon    14      2 12.011000 <int [9]>
#>  7     7      N  Nitrogen    15      2 14.007000 <int [9]>
#>  8     8      O    Oxygen    16      2 15.999000 <int [5]>
#>  9     9      F  Fluorine    17      2 18.998403 <int [2]>
#> 10    10     Ne      Neon    18      2 20.179760 <int [1]>
#> # ... with 108 more rows
```

Molecules
---------

Molecules are a collection of counted elements (or sub-molecules) with a charge. While it's possible to create a molecule by "hand", it's much easier to use the character representation of a molecule, which is usually what you get when copy/pasting from a source.

``` r
mol("H2O")
#> <mol>
#> [1] H2O
```

And like everything else in R, `mol` objects are vectorized, so you can serialize an entire column of molecule formulas.

``` r
as_mol(c("H2O", "H+", "Fe(OH)3", "Ca+2"))
#> <mol>
#> [1] H2O     H+      Fe(OH)3 Ca+2
```

You can access the mass, charge, and elemental composition of a molecule using `mass()`, `charge()`, and `as.data.frame()` or `as.matrix()`

``` r
m <- as_mol(c("H2O", "H+", "Fe(OH)3", "Ca+2"))
mass(m)
#> [1]  18.0150   1.0080 106.8662  40.0784
```

``` r
charge(m)
#> [1] 0 1 0 2
```

``` r
as.data.frame(m)
#>       mol mol_text     mass charge H O Fe Ca
#> 1    2, 1      H2O  18.0150      0 2 1  0  0
#> 2       1       H+   1.0080      1 1 0  0  0
#> 3 1, 1, 1  Fe(OH)3 106.8662      0 3 3  1  0
#> 4       1     Ca+2  40.0784      2 0 0  0  1
```

Reactions
---------

Reactions are essentially a molecule vector with coefficients (positive for the left side, negative for the right side). Similar to molecules, it's easiest to use the serialized form (conveniently, what is generally copied/pasted):

``` r
as_reaction("2H2 + O2 = 2H2O")
#> <reaction> 2H2 + O2 = 2H2O
```

The `is_balanced()` and `balance()` functions will happily balance these for you, provided you have the correct number of species defined.

``` r
balance("H2 + O2 = H2O")
#> <reaction> 2H2 + O2 = 2H2O
```

You can access various components of a reaction in the same way as for molecules:

``` r
r <- as_reaction("2H2 + O2 = 2H2O")
lhs(r)
#> <reaction> 2H2 + O2 =
```

``` r
rhs(r)
#> <reaction> 2H2O =
```

``` r
mass(r) # mass balance of the reaction
#> [1] 0
```

``` r
charge(r) # charge balance of the reaction
#> [1] 0
```

``` r
as.data.frame(r)
#>    mol mol_text charge   mass coefficient H O
#> 1    2       H2      0  2.016           2 2 0
#> 2    2       O2      0 31.998           1 0 2
#> 3 2, 1      H2O      0 18.015          -2 2 1
```

``` r
as.matrix(r)
#>      H  O
#> H2   4  0
#> O2   0  2
#> H2O -4 -2
```

Molecule and Reaction arithmetic
--------------------------------

Various arithmetic operators are available for molecule and reaction objects, such as `+`, `*` and `==`.

``` r
m <- mol(~Fe2O3, ~H2O, ~NH3, ~`H+`)
m + as_mol("OH-")
#> <mol>
#> [1] Fe2O3OH- H2OOH-   NH3OH-   HOH
```

``` r
m * 2
#> <mol>
#> [1] Fe4O6 H4O2  N2H6  H2+2
```

``` r
m == as_mol(~H2O)
#> [1] FALSE  TRUE FALSE FALSE
```

Reactions have similar arithmetic, with coefficients to various molecules being added together.

``` r
r1 <- as_reaction("2H2 + O2 = 2H2O")
r1 + as_reaction("H2O = H2O")
#> <reaction> 2H2 + O2 + H2O = 2H2O + H2O
```

By default the reaction isn't simplified, but can be using `simplify()` and `remove_zero_counts()`.

``` r
simplify(r1 + as_reaction("H2O = H2O"))
#> <reaction> 2H2 + O2 = 2H2O
```

``` r
simplify(r1 - as_reaction("2H+ + 2OH- = 2H2O"))
#> <reaction> 2H2 + O2 + 0H2O = 2H+ + 2OH-
```

``` r
remove_zero_counts(simplify(r1 - as_reaction("2H+ + 2OH- = 2H2O")))
#> <reaction> 2H2 + O2 = 2H+ + 2OH-
```

The Wish List
-------------

There are lots of things missing from this package that should exist, including the include various parameters to molecules and equations such as *Δ**H* or aliases (e.g., `CaSO4` as "gypsum"). Additionally, there is currently no way to indicate hydration in the same way as PHREEQC (e.g., `CaSO4:2H2O`). Currently this is possible only as `CaSO4(H2O)2`. Feel free to [contribute to development](https://github.com/paleolimbot/chemr) or [submit feature requests](https://github.com/paleolimbot/chemr/issues) on GitHub.
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1298</wp:post_id>
		<wp:post_date><![CDATA[2017-08-25 13:17:51]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-25 16:17:51]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[r-data-structures-for-chemical-modeling]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="chemistry"><![CDATA[chemistry]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="releases"><![CDATA[Releases]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Abbreviating journal titles using BibTex &amp; R		</title>
		<link>https://apps.fishandwhistle.net/archives/1312</link>
		<pubDate>Sat, 04 Nov 2017 20:09:55 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1312</guid>
		<description></description>
		<content:encoded>
				<![CDATA[It seems that the tools for writing papers in R/RStudio keep getting better and better, to the point where it is rare that I have something I need to do to write a paper that happens outside of RStudio. One of these things is abbreviating journal names, because for whatever reason the checkbox that does this within <a href="http://zotero.org/">Zotero</a>'s BibTex export doesn't work particularly well. My way around this in the past was to wait until the article was about to be submited, and figure everything out in Microsoft Word at the very end. For my last submission (mostly as a break from fixing comma splices), I decided to automate the abbreviating of journals to save myself another manual step that, realistically, I was probably going to forget to do anyway.

[markdown]
There's a whole lot of journals out there, but luckily there are a bunch of sources that have collected journal abbreviations in a form that are more or less machine-readable. I used the [UBC Library Journal Abbreviation Page](http://woodward.library.ubc.ca/research-help/journal-abbreviations/), with a short and kind of messy web-scraping script. Whatever the source, you will end up with a data frame that looks like this:

``` r
abbrevs <- tribble(
  ~title,                                 ~abbrev,
  "Environmental Science and Technology", "Environ. Sci. Technol.",
  "Journal of Paleolimnology",            "J. Paleolimnol."
)
```

Given this list, I needed a function that could take a journal title and turn it into an abbreviation. This is problematic because my BibTex file contained a number of oddly formatted journal titles (in particular, there were a few ways of encoding the "&" in "Environmental Science & Technology", in addition to some inconsistent spacing/capitalization). To get around this, I built a function that is a kind of one-way hash of a journal title that turns it to lower case and strips non-numeric characters (as well as "and"), so that all the spellings of ES&T could be identified as such.

``` r
id_journal <- function(j) {
  # make lower case
  str_to_lower(j) %>% 
    # strip "and" symbols
    str_replace_all("\\band\\b", "") %>% 
    # remove non a-z characters
    str_replace_all("[^a-z]", "")
}

id_journal("Environmental Science & Technology")
```

    ## [1] "environmentalsciencetechnology"

``` r
id_journal("environmental science and technology")
```

    ## [1] "environmentalsciencetechnology"

Then, I built a function to abbreviate (a vector of) journal titles using the new `id_journal()` function and `match()`. This has the nice property that journals without an abbreviation become `NA`.

``` r
# make function to abbreviate any journal title
abbreviate_journal <- function(j) {
  matches <- match(id_journal(j), id_journal(abbrevs$title))
  set_names(abbrevs$abbrev[matches], j)
}

abbreviate_journal("Environmental Science & Technology")
```

    ## Environmental Science & Technology 
    ##           "Environ. Sci. Technol."

The more complicated bit comes in doing a search and replace on journal titles from the `.bib` file (see the end of this post for the `sample_bib` file). I used a regular expression to look for text that looked like `journal = {Journal of Paleolimnology}` within the `.bib` file to extract journal titles that needed abbreviating.

``` r
journal_strings <- sample_bib %>%
  # match journal strings
  str_match("journal\\s*=\\s*\\{(.*?)\\}") %>% 
  # convert to a tibble, change column names
  as_tibble() %>%
  select(journal_string = V2)

journal_strings %>%
  filter(!is.na(journal_string))
```

    ## # A tibble: 2 x 1
    ##                           journal_string
    ##                                    <chr>
    ## 1              Journal of Paleolimnology
    ## 2 "Environmental Science \\& Technology"

Given a list of journal titles, we can then use the `abbreviate_journal()` function to look up the abbreviation.

``` r
journal_abbrevs <- journal_strings %>%
  # abbreviate the journal using our function from above
  mutate(journal_abbrev = abbreviate_journal(journal_string)) 

journal_abbrevs %>%
  filter(!is.na(journal_string))
```

    ## # A tibble: 2 x 2
    ##                           journal_string         journal_abbrev
    ##                                    <chr>                  <chr>
    ## 1              Journal of Paleolimnology        J. Paleolimnol.
    ## 2 "Environmental Science \\& Technology" Environ. Sci. Technol.

Then, we can use `str_replace()` to do a vectorized search and replace, searching `sample_bib` (the original text, with one element per line) for the original journal title, replacing it with the journal abbreviation.

``` r
fixed_lines <- journal_abbrevs %>%
  # search the old title and replace the new title
  mutate(fixed_line = str_replace(sample_bib,
                                  fixed(journal_string), 
                                  journal_abbrev)) 

fixed_lines %>%
  filter(!is.na(journal_string)) %>%
  select(fixed_line)
```

    ## # A tibble: 2 x 1
    ##                                fixed_line
    ##                                     <chr>
    ## 1        "\tjournal = {J. Paleolimnol.},"
    ## 2 "\tjournal = {Environ. Sci. Technol.},"

The `fixed_line` column we just created actually has quite a few `NA` values, because `str_replace()` propogates missing values that are in the search or replace vector (and any line that didn't have a journal or had an abbreviation that wasn't found would generate this). We can replace any line that has an `NA` value with the original vector using `coalesce()`, which fills `NA` values from the first vector with values from the second.

``` r
final_lines <- fixed_lines %>%
  # where there was no journal or no abbreviation, we have NAs
  # which we can replace using coalesce()
  mutate(final_line = coalesce(fixed_line, sample_bib)) 
```

Finally, we can export the final vector of lines. For this last paper, I wrote a second `.bib` file ("bibliography\_abbreved.bib"), and refered to that in my RMarkdown file. That way, when I inevitably had to update the original file, I could re-abbreviate all the journals using this script.

``` r
final_lines %>%
  # print the result
  pull(final_line) %>% paste(collapse = "\n") %>% cat()
```


    @article{dunnington_geochemical_2016,
        title = {A geochemical perspective on the impact of development at {Alta} {Lake}, {British} {Columbia}, {Canada}},
        volume = {56},
        doi = {10.1007/s10933-016-9919-x},
        number = {4},
        journal = {J. Paleolimnol.},
        author = {Dunnington, Dewey W. and Spooner, Ian S. and White, Chris E. and Cornett, R. Jack and Williamson, Dave and Nelson, Mike},
        year = {2016},
        pages = {315-330}
    }

    @article{anderson_lake_2017,
        title = {Lake {Recovery} {Through} {Reduced} {Sulfate} {Deposition}: {A} {New} {Paradigm} for {Drinking} {Water} {Treatment}},
        volume = {51},
        doi = {10.1021/acs.est.6b04889},
        number = {3},
        journal = {Environ. Sci. Technol.},
        author = {Anderson, Lindsay E. and Krkošek, Wendy H. and Stoddart, Amina K. and Trueman, Benjamin F. and Gagnon, Graham A.},
        year = {2017},
        pages = {1414-1422}
    }

It seems surprising that I couldn't find a ready-to-go abbreviater out there, and the only thing I can think of as a reason why is that the official list of abbreviations is somehow copyrighted. It seems like there should be an additional field in the BibTex/CSL format to handle multiple journal titles for multiple situations (or maybe there is and is just not implemented in Zotero). In the meantime, this hack of a solution seems to do the trick...

(for reference, the original `.bib` file...)

    @article{dunnington_geochemical_2016,
        title = {A geochemical perspective on the impact of development at {Alta} {Lake}, {British} {Columbia}, {Canada}},
        volume = {56},
        doi = {10.1007/s10933-016-9919-x},
        number = {4},
        journal = {Journal of Paleolimnology},
        author = {Dunnington, Dewey W. and Spooner, Ian S. and White, Chris E. and Cornett, R. Jack and Williamson, Dave and Nelson, Mike},
        year = {2016},
        pages = {315-330}
    }

    @article{anderson_lake_2017,
        title = {Lake {Recovery} {Through} {Reduced} {Sulfate} {Deposition}: {A} {New} {Paradigm} for {Drinking} {Water} {Treatment}},
        volume = {51},
        doi = {10.1021/acs.est.6b04889},
        number = {3},
        journal = {Environmental Science \& Technology},
        author = {Anderson, Lindsay E. and Krkosek, Wendy H. and Stoddart, Amina K. and Trueman, Benjamin F. and Gagnon, Graham A.},
        year = {2017},
        pages = {1414-1422}
    }

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1312</wp:post_id>
		<wp:post_date><![CDATA[2017-11-04 17:09:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-04 20:09:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[abbreviating-journals-using-bibtex-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="academia"><![CDATA[Academia]]></category>
		<category domain="post_tag" nicename="bibtex"><![CDATA[bibtex]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				How to look up timezones for 8,000 locations in R		</title>
		<link>https://apps.fishandwhistle.net/archives/1323</link>
		<pubDate>Mon, 08 Jan 2018 18:33:48 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1323</guid>
		<description></description>
		<content:encoded>
				<![CDATA[If you have ever worked with dates and times over a wide geographical area, you will know that timezone math is tedious but important. In the forthcoming update of the <a href="https://github.com/paleolimbot/rclimateca">rclimateca package</a>, the list of climate locations provided in the package will contain the UTC offsets for what Environment Canada calls <a href="http://climate.weather.gc.ca/glossary_e.html#l">“local standard time”</a>. Because the UTC offset for local standard time at each location is not provided with the list of locations from Environment Canada, I had to use the latitude and longitude of each site to obtain this information. The list of locations is long (&gt;8,000), and I ran into problems doing this using web APIs as a result of rate limits and a number of obviously wrong timezone assignments. In the end, I settled on using the shapefile output of <a href="https://github.com/evansiroky/timezone-boundary-builder/">Evan Siroky’s Timezone Boundary Builder</a> and the <a href="https://github.com/r-spatial/sf">simple features package</a> to solve the problem in about 4 lines! The most recent <a href="https://stackoverflow.com/questions/16086962/how-to-get-a-time-zone-from-a-location-using-latitude-and-longitude-coordinates">Stack Overflow answer for this problem</a> lists a number of resources, but not necessarily how to implement them in R. In this post I’ll document a few ways of finding the local timezone given only latitude and longitude of a site.

[markdown]

### Test Locations

In the [rclimateca package](https://github.com/paleolimbot/rclimateca) I did these excercises on all 8,000 locations, but for this post I'll just pick the first site from each province (see the end of the post for how to load this data for yourself).

| name                 | province |  latitude|  longitude|
|:---------------------|:---------|---------:|----------:|
| ABEE AGDM            | AB       |     54.28|    -112.97|
| ACTIVE PASS          | BC       |     48.87|    -123.28|
| PEACE RIVER OVERLAP  | MB       |     56.23|    -117.45|
| ACADIA FOREST EXP ST | NB       |     45.99|     -66.36|
| ARGENTIA A           | NL       |     47.30|     -54.00|
| ABERCROMBIE POINT    | NS       |     45.65|     -62.72|
| AKLAVIK A            | NT       |     68.22|    -135.01|
| ARVIAT AWOS          | NU       |     61.10|     -94.07|
| ATTAWAPISKAT         | ON       |     52.92|     -82.45|
| ALBANY               | PE       |     46.27|     -63.58|
| BARRIERE STONEHAM    | QC       |     47.17|     -71.25|
| ALAMEDA CDA EPF      | SK       |     49.25|    -102.28|
| AISHIHIK A           | YT       |     61.65|    -137.48|

### Using Geonames

The first hit when I googled this problem was a message from r-sig-geo that sugested using the [GeoNames Timezone API](http://www.geonames.org/export/web-services.html#timezone) (there is also a [GeoNames R package](https://cran.r-project.org/package=geonames) that, at the time of this writing, I cannot get to work). The GeoNames API has a rate limit of 2,000 requests per hour, which was a problem for me, but is probably sufficient for most small projects. Use of the API requires a [free account with GeoNames](http://www.geonames.org/login).

``` r
library(jsonlite)

get_timezone_geonames <- function(lon, lat) {
  # generate url for the geonames query
  url <- sprintf(
    "http://api.geonames.org/timezoneJSON?lat=%s&lng=%s&username=%s",
    lat, lon, geonames_user_id
  )
  
  # parse using jsonlite::fromJSON
  fromJSON(url)$timezoneId
}
```

This function was my solution to quickly parsing the output of the API. The response contains a few more fields, but all of these can be obtained from a few sources using the timezone identifier (e.g., "America/New\_York"). The function isn't vectorized, but we can use the `map2_chr()` function from the **purrr** package to apply it along our longitude and latitude vectors.

``` r
locs <- locs %>%
  mutate(tz_geonames = map2_chr(longitude, latitude, get_timezone_geonames))

locs %>%
  select(-latitude, -longitude)
```

| name                 | province | tz\_geonames          |
|:---------------------|:---------|:----------------------|
| ABEE AGDM            | AB       | America/Edmonton      |
| ACTIVE PASS          | BC       | America/Vancouver     |
| PEACE RIVER OVERLAP  | MB       | America/Edmonton      |
| ACADIA FOREST EXP ST | NB       | America/Moncton       |
| ARGENTIA A           | NL       | America/St\_Johns     |
| ABERCROMBIE POINT    | NS       | America/Halifax       |
| AKLAVIK A            | NT       | America/Yellowknife   |
| ARVIAT AWOS          | NU       | America/Rankin\_Inlet |
| ATTAWAPISKAT         | ON       | America/Toronto       |
| ALBANY               | PE       | America/Halifax       |
| BARRIERE STONEHAM    | QC       | America/Toronto       |
| ALAMEDA CDA EPF      | SK       | America/Regina        |
| AISHIHIK A           | YT       | America/Whitehorse    |

### Using the Google Maps Timezone API

The second options I tried was the [Google Maps Timezone API](https://developers.google.com/maps/documentation/timezone/intro). This API is similar to the GeoNames API, with slight differences to the input and output formats. Also, the Google Maps API requires a timestamp (number of seconds since 1970), although this doesn't affect the timezone ID. The Google Maps API has a rate limit of 50 requests per second and 1,500 per day, which limits its applicability to large projects. An API key is free and required.

``` r
get_timezone_google <- function(lon, lat) {
  # generate url for the google query
  timestamp <- "331161200"
  url <- sprintf(
    "https://maps.googleapis.com/maps/api/timezone/json?location=%s,%s&key=%s&timestamp=%s",
    lat, lon, google_api_key, timestamp
  )
  
  # parse using jsonlite::fromJSON
  fromJSON(url)$timeZoneId
}
```

``` r
locs <- locs %>%
  mutate(tz_google = map2_chr(longitude, latitude, get_timezone_google))

locs %>%
  select(-latitude, -longitude)
```

| name                 | province | tz\_geonames          | tz\_google        |
|:---------------------|:---------|:----------------------|:------------------|
| ABEE AGDM            | AB       | America/Edmonton      | America/Edmonton  |
| ACTIVE PASS          | BC       | America/Vancouver     | America/Vancouver |
| PEACE RIVER OVERLAP  | MB       | America/Edmonton      | America/Edmonton  |
| ACADIA FOREST EXP ST | NB       | America/Moncton       | America/Halifax   |
| ARGENTIA A           | NL       | America/St\_Johns     | America/St\_Johns |
| ABERCROMBIE POINT    | NS       | America/Halifax       | America/Halifax   |
| AKLAVIK A            | NT       | America/Yellowknife   | America/Edmonton  |
| ARVIAT AWOS          | NU       | America/Rankin\_Inlet | America/Winnipeg  |
| ATTAWAPISKAT         | ON       | America/Toronto       | America/Toronto   |
| ALBANY               | PE       | America/Halifax       | America/Halifax   |
| BARRIERE STONEHAM    | QC       | America/Toronto       | America/Toronto   |
| ALAMEDA CDA EPF      | SK       | America/Regina        | America/Regina    |
| AISHIHIK A           | YT       | America/Whitehorse    | America/Vancouver |

### Using Evan Siroky's Timezone Boundary Builder

My prefered solution to the timezone-by-location problem was using the shapefile output of [Evan Siroky's Timezone Boundary Builder](https://github.com/evansiroky/timezone-boundary-builder/) and the [simple features package](https://github.com/r-spatial/sf). Firstly, there is no limit to the number of locations, secondly, the output is more accurate than either GeoNames or Google (based on my use case of 8,000 obscure Canadian locations), and thirdly, it is more repeatable than using a web API because it completes in a negligible amount of time. It involves downloading a large-ish (50 MB) shapefile from the releases section of the Timezone Boundary Builder GitHub page (see the end of this post), and doing a spatial join using the `st_join()` function.

``` r
library(sf)

# load the timezone information
tz_polygons <- read_sf(
  "timezones.shapefile/dist/combined_shapefile.shp"
)

locs_sf <- locs %>%
  # turn the locs data frame into an sf-data frame
  st_as_sf(coords = c("longitude", "latitude"), 
           crs = 4326) %>%
  # do a left_join of the timezone polygons to the 
  # location point data
  st_join(tz_polygons)

# extract the timezone ID column from the locs_sf
# data frame, add it to the locs data frame
locs <- locs %>%
  mutate(tz_shp = locs_sf$tzid)

# print the result
locs %>%
  select(-latitude, -longitude)
```

| name                 | province | tz\_geonames          | tz\_google        | tz\_shp               |
|:---------------------|:---------|:----------------------|:------------------|:----------------------|
| ABEE AGDM            | AB       | America/Edmonton      | America/Edmonton  | America/Edmonton      |
| ACTIVE PASS          | BC       | America/Vancouver     | America/Vancouver | America/Vancouver     |
| PEACE RIVER OVERLAP  | MB       | America/Edmonton      | America/Edmonton  | America/Edmonton      |
| ACADIA FOREST EXP ST | NB       | America/Moncton       | America/Halifax   | America/Moncton       |
| ARGENTIA A           | NL       | America/St\_Johns     | America/St\_Johns | America/St\_Johns     |
| ABERCROMBIE POINT    | NS       | America/Halifax       | America/Halifax   | America/Halifax       |
| AKLAVIK A            | NT       | America/Yellowknife   | America/Edmonton  | America/Yellowknife   |
| ARVIAT AWOS          | NU       | America/Rankin\_Inlet | America/Winnipeg  | America/Rankin\_Inlet |
| ATTAWAPISKAT         | ON       | America/Toronto       | America/Toronto   | America/Toronto       |
| ALBANY               | PE       | America/Halifax       | America/Halifax   | America/Halifax       |
| BARRIERE STONEHAM    | QC       | America/Toronto       | America/Toronto   | America/Toronto       |
| ALAMEDA CDA EPF      | SK       | America/Regina        | America/Regina    | America/Regina        |
| AISHIHIK A           | YT       | America/Whitehorse    | America/Vancouver | America/Whitehorse    |

Even for these 13 locations, there are discrepancies among the timezone IDs returned by the three methods. In particular, I think the Google result is the least accurate, often assigning locations that may be closest but in the wrong political jurisdiction. In these locations the GeoNames result was identical to the shapefile result, but in my list of 8,000 locations, many errors occured in the GeoNames output that were not present in the shapefile output.

### Doing more

The timezone identifier is the key to the timezone information, but the useful data is the summer and winter UTC offsets. These data are available from a few locations: [Wikipedia](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones), [GeoNames](https://www.iana.org/time-zones), and the [official timezone database](https://www.iana.org/time-zones). I found the GeoNames version to be the most easily accessible from R (it is a text file), but it doesn't contain the current or historical dates of daylight savings time adjustments.

### Getting the Files

To follow along to the code in this blog post, you will need to download the location list from Environment Canada, and download the timezone information from the Timezone Boundary Calculator GitHub releases page. To do so without leaving R, run the following code:

``` r
library(curl)
locs_url <- paste0(
  "ftp://client_climate@ftp.tor.ec.gc.ca/Pub/",
  "Get_More_Data_Plus_de_donnees/", 
  "Station%20Inventory%20EN.csv"
)
curl_download(data_url, "locs.csv")

province_abbrevs <- provinces <- c(
  "ALBERTA" = "AB",  
  "BRITISH COLUMBIA" = "BC",
  "MANITOBA" = "MB",  
  "NEW BRUNSWICK" = "NB",  
  "NEWFOUNDLAND" = "NL",
  "NORTHWEST TERRITORIES" = "NT",  
  "NOVA SCOTIA" = "NS",  
  "NUNAVUT" = "NU",
  "ONTARIO" = "ON",  
  "PRINCE EDWARD ISLAND" = "PE",  
  "QUEBEC" = "QC",
  "SASKATCHEWAN" = "SK",  
  "YUKON TERRITORY" = "YT"
)

locs <- read_csv("locs.csv", skip = 3) %>%
  select(name = Name, province = Province,
         latitude = `Latitude (Decimal Degrees)`,
         longitude = `Longitude (Decimal Degrees)`) %>%
  mutate(province = province_abbrevs[province]) %>%
  group_by(province) %>%
  slice(1) %>%
  ungroup()
```

``` r
# download the file
curl::curl_download(
  paste0(
    "https://github.com/evansiroky/",
    "timezone-boundary-builder/releases/download/",
    "2017c/timezones.shapefile.zip"
  ),
  "timezones.shapefile.zip",
  quiet = FALSE
)

# extract the archive
unzip(
  "timezones.shapefile.zip", 
  exdir = "timezones.shapefile"
)
```
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1323</wp:post_id>
		<wp:post_date><![CDATA[2018-01-08 14:33:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-08 18:33:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[how-to-look-up-timezones-for-8000-locations-in-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="sf"><![CDATA[sf]]></category>
		<category domain="post_tag" nicename="timezones"><![CDATA[timezones]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Class-based Views and Model Formsets in Django		</title>
		<link>https://apps.fishandwhistle.net/archives/1327</link>
		<pubDate>Sat, 20 Jan 2018 20:37:40 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1327</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<a href="https://docs.djangoproject.com/en/dev/topics/class-based-views/intro/">Generic class-based views in Django</a> are powerful and result in minimal code to handle the basic tasks of viewing, creating, and modifying models. Recently I was trying to find a way to create a bunch of models at once (in my case, lab samples that tend to get generated in batches) in an application using class-based views. Creating a single model using a class-based view is handled by the <a href="https://docs.djangoproject.com/en/dev/ref/class-based-views/generic-editing/">django.views.generic.CreateView class</a>, but creating multiple models using a Formset isn't supported using generic class-based views (as far as I can tell from a pretty lengthy search...). Fortunately, it is easy to do and requires very little code!

[markdown]
Using a `CreateView`, one can provide describe a view that automatically creates and validates a form based on the model specification:

```python
from django.views import generic
from django.urls import reverse_lazy
from . import models

class SampleAddView(generic.CreateView):
    model = models.Sample
    fields = ['name', 'location', 'created']
    success_url = reverse_lazy('lims:sample_list')
    template_name = 'lims/sample_form.html'
```

Using the template below, this gets rendered to the page, and will redirect to the sample list on success (providing helpful error messages if form validation fails).

```
<form action="" method="post">
    {% csrf_token %}
    {{ form }}
    <input type="submit" value="Create" />
</form>
```

To use a `Formset` instead of a `Form`, we need to make a few modifications to the code, since we are no longer dealing with a single object. Since we are still dealing with a `Form`, we can use the `FormView` generic view instead of `CreateView`. In particular, we need to instantiate the model `Formset` class using a `QuerySet`, or it will use `Model.objects.all()`, which is probably never what you want. To get around this, we can instantiate the form class using `queryset=Model.objects.none()` (along with the rest of the form keyword arguments, which are generated using the `FormView.get_form_kwargs()` method). The only other thing that needs to be handled is the actual saving of the individual objects. Because the `Formset` is a model formset, each form is a `ModelForm` with a `.save()` method. Iterating through the forms by overriding `form_valid()` in the class-based view takes care of these new model instances getting saved.


```python
from django.views import generic
from django.urls import reverse_lazy
from django.forms import modelformset_factory
from . import models


class SampleAddView(generic.FormView):
    success_url = reverse_lazy('lims:sample_list')
    form_class = modelformset_factory(
        models.Sample,
        fields=['name', 'collected', 'location'],
        extra=3
    )
    template_name = 'lims/sample_form.html'

    def get_form_kwargs(self):
        kwargs = super(SampleAddView, self).get_form_kwargs()
        kwargs["queryset"] = models.Sample.objects.none()
        return kwargs

    def form_valid(self, form):
        for sub_form in form:
            if sub_form.has_changed():
                sub_form.save()

        return super(SampleAddView, self).form_valid(form)
```

And that's it! There's far prettier ways to render the `Formset` in the template, which is already well-described in the [Django documentation on formsets](https://docs.djangoproject.com/en/dev/topics/forms/formsets/).
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1327</wp:post_id>
		<wp:post_date><![CDATA[2018-01-20 16:37:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-20 20:37:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[model-formsets-in-django]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="django"><![CDATA[Django]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[Python]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Verbifying nouns and using the pipe in ggplot2		</title>
		<link>https://apps.fishandwhistle.net/archives/1336</link>
		<pubDate>Tue, 27 Feb 2018 04:19:09 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1336</guid>
		<description></description>
		<content:encoded>
				<![CDATA[There is a lot of talk about the <b>ggplot2</b> package and the pipe. <a href="https://community.rstudio.com/t/why-cant-ggplot2-use/4372/7">Should it be used?</a> Some approaches, like the <a href="https://github.com/zeehio/ggpipe"><b>ggpipe</b> package</a>, replace many <strong>ggplot2</strong> functions, adding the plot as the first argument so they can be used with the pipe. This ignores the fact that <strong>ggplot2</strong> functions construct objects that can (and should) be re-used. Verbifying these noun functions to perform the task of creating the object <em>and</em> updating the plot object is one approach, and recently I wrote an <a href="https://github.com/paleolimbot/ggverbs">experimental R package</a> that implements it in just under 50 lines of code.

Constructing a plot using the <b>ggplot2</b> package is like adding a bunch of things together. Right? Except the verb "add" isn't a particularly good verb for some of the things we use the <code>+</code> symbol for. Consider the following example:

[markdown]
``` r
ggplot(mtcars, aes(wt, mpg, col = disp)) + 
  geom_point() +
  scale_colour_gradient(low = "black", high = "white") +
  labs(x = "Weight")
```

![](/wp-content/uploads/2018/02/README-plot-ggplot-1.png)

In the above code, `+ geom_point()` **adds** a layer to the plot, `+ scale_colour_gradient(low = "black", high = "white")` **replaces** (or **sets**) the colour scale, and `+ labs(x = "Weight")` **updates** the current set of labels. In the [**ggverbs** package](https://github.com/paleolimbot), the multitude of element constructors (currently nouns) are transformed into verbs that describe what they do to the plot. This has the added benefit of using the pipe (`%>%`) rather than the `+` operator to construct a plot, without masking any functions exported by **ggplot2**.

``` r
library(ggverbs)
ggplot(mtcars, aes(wt, mpg, col = disp)) %>%
  add_geom_point() %>%
  set_scale_colour_gradient(low = "black", high = "white") %>%
  update_labs(x = "Weight")
```

![](/wp-content/uploads/2018/02/README-plot-ggverb-1.png)

The [**ggverbs** package](https://github.com/paleolimbot) doesn't actually define any functions. Instead, it uses whatever the currently installed version of **ggplot2** exports, and uses a couple of regular expressions to change nouns into verbs. This has the advantage of not depending on any particular version of **ggplot2**, and because the functions are created on namespace load, it isn't bothered by the user updating **ggplot2**, and doesn't care if you have either package attached. The list of regexes looks something like this:

``` r
verbs_regexes <- c(
    "^aes_?$" = "update",
    "^aes_(string|q)$" = "update",
    "^layer$"  = "add",
    "^(geom|stat|annotation)_" = "add",
    "^scale_[a-z0-9]+_[a-z0-9]+$" = "set",
    "^theme_(?!get|set|update|replace)[a-z]+$" = "set",
    "^theme$" = "update",
    "^coord_(?!munch)[a-z]+$" = "set",
    "^facet_" = "set",
    "^labs$" = "update",
    "^guides$" = "update"
  )
```

Verbifying noun functions currently takes the strategy of modifying the call to the verb function to remove the `.plot` argument and pass on all the others. This has the advantage of keeping the autocomplete of arguments, although there is still no way to use R's help system with this approach. It throws 1 WARNING on the R CMD check (undocumented objects, naturally), but only consists of about 47 lines of code.
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1336</wp:post_id>
		<wp:post_date><![CDATA[2018-02-27 00:19:09]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-27 04:19:09]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[verbifying-nouns-and-using-the-pipe-in-ggplot2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ggplot"><![CDATA[ggplot]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Modifying facet scales in ggplot2		</title>
		<link>https://apps.fishandwhistle.net/archives/1344</link>
		<pubDate>Wed, 28 Feb 2018 01:24:27 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">http://apps.fishandwhistle.net/?p=1344</guid>
		<description></description>
		<content:encoded>
				<![CDATA[There is a <a href="https://github.com/tidyverse/ggplot2/issues/187">very old issue</a> in <strong>ggplot2</strong> about the ability to modify particular scales when using <code>facet_wrap()</code> or <code>facet_grid()</code>. I often have this problem when using lots of facets, as sometimes the labels overlap with eachother on some of the scales. Without a way to set the <code>breaks</code> on one particular scale, it's hard to fix this without exporting an SVG and modifying the result (it's usually possible to fix it by specifying an overall set of <code>breaks</code>, or by rotating the x labels using <code>theme(axis.text.x = element_text(angle = 90, vjust = 0.5))</code>, but sometimes it's just <em>so close</em> that it might be nice to set the breaks on just one of the panels...). A sort of contrived example:

[markdown]
``` r
set.seed(123)
test_large_values_data <- 1:9 %>%
  set_names() %>%
  map_dfr(~data.frame(
    x_variable = runif(1, 100, 20000) + runif(10, -100, 100),
    y_variable = runif(10, 0, 1)
  ), .id = "facet_name")

p_annoying_x_scale <- ggplot(test_large_values_data, aes(x_variable, y_variable)) +
  geom_point() +
  facet_wrap(~facet_name, scales = "free", ncol = 4)

p_annoying_x_scale
```

![](/wp-content/uploads/2018/02/fig-annoying-x-scale-orig-1.png)

The second problem I have is when creating a "these values are equal to these values?" plot with lots of facets. Here I need the limits on the x and y axes to the the same, but I don't really need to modify any particular one of them. Ideally the plot below would show the dashed line directly through both corners of each plot. (Using `coord_equal()` can sometimes approximate this behaviour, but still isn't quite what I need.)

``` r
test_equal_values_data <- 1:9 %>%
  set_names() %>%
  map_dfr(~tibble(
    x_variable = runif(1, 100, 2000) + runif(10, -100, 100),
    y_variable = x_variable + runif(10, -100, 100)
  ), .id = "facet_name")

p_equal_plot <- ggplot(test_equal_values_data, aes(x_variable, y_variable)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.7, lty = 2) +
  geom_point() +
  facet_wrap(~facet_name, scales = "free")

p_equal_plot
```

![](/wp-content/uploads/2018/02/fig-equal-orig-1.png)

Solutions to these problems are bound to be in a future version of **ggplot2**, but until then, here are a few (somewhat hack-y) solutions.

### Modifying specific scales

The problem of modifying specific scales has [long been a requested feature](https://github.com/tidyverse/ggplot2/issues/187) in **ggplot2**, but it's a bit tricky to implement and hard to think of exactly the syntax should go. The solution I use below is far from perfect but gets the job done, and probably will for a while. It hinges on modifying the `init_scales()` method of the underlying `Facet` class. From the[GitHub source](https://github.com/tidyverse/ggplot2/blob/master/R/facet-.r#L91):

``` r
Facet <- ggproto(
  ...               
  init_scales = function(layout, x_scale = NULL, y_scale = NULL, params) {
    scales <- list()
    if (!is.null(x_scale)) {
      scales$x <- plyr::rlply(max(layout$SCALE_X), x_scale$clone())
    }
    if (!is.null(y_scale)) {
      scales$y <- plyr::rlply(max(layout$SCALE_Y), y_scale$clone())
    }
    scales
  },
  ...
)
```

Essentially, when the plot is built, `init_scales()` copies whatever scale you've specified for the `x` and `y` aesthetics (or the default one if you haven't) over all the panels. As long as you know what panel you're looking to modify, you could create a specification that overrides an `x` or `y` scale at a particular panel. There's probably better ways to go about this, but one might be this:

``` r
scale_override <- function(which, scale) {
  if(!is.numeric(which) || (length(which) != 1) || (which %% 1 != 0)) {
    stop("which must be an integer of length 1")
  }
  
  if(is.null(scale$aesthetics) || !any(c("x", "y") %in% scale$aesthetics)) {
    stop("scale must be an x or y position scale")
  }
  
  structure(list(which = which, scale = scale), class = "scale_override")
}
```

Next, we need to implement a version of `init_scales()` that looks for scale overrides and replaces the default scale with the overridden one. It's a bit verbose, but because `init_scales()` isn't really well-documented it's hard to predict what gets called with what. I'm overriding `FacetWrap` here, but it could easily be `FacetGrid`.

``` r
CustomFacetWrap <- ggproto(
  "CustomFacetWrap", FacetWrap,
  init_scales = function(self, layout, x_scale = NULL, y_scale = NULL, params) {
    # make the initial x, y scales list
    scales <- ggproto_parent(FacetWrap, self)$init_scales(layout, x_scale, y_scale, params)
    
    if(is.null(params$scale_overrides)) return(scales)
    
    max_scale_x <- length(scales$x)
    max_scale_y <- length(scales$y)
    
    # ... do some modification of the scales$x and scales$y here based on params$scale_overrides
    for(scale_override in params$scale_overrides) {
      which <- scale_override$which
      scale <- scale_override$scale
      
      if("x" %in% scale$aesthetics) {
        if(!is.null(scales$x)) {
          if(which < 0 || which > max_scale_x) stop("Invalid index of x scale: ", which)
          scales$x[[which]] <- scale$clone()
        }
      } else if("y" %in% scale$aesthetics) {
        if(!is.null(scales$y)) {
          if(which < 0 || which > max_scale_y) stop("Invalid index of y scale: ", which)
          scales$y[[which]] <- scale$clone()
        }
      } else {
        stop("Invalid scale")
      }
    }
    
    # return scales
    scales
  }
)
```

Lastly, we need a constructor function. Unfortunately, `facet_wrap()` doesn't let you specify a `Facet` class, so we have to hack the result of `facet_wrap()` so we can use the syntax that we are used to with the function.

``` r
facet_wrap_custom <- function(..., scale_overrides = NULL) {
  # take advantage of the sanitizing that happens in facet_wrap
  facet_super <- facet_wrap(...)
  
  # sanitize scale overrides
  if(inherits(scale_overrides, "scale_override")) {
    scale_overrides <- list(scale_overrides)
  } else if(!is.list(scale_overrides) || 
            !all(vapply(scale_overrides, inherits, "scale_override", FUN.VALUE = logical(1)))) {
    stop("scale_overrides must be a scale_override object or a list of scale_override objects")
  }
  
  facet_super$params$scale_overrides <- scale_overrides
  
  ggproto(NULL, CustomFacetWrap,
    shrink = facet_super$shrink,
    params = facet_super$params
  )
}
```

Then we can nix some annoying breaks on the first and sixth panels:

``` r
p_annoying_x_scale +
  facet_wrap_custom(~facet_name, scales = "free", ncol = 4, scale_overrides = list(
    scale_override(1, scale_x_continuous(breaks = c(5750, 5900))),
    scale_override(6, scale_x_continuous(breaks = c(17800, 17900)))
  ))
```

![](/wp-content/uploads/2018/02/fig-final-scale-1.png)

### Equal X and Y scales

The other problem of making the x and y scales equal isn't a matter of modifying the initial scales, it's a matter of modifying the process by which they are **trained**, or fed the data to have their limits determined automatically. It might be possible to set all the limits by hand using the above method, but it would sure take a long time and would change with each new dataset. Just like there is a `init_scales()` method in the `Facet` class, there is also a `train_scales()` function that performs this very process.

``` r
Facet <- ggproto(
  ...,
  train_scales = function(x_scales, y_scales, layout, data, params) {
    # loop over each layer, training x and y scales in turn
    for (layer_data in data) {
      match_id <- match(layer_data$PANEL, layout$PANEL)

      if (!is.null(x_scales)) {
        x_vars <- intersect(x_scales[[1]]$aesthetics, names(layer_data))
        SCALE_X <- layout$SCALE_X[match_id]

        scale_apply(layer_data, x_vars, "train", SCALE_X, x_scales)
      }

      if (!is.null(y_scales)) {
        y_vars <- intersect(y_scales[[1]]$aesthetics, names(layer_data))
        SCALE_Y <- layout$SCALE_Y[match_id]

        scale_apply(layer_data, y_vars, "train", SCALE_Y, y_scales)
      }
    }
  },
  ...
)
```

Essentially, I need the x and y scales to be fed both the x and y data, so that the limits for both are based on the same points. The solution I found for overriding this method seems a bit more hack-y than the above solution, but seems to work when put to the test:

``` r
FacetEqualWrap <- ggproto(
  "FacetEqualWrap", FacetWrap,
  
  train_scales = function(self, x_scales, y_scales, layout, data, params) {
    
    # doesn't make sense if there is not an x *and* y scale
    if (is.null(x_scales) || is.null(x_scales)) {
        stop("X and Y scales required for facet_equal_wrap")
    }
    
    # regular training of scales
    ggproto_parent(FacetWrap, self)$train_scales(x_scales, y_scales, layout, data, params)
    
    # switched training of scales (x and y and y on x)
    for (layer_data in data) {
      match_id <- match(layer_data$PANEL, layout$PANEL)
      
      x_vars <- intersect(x_scales[[1]]$aesthetics, names(layer_data))
      y_vars <- intersect(y_scales[[1]]$aesthetics, names(layer_data))
      
      SCALE_X <- layout$SCALE_X[match_id]
      ggplot2:::scale_apply(layer_data, y_vars, "train", SCALE_X, x_scales)
      
      SCALE_Y <- layout$SCALE_Y[match_id]
      ggplot2:::scale_apply(layer_data, x_vars, "train", SCALE_Y, y_scales)
    }
    
  }
)
```

Similar to above, we need a constructor function to replicate the magic that happens in `facet_wrap()`.

``` r
facet_wrap_equal <- function(...) {
  # take advantage of the sanitizing that happens in facet_wrap
  facet_super <- facet_wrap(...)
  
  ggproto(NULL, FacetEqualWrap,
    shrink = facet_super$shrink,
    params = facet_super$params
  )
}
```

And voila! The scales are the same on the X and Y axes, and the dotted line is always across the middle of the plot...

``` r
p_equal_plot +
  facet_wrap_equal(~facet_name, scales = "free")
```

![](/wp-content/uploads/2018/02/fig-final-equal-1.png)

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1344</wp:post_id>
		<wp:post_date><![CDATA[2018-02-27 21:24:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-28 01:24:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[modifying-facet-scales-in-ggplot2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ggplot"><![CDATA[ggplot]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Comparing approaches to age-depth modelling in R		</title>
		<link>https://apps.fishandwhistle.net/archives/1352</link>
		<pubDate>Thu, 15 Mar 2018 05:32:23 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">https://apps.fishandwhistle.net/?p=1352</guid>
		<description></description>
		<content:encoded>
				<![CDATA[Working with radiocarbon dating in R has long been possible, especially since the <a href="http://www.radiocarbon.org/IntCal13.htm">Intcal dataset itself</a> contains R code in the supplement. Other tools like <a href="http://chrono.qub.ac.uk/blaauw/bacon.html">Bacon</a> (<a href="http://dx.doi.org/10.1214/11-BA618">Blauuw and Christen 2011</a>), the slightly simpler <a href="http://chrono.qub.ac.uk/blaauw/clam.html">Clam</a> (<a href="http://dx.doi.org/10.1016/j.quageo.2010.01.002">Blaauw 2010</a>), and <a href="https://github.com/andrewcparnell/Bchron">BChron</a> (<a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2008.00623.x/abstract">Haslett and Parnell 2008</a>) have helped users calibrate radiocarbon dates and produce reproducible age models. A comprehensive analysis of the quality of these and other methods is available in an article by <a href="http://journals.sagepub.com/doi/abs/10.1177/0959683616675939">Trachsel and Telford (2016)</a>.

In a recent quest to learn how the calibration process happens, I <a href="https://github.com/paleolimbot/carbon14">wrote my own version of R radiocarbon calibration</a>. All of these methods are useful, so I thought I would compare and contrast the mechanics of getting data into and out of these methods using a few published radiocarbon dates from <a href="http://www.facetsjournal.com/doi/10.1139/facets-2017-0004">Long Lake, Nova Scotia, Canada</a>. There is a spurious date at 35 cm, which makes it an interesting test case to see how the different methods handle the dates. These are in the <code>dates</code> variable, which I'll use throughout this post (note also that I'm using <code>library(tidyverse)</code>).

[markdown]

``` r
library(tidyverse)

dates <- tribble(
  ~sample_id, ~depth, ~age_14C, ~age_error,
  "LL082011-C2-39", 94, 9148, 49,
  "UOC-0844", 70.5, 8582, 28,
  "LL082011-C2-87", 46, 4396, 55,
  "UOC-0845", 37.5, 575, 18,
  "LL082011-C2-124", 9, 623, 34
)

dates
```

| sample\_id      |  depth|  age\_14C|  age\_error|
|:----------------|------:|---------:|-----------:|
| LL082011-C2-39  |   94.0|      9148|          49|
| UOC-0844        |   70.5|      8582|          28|
| LL082011-C2-87  |   46.0|      4396|          55|
| UOC-0845        |   37.5|       575|          18|
| LL082011-C2-124 |    9.0|       623|          34|

### carbon14

Using the [carbon14](https://github.com/paleolimbot/carbon14) package (still in draft form), it is possible to calibrate radiocarbon dates, although it doesn't construct an age-depth model. The syntax is a little like `mutate()`, and the result is a data frame-ish object.

``` r
# install.packages("devtools")
# devtools::install_github("paleolimbot/carbon14")
library(carbon14)

carbon14_result <- dates %>%
  calibrate(
    measured_age = age_14C, 
    measured_age_error = age_error,
    name = sample_id
  )

plot(carbon14_result)
```

![](/wp-content/uploads/2018/03/unnamed-chunk-2-1.png)

Extracting the distribution data isn't straightforward, but it's definitely possible.

``` r
calib_carbon14 <- map2_dfr(
  dates$depth, 
  carbon14_result$cal_age, 
  function(depth, cal_age_item) {
    df <- cal_age_item$data %>%
      select(cal_age_bp = values, density = densities)
    df$depth <- depth
    df
  }
) %>% select(depth, everything())

calib_carbon14 %>% head()
```

|  depth|  cal\_age\_bp|  density|
|------:|-------------:|--------:|
|     94|      9155.000|        0|
|     94|      9159.491|        0|
|     94|      9163.982|        0|
|     94|      9168.474|        0|
|     94|      9172.965|        0|
|     94|      9177.456|        0|

### clam

Clam (Classical Age-Depth Modelling of Cores from Deposits) used to come in collection of `.R` files, but now is [available from CRAN in package form](https://cran.r-project.org/package=clam). Clam takes a simpler approach than Bacon, more closely resembling the "draw a line between the things" method. The clam package highly dependent on the directory structure, and requires a similar structure as `Bacon()` with a different type of `.csv` file in the directory. With the `dates` data frame, this looks something like the following:

``` r
# install.packages("clam")
library(clam)
dir.create("cores_clam/LL-PC2", recursive = TRUE)

dates %>%
  mutate(reservoir = "", cal_BP = "") %>%
  select(ID = sample_id, C14_age = age_14C, cal_BP,
         error = age_error, reservoir, depth) %>%
  write_csv("cores_clam/LL-PC2/LL-PC2.csv")

clam(core = "LL-PC2", coredir = "cores_clam")
```

![](/wp-content/uploads/2018/03/unnamed-chunk-6-1.png)

The dates from Long Lake have an age reversal, which `Bacon()` is not bothered by, since all the age models that pass through the weird age at 35 cm are highly unlikely under the assumptions of `Bacon()` (notably, that accumulation rates don't change quickly). To exclude the date, one can use the `outliers` argument. Calibration curves can be changed using combinations of the `cc` arguments.

Similar to `Bacon()`, `clam()` puts an object in the global environment that contains information about the last run. The object is called `dat`, and has a similar structure to Bacon's `info` object. In particular, the distribution of calibrated dates is included:

``` r
calib_clam <- map2_dfr(dat$depth, dat$calib, function(depth, prob_matrix) {
  colnames(prob_matrix) <- c("cal_age_bp", "density")
  df <- as_tibble(prob_matrix)
  df$depth <- depth
  df
}) %>%
  select(depth, cal_age_bp, density)

calib_clam %>% head()
```

|  depth|  cal\_age\_bp|  density|
|------:|-------------:|--------:|
|     94|         10167|  1.0e-06|
|     94|         10168|  1.1e-06|
|     94|         10169|  1.2e-06|
|     94|         10170|  1.3e-06|
|     94|         10171|  1.7e-06|
|     94|         10172|  2.2e-06|

The output of the model (the grey area) is placed in the global environment as the `calrange` object. This is probably more useful as a data frame, the columns of which I've renamed to match the output of `Bacon()`.

``` r
ages_clam <- as_tibble(calrange) %>%
  select(depth = Depth, min = starts_with("min"), 
         max = starts_with("max"), mean = "point")

ages_clam %>% head()
```

|  depth|       min|       max|      mean|
|------:|---------:|---------:|---------:|
|      9|  546.9500|  660.0000|  603.0831|
|     10|  548.3140|  657.3219|  602.7287|
|     11|  549.7009|  655.0114|  602.3743|
|     12|  551.9974|  653.2658|  602.0199|
|     13|  552.8456|  651.1649|  601.6655|
|     14|  554.0526|  649.3860|  601.3111|

The `clam()` approach has the same drawback of `Bacon()`, in that it does not work in reproducible documents (hopefully this will be fixed in future versions). It also places objects in the global environment, which makes it possible that name collisions will occur (I tend to run `rm(dat, calrange, dets, smp, chron)` to clean up the namespace after running `clam()`. While the design of the package seems more geared towards that of a script than a RMarkdown-like document, with some coaxing it can be done.

### Bchron

The Bchron package is based on a [paper from 2008](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2008.00623.x/abstract), and will both calibrate radiocarbon dates and create age-depth models with uncertainty. Creating an age-depth model from the `dates` data frame looks something like this:

``` r
library(Bchron)
result <- Bchronology(
  ages = dates$age_14C,
  ageSds = dates$age_error,
  positions = dates$depth,
  positionThicknesses = rep(1, nrow(dates))
)
```

``` r
plot(result)
```

![](/wp-content/uploads/2018/03/unnamed-chunk-10-1.png)

Extracting the raw calibrated distributions looks something like this:

``` r
calib_bchron <- map_dfr(result$calAges, function(cal_age_item) {
  tibble(
    depth = cal_age_item$positions,
    cal_age_bp = cal_age_item$ageGrid,
    density = cal_age_item$densities
  )
})

calib_bchron %>%
  head()
```

|  depth|  cal\_age\_bp|  density|
|------:|-------------:|--------:|
|      9|             0|        0|
|      9|             1|        0|
|      9|             2|        0|
|      9|             3|        0|
|      9|             4|        0|
|      9|             5|        0|

...and extracting the minimum, mean, and maximum ages looks like this (at a 95% confidence interval).

``` r
ages_bchron <- tibble(
  depth = result$predictPositions,
  min = apply(result$thetaPredict, 2, "quantile", probs = (1 - 0.95)/2),
  median = apply(result$thetaPredict, 2, "quantile", probs = 0.5),
  mean = apply(result$thetaPredict, 2, mean),
  max = apply(result$thetaPredict, 2, "quantile", probs = 1 - (1 - 0.95)/2)
)
```

### rbacon

Bacon (Age-Depth Modelling using Bayesian Statistics) also used to come in a collection of `.R` files, but now is [available from CRAN in package form](https://cran.r-project.org/package=rbacon). Calibrating dates is the first step in the Bayesian age-depth modelling approach, which uses the full distribution of the calibrated radiocarbon date. It is probably because Bacon originated in the form of an R script that it requires a very specific directory structure...in particular, it needs a `.csv` file with the columns `labID`, `age`, `error`, `depth`, and `cc`, which contains a numbered calibration curve (there is documentation of which value of `cc` means what at `?Bacon`).

``` r
# install.packages("rbacon")
library(rbacon)
dir.create("cores_bacon/LL-PC2", recursive = TRUE)
dates %>%
  select(labID = sample_id, age = age_14C, error = age_error, depth) %>%
  arrange(depth) %>%
  write_csv("cores_bacon/LL-PC2/LL-PC2.csv")

Bacon(core = "LL-PC2", coredir = "cores_bacon", 
      ask = FALSE, plot.pdf = FALSE,
      thick = 2, acc.mean = 100)
```

I've hidden the output because it's quite extensive, but to reproduce a plot of the calibrated dates, one can use the `calib.plot()` function to plot the last run (I've aligned the axes like the Bchron plot):

``` r
calib.plot(rotate.axes = TRUE, rev.yr = TRUE)
```

![](/wp-content/uploads/2018/03/unnamed-chunk-16-1.png)

...and to reproduce the modeled densities, one can use the `agedepth()` function.

``` r
agedepth(rotate.axes = TRUE, rev.yr = TRUE)
```

![](/wp-content/uploads/2018/03/unnamed-chunk-17-1.png)

The `agedepth()` plot is the one most people use in publication (it contains the calibrated dates, the confidence of the modeled dates, and the diagnostic information needed to make sure that the model is reasonable), but sometimes it helps to get some of the raw information out of the model. Calibrated dates can be found in `info$calib`, although they are a bit nicer if they are converted into a data frame (the first six rows are shown below).

``` r
calib_bacon <- map2_dfr(
  info$calib$d, 
  info$calib$probs, 
  function(depth, prob_matrix) {
    colnames(prob_matrix) <- c("cal_age_bp", "density")
    df <- as_tibble(prob_matrix)
    df$depth <- depth
    df
}) %>%
  select(depth, cal_age_bp, density)

calib_bacon %>% head()
```

|  depth|  cal\_age\_bp|    density|
|------:|-------------:|----------:|
|      9|           520|  0.0015886|
|      9|           525|  0.0022892|
|      9|           530|  0.0034534|
|      9|           535|  0.0052350|
|      9|           540|  0.0096007|
|      9|           545|  0.0163060|

As a summary of the model, `info$ranges` is probably the most useful. This information is also written to the `cores_bacon/LL-PC1` folder as a text file.

``` r
ages_bacon <- as_tibble(info$ranges)
ages_bacon %>% head()
```

|  depth|  min|   max|  median|  mean|
|------:|----:|-----:|-------:|-----:|
|      9|  542|   666|     602|   604|
|     10|  582|   968|     701|   720|
|     11|  599|  1311|     793|   836|
|     12|  655|  1436|     909|   943|
|     13|  684|  1612|    1013|  1050|
|     14|  757|  1741|    1128|  1159|

Note that I've found that when `Bacon()` experiences an error, it is usually helpful to restart R, which sometimes fixes the error for an unknown reason. `Bacon()` also has problems when used inside a reproducible document (like this one), which will hopefully be fixed in future versions. While it is a little hard to use in a reproducible document context, it is excellent, well-documented software, and once the file with the dates is in place, it is quite approachable.

### Comparing approaches

All four packages allow calibrating of radiocarbon dates, and three of the packages create age-depth models. Throughout this post I've gone to a bit of trouble to extract the raw information out of the output so that it can be compared with the output from the other packages. Note that I used the default for most of the options, which represents a common use case, especially for beginners (and when it comes to Bayesian age-depth modelling, I will probably always consider myself a beginner).

![](/wp-content/uploads/2018/03/unnamed-chunk-21-1.png)

There is a slight difference in the probability distributions of the calibrated dates, notably that rbacon distributions have more tail than the other packages. The default option for `Bacon()` is a t distribution with low degrees of freedom, which means that the original radiocarbon date is given a distribution with more prominent tails. The overall shape is similar, which means that each package probably implements the IntCal13 calibration curve the same, with some slight differences in computations. For example, the rbacon package calculates density on a 5-year scale and a cutoff of 0.001 (higher than the other packages), which means that some of the tails are lost.

The set of 5 ages from Long Lake contain a spurious date at 35 cm, which is handled differently by each of the packages when creating an age-depth model. Clam takes the "straight line" approach, Bchron appears to have the widest predicted error range, while Bacon predicts a much narrower age range for most depths. In addition, Bacon doesn't try to accommodate the spurious date.

![](/wp-content/uploads/2018/03/unnamed-chunk-22-1.png)

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1352</wp:post_id>
		<wp:post_date><![CDATA[2018-03-15 02:32:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-15 05:32:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[1352]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="academia"><![CDATA[Academia]]></category>
		<category domain="category" nicename="general"><![CDATA[General]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				The Circumpolar Diatom Database using R, the tidyverse, and mudata2		</title>
		<link>https://apps.fishandwhistle.net/archives/1397</link>
		<pubDate>Mon, 09 Apr 2018 00:13:57 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">https://apps.fishandwhistle.net/?p=1397</guid>
		<description></description>
		<content:encoded>
				<![CDATA[It is an exciting time for the integration of limnological and paleolimnological datasets. The <a href="https://www.waterqualitydata.us/">National (US) Water Quality Monitoring Council Water Quality Portal</a> has just made decades of state and federal water quality measurements available, the <a href="https://www.ncdc.noaa.gov/paleo-search/study/21171">Pages2k project</a> has collected hundreds of temperature proxy records for the last 2000 (ish) years, and the <a href="https://www.neotomadb.org/">Neotoma</a> database provides access to a large number of paleoecological datasets. For a final project in a course last fall, I chose to analyze the <a href="http://www.cen.ulaval.ca/CDD/About.aspx">Circumpolar Diatom Database (CDD)</a>, which is a collection of water chemistry and diatom assemblage data hosted by the Aquatic Paleoecology Laboratory at ULaval. In this post, we will (1) download and clean the data and metadata from the CDD website, and (2) use the <a href="https://cran.r-project.org/package=mudata2"><strong>mudata2</strong></a> package to extract some data. Part 2 is more exciting, so I'll put that first...

[markdown]

### The point of it all

This post takes a single webpage, and uses it to create a `mudata()` object. The [(Mostly) Universal Data](http://www.facetsjournal.com/doi/10.1139/facets-2017-0026) format is a format and R package that lets you store metadata (i.e., location, parameter, and dataset information) and data in a way that is easy to read, write, and subset. It's a good way to store clean data that will get used later, or distribute data for others to use. With it, we can do things like this:

``` r
library(mudata2)
cdd <- read_mudata("cdd.mudata")
cdd %>%
  select_datasets(ALERT, BYL) %>%
  filter_params(param_type == "diatom_count") %>%
  filter_data(value > 5)
```

    ## A mudata object aligned along <none>
    ##   distinct_datasets():  "ALERT", "BYL"
    ##   distinct_locations(): "A-A", "A-AA" ... and 35 more
    ##   distinct_params():    "ACsubato", "ACsuchla" ... and 56 more
    ##   src_tbls():           "data", "locations" ... and 3 more
    ## 
    ## tbl_data() %>% head():
    ## # A tibble: 6 x 5
    ##   dataset location param                                 value unit       
    ##   <chr>   <chr>    <chr>                                 <dbl> <chr>      
    ## 1 BYL     BI-02    Navicula rynchocephala                 6.82 % rel abund
    ## 2 BYL     BI-02    Staurosira construens var. subrotunda 17.3  % rel abund
    ## 3 BYL     BI-02    PIbalf                                 8.40 % rel abund
    ## 4 BYL     BI-02    SApinnat                              47.0  % rel abund
    ## 5 BYL     BI-23    Undetermined diatom [CDD]              6.76 % rel abund
    ## 6 BYL     BI-23    ACsuchla                               8.75 % rel abund

The resulting object contains all the information of the original object, but only for diatom data from the ALERT and BYL datasets, where relative abundance is greater than 5%. Similarly, one could find all the locations where silica was measured:

``` r
cdd %>%
  select_params(SiO2)
```

    ## A mudata object aligned along <none>
    ##   distinct_datasets():  "ALERT", "BYL" ... and 10 more
    ##   distinct_locations(): "A-A", "A-AA" ... and 402 more
    ##   distinct_params():    "SiO2"
    ##   src_tbls():           "data", "locations" ... and 3 more
    ## 
    ## tbl_data() %>% head():
    ## # A tibble: 6 x 5
    ##   dataset location param  value unit 
    ##   <chr>   <chr>    <chr>  <dbl> <chr>
    ## 1 BYL     BI-01    SiO2  0.0600 mg/L 
    ## 2 BYL     BI-02    SiO2  0.0700 mg/L 
    ## 3 BYL     BI-03    SiO2  0.270  mg/L 
    ## 4 BYL     BI-04    SiO2  0.660  mg/L 
    ## 5 BYL     BI-05    SiO2  1.44   mg/L 
    ## 6 BYL     BI-06    SiO2  1.02   mg/L

To get data out of the object, one can use the `tbl_data()` (or `tbl_data_wide()` for the classic parameter-wide version), `tbl_params()`, `tbl_datasets()` and `tbl_locations()` functions. Where on earth is the CDD, anyway?

``` r
cdd %>%
  tbl_locations() %>%
  ggplot(aes(x = long_w, y = lat_n, col = dataset)) +
  annotation_map(map_data("world"), fill = "white", col = "grey50") +
  geom_point() +
  coord_map("ortho") +
  theme(panel.background = element_rect(fill = "lightblue"))
```

![](/wp-content/uploads/2018/04/unnamed-chunk-3-1.png)

For some good old-fashioned relative abundance diagrams, we can use `tbl_data()` to extract ggplot-friendly data (in this case, only plotting diatom abundance for diatoms with &gt;25% relative abundance somewhere).

``` r
cdd %>%
  select_datasets(ALERT) %>%
  filter_params(param_type == "diatom_count") %>%
  tbl_data() %>%
  group_by(param) %>%
  filter(any(value > 25)) %>%
  ungroup() %>%
  ggplot(aes(x = location, y = value)) +
  geom_col() +
  facet_grid(~param, space = "free_x", scales = "free_x") +
  coord_flip()
```

![](/wp-content/uploads/2018/04/unnamed-chunk-4-1.png)

Convenient, right!? To get there, however, there is a long road of data cleaning ahead...

### Cleaning the data

Welcome to the data cleaning section! It is a story of intrigue, excitement, webscraping, and mildly complicated regular expressions. This section of the post is for those who are somewhat familiar with [data manipulation](http://r4ds.had.co.nz/transform.html) and [data tidying](http://r4ds.had.co.nz/tidy-data.html) using the [tidyverse](https://www.tidyverse.org/), and are looking to practice on a real-world dataset.

In this post I'll use the **tidyverse** core packages to do the hard work, the **lubridate** package to do some date parsing, the **curl** package to download files, the **rvest** package to extract data from web pages, the **readxl** package to read the spreadsheets that contain the data, and the **mudata2** package to organize the cleaned data at the end.

``` r
library(tidyverse)
```

I'll also use the `rename_friendly()` function below to make column names from the wild (in this case, Excel files that other people have created) easier to type in R. Essentially, it takes the current column names, makes them lowercase, and replaces any non-alpha-numeric character with an underscore. I promise this is useful...

``` r
rename_friendly <- function(df) {
  names(df) <- names(df) %>%
    str_to_lower() %>%
    str_replace_all("[^a-z0-9-_]+", "_") %>%
    str_remove("^_") %>%
    str_remove("_$") %>%
    tidy_names()
  df
}
```

### Extracting the dataset information

The CDD is one of the more accessible datasets on the web, in that the entire dataset is available for download, and the metadata is included on the website (notably in the [datasets table](http://www.cen.ulaval.ca/CDD/DatasetList.aspx)). We *could* copy and paste the table into a spreadsheet and then do some modifying, but that would be no fun. Instead, we will use the **rvest** package to extract the relevant information about each dataset from the table. Below, the page HTML is read using `read_html()`, and the tables are extracted using `html_table()` (the datasets table is the first one on the page, hence the `[[1]]`).

``` r
library(rvest)
# scrape datasets info from datasets page
datasets_page_url <- "http://www.cen.ulaval.ca/CDD/DatasetList.aspx"
datasets_page <- read_html(datasets_page_url)

datasets_table <- html_table(datasets_page)[[1]] %>%
  rename_friendly() %>%
  select(-data)
```

The datasets table is great for display, but in R we will need that information in a machine-understandable format. For example, the range of visit dates is a bit of a mess as extracted by `html_table()`:

``` r
datasets_table %>%
  select(dataset_code, visit_date) %>%
  head()
```

| dataset\_code | visit\_date                          |
|:--------------|:-------------------------------------|
| BYL           | June 3, 2005to August 5, 2006        |
| SAL           | April 1st, 2002to September 30, 2004 |
| SI            | July 12, 2004to July 18, 2004        |
| ALERT         | July 24, 2000to August 7, 2000       |
| MB            | July 12, 1999to July 21, 1999        |
| ABK           | August 15, 1997to September 7, 1997  |

In R, these are more useful as two columns of `Date` objects: `visit_date_start` and `visit_date_end`. The strategy I used was to use the `separate()` function to split the `visit_date` column using the text `"to"`, and then use the `mdy()` (month, date, year) function in the **lubridate** package to make them `Date` objects.

``` r
datasets_table <- datasets_table %>%
  separate(
    visit_date, 
    into = c("visit_date_start", "visit_date_end"), 
    sep = "to"
  ) %>%
  mutate(
    visit_date_start = lubridate::mdy(visit_date_start),
    visit_date_end = lubridate::mdy(visit_date_end)
  )

datasets_table %>%
  select(dataset_code, visit_date_start, visit_date_end) %>%
  head()
```

| dataset\_code | visit\_date\_start | visit\_date\_end |
|:--------------|:-------------------|:-----------------|
| BYL           | 2005-06-03         | 2006-08-05       |
| SAL           | 2002-04-01         | 2004-09-30       |
| SI            | 2004-07-12         | 2004-07-18       |
| ALERT         | 2000-07-24         | 2000-08-07       |
| MB            | 1999-07-12         | 1999-07-21       |
| ABK           | 1997-08-15         | 1997-09-07       |

The second column that is readable to humans but perhaps less useful in R is the `coordinates` column.

``` r
datasets_table %>%
  select(dataset_code, coordinates) %>%
  head()
```

| dataset\_code | coordinates                                   |
|:--------------|:----------------------------------------------|
| BYL           | LAT: 72.51° to 73.12°LONG: -79.25° to -80.08° |
| SAL           | LAT: 60.86° to 62.18°LONG: -69.56° to -75.72° |
| SI            | LAT: 64.21° to 65.21°LONG: -82.52° to -84.2°  |
| ALERT         | LAT: NA° to NA°LONG: NA° to NA°               |
| MB            | LAT: NA° to NA°LONG: NA° to NA°               |
| ABK           | LAT: 67.07° to 68.48°LONG: 23.52° to 17.67°   |

This is a little more complicated to parse, but each string is in the form `"(LAT or LON): (a number or NA)° to (a number or NA)°"`. Because the text is structured, we can use a regular expression and the `extract()` function to extract the numbers. Then, we can apply the `as.numeric()` function to the numbers we just extracted.

``` r
datasets_table <- datasets_table %>%
  extract(
    coordinates, 
    into = c("lat_min", "lat_max", "lon_min", "lon_max"),
    regex = "LAT: ([0-9.-]+|NA)° to ([0-9.-]+|NA)°LONG: ([0-9.-]+|NA)° to ([0-9.-]+|NA)°"
  ) %>%
  mutate_at(vars(lat_min, lat_max, lon_min, lon_max), as.numeric) 

datasets_table %>%
  select(dataset_code, lat_min, lat_max, lon_min, lon_max) %>%
  head()
```

| dataset\_code |  lat\_min|  lat\_max|  lon\_min|  lon\_max|
|:--------------|---------:|---------:|---------:|---------:|
| BYL           |     72.51|     73.12|    -79.25|    -80.08|
| SAL           |     60.86|     62.18|    -69.56|    -75.72|
| SI            |     64.21|     65.21|    -82.52|    -84.20|
| ALERT         |        NA|        NA|        NA|        NA|
| MB            |        NA|        NA|        NA|        NA|
| ABK           |     67.07|     68.48|     23.52|     17.67|

There are two columns in the online table that contain links, which aren't kept by `html_table()`, which only extracts the text. The links will be useful for us when we need to download the data for each dataset, but even if we weren't, it would be useful information to have as a column in our `datasets_table`. The approach I took for this was to create a data.frame (tibble) of all the links, then filter out the ones I needed (the data link and the more information link) separately. Essentially, it is looking for text in the web page that looks like `<a href="...data_(dataset code).zip">...</a>` for the data link, and `<a href="...DatasetInfo2.aspx...">(dataset code)</a>` for the dataset info link. Finally, it uses a `left_join()` to add this information to the `datasets_table`.

``` r
dataset_page_links <- tibble(
  node = html_nodes(datasets_page, "a"),
  href = html_attr(node, "href"),
  text = html_text(node)
)

dataset_links_tbl <- dataset_page_links %>%
  select(data_link = href) %>%
  extract(
    data_link, 
    into = "dataset_code", 
    regex = "data_([a-z]+).zip$", 
    remove = FALSE
  ) %>%
  mutate(dataset_code = str_to_upper(dataset_code)) %>%
  filter(!is.na(dataset_code))

dataset_info_links_tbl <- dataset_page_links %>%
  filter(str_detect(href, "DatasetInfo2.aspx")) %>%
  select(dataset_code = text, info_link = href) %>%
  mutate(info_link = paste0("http://www.cen.ulaval.ca/CDD/", info_link))

datasets_table <- datasets_table %>%
  left_join(dataset_links_tbl, by = "dataset_code") %>%
  left_join(dataset_info_links_tbl, by = "dataset_code")

datasets_table %>%
  select(dataset_code, data_link, info_link) %>%
  head()
```

| dataset\_code | data\_link                                         | info\_link                                                  |
|:--------------|:---------------------------------------------------|:------------------------------------------------------------|
| BYL           | <http://www.cen.ulaval.ca/cdd/data/data_byl.zip>   | <http://www.cen.ulaval.ca/CDD/DatasetInfo2.aspx?IDVisit=6>  |
| SAL           | <http://www.cen.ulaval.ca/cdd/data/data_sal.zip>   | <http://www.cen.ulaval.ca/CDD/DatasetInfo2.aspx?IDVisit=5>  |
| SI            | <http://www.cen.ulaval.ca/cdd/data/data_si.zip>    | <http://www.cen.ulaval.ca/CDD/DatasetInfo2.aspx?IDVisit=7>  |
| ALERT         | <http://www.cen.ulaval.ca/cdd/data/data_alert.zip> | <http://www.cen.ulaval.ca/CDD/DatasetInfo2.aspx?IDVisit=15> |
| MB            | <http://www.cen.ulaval.ca/cdd/data/data_mb.zip>    | <http://www.cen.ulaval.ca/CDD/DatasetInfo2.aspx?IDVisit=16> |
| ABK           | <http://www.cen.ulaval.ca/cdd/data/data_abk.zip>   | <http://www.cen.ulaval.ca/CDD/DatasetInfo2.aspx?IDVisit=18> |

### Parsing the data

Now that we have the link for all the datasets, we can download them! Because they are ZIP files, we can extract them as well. To do this in one go, I used `pmap()` to take a tibble with the columns `url` and `destfile`, and apply the `curl_download()` function from the **curl** package to each row. The `curl_download()` function outputs the downloaded file, so we can then take the result and pipe it to the `unzip()` function, which extracts a zip file in the working directory.

``` r
datasets_table %>%
  select(url = data_link) %>%
  mutate(destfile = basename(url)) %>%
  pmap(curl::curl_download, quiet = TRUE) %>%
  map(unzip)
```

If all went well, you should have a whole lot of Excel files in your working directory, named something like `diatom_count_....xlsx` and `water_chemistry_....xlsx`. These excel files *are* the CDD, but what do they contain? We can use the **readxl** package to have a look.

``` r
library(readxl)
read_excel("diatom_count_byl.xlsx", skip = 1) %>%
  head()
```

| Diatom taxon                          | Taxon code |      BI-02|     BI-23|      BI-24|      BI-27|
|:--------------------------------------|:-----------|----------:|---------:|----------:|----------:|
| Chamaepinnularia grandupii            | NA         |  0.2624672|        NA|         NA|         NA|
| Diatoma monoliformis                  | NA         |         NA|        NA|  6.0000000|  1.4950166|
| Encyonema elginense                   | NA         |         NA|        NA|  0.1980198|         NA|
| Encyonema fogedii                     | NA         |         NA|        NA|  0.1980198|  0.8305648|
| Eunotia binularis                     | NA         |         NA|        NA|  1.5841584|         NA|
| Fragilaria construens var. subrotunda | NA         |         NA|  4.373757|         NA|         NA|

``` r
read_excel("water_chemistry_byl.xlsx", skip = 1) %>%
  select(1:7) %>%
  head()
```

| Lake \# |  Lat (°N)|  Long (°W)|  Alt (m)|  Depth (m)|  Area (ha)|   pH|
|:--------|---------:|----------:|--------:|----------:|----------:|----:|
| BI-01   |  73.13333|  -80.10000|        8|        2.8|      84491|  6.7|
| BI-02   |  73.05000|  -80.13333|       10|        2.1|      18265|  6.3|
| BI-03   |  73.18333|  -79.86667|       10|       10.0|      17916|  7.0|
| BI-04   |  73.18333|  -79.86667|       10|        4.0|      30113|  7.0|
| BI-05   |  73.20000|  -79.85000|       10|        9.5|      47481|  7.0|
| BI-06   |  73.20000|  -79.85000|       10|       10.5|      70546|  6.7|

It looks like the `diatom_counts...` spreadsheets have one row for each taxon, with locations as columns, whereas the `water_chemistry...` spreadsheets have one row for each location, with each water quality parameter. Additionally, the `water_chemistry...` spreadsheets have information about each location that is unlikely to change with time. This is perhaps a semantic difference, but an important one for structuring the data, as water quality measurements and the relative abundance of diatom taxa are measurements from the CDD, and the location/altitude/depth/area information are not measurements in quite the same way.

The two tables contain similar information that is structured in slightly different ways: one is "location-wide", and the other is "parameter-wide". One way to combine them is to make them both tables that have one row per measurement. From a tidy data standpoint, this means we can `gather()` values that are in columns into rows. Here is what it looks like for both spreadsheets (note that the negative numbers mean all the columns *except* those ones):

``` r
read_excel("diatom_count_byl.xlsx", skip = 1) %>%
  gather(-1, -2, key = "lake", value = "rel_abundance") %>%
  rename_friendly() %>%
  head()
```

| diatom\_taxon                         | taxon\_code | lake  |  rel\_abundance|
|:--------------------------------------|:------------|:------|---------------:|
| Chamaepinnularia grandupii            | NA          | BI-02 |       0.2624672|
| Diatoma monoliformis                  | NA          | BI-02 |              NA|
| Encyonema elginense                   | NA          | BI-02 |              NA|
| Encyonema fogedii                     | NA          | BI-02 |              NA|
| Eunotia binularis                     | NA          | BI-02 |              NA|
| Fragilaria construens var. subrotunda | NA          | BI-02 |              NA|

``` r
read_excel("water_chemistry_byl.xlsx", skip = 1) %>%
  select(-(2:6)) %>%
  gather(-1, key = "param", value = "value") %>%
  rename_friendly() %>%
  head()
```

| lake  | param |  value|
|:------|:------|------:|
| BI-01 | pH    |    6.7|
| BI-02 | pH    |    6.3|
| BI-03 | pH    |    7.0|
| BI-04 | pH    |    7.0|
| BI-05 | pH    |    7.0|
| BI-06 | pH    |    6.7|

As I mentioned above, some of the information in the water chemistry spreadsheet is more suited to a "location information" table (similar to our "dataset information" table above).

``` r
read_excel("water_chemistry_byl.xlsx", skip = 1) %>%
  select(1:6) %>%
  rename_friendly() %>%
  head()
```

| lake  |    lat\_n|    long\_w|  alt\_m|  depth\_m|  area\_ha|
|:------|---------:|----------:|-------:|---------:|---------:|
| BI-01 |  73.13333|  -80.10000|       8|       2.8|     84491|
| BI-02 |  73.05000|  -80.13333|      10|       2.1|     18265|
| BI-03 |  73.18333|  -79.86667|      10|      10.0|     17916|
| BI-04 |  73.18333|  -79.86667|      10|       4.0|     30113|
| BI-05 |  73.20000|  -79.85000|      10|       9.5|     47481|
| BI-06 |  73.20000|  -79.85000|      10|      10.5|     70546|

The code above works for *one* dataset, but we have 15! Because the files are well-named and all structured the same way, we can write a function to read a single dataset for each table type from above.

``` r
read_diatom_count <- function(dataset_code) {
  paste0("diatom_count_", str_to_lower(dataset_code), ".xlsx") %>%
    read_excel(skip = 1) %>%
    gather(-1, -2, key = "lake", value = "rel_abundance") %>%
    rename_friendly() %>%
    mutate(dataset_code = dataset_code)
}

read_water_chemistry <- function(dataset_code) {
  paste0("water_chemistry_", str_to_lower(dataset_code), ".xlsx") %>%
    read_excel(skip = 1) %>%
    select(-(2:6)) %>%
    gather(-1, key = "label", value = "value") %>%
    rename_friendly() %>%
    mutate(dataset_code = dataset_code)
}

read_lake_info <- function(dataset_code) {
  paste0("water_chemistry_", str_to_lower(dataset_code), ".xlsx") %>%
    read_excel(skip = 1) %>%
    select(1:6) %>%
    rename_friendly() %>%
    mutate(dataset_code = dataset_code)
}
```

Then, we can apply the functions along the list of dataset codes (`map_`) and combining the resulting data frames by row (`_dfr`) to make one big table for each data type.

``` r
diatom_counts <- datasets_table %>%
  pull(dataset_code) %>%
  map_dfr(read_diatom_count)

water_chemistry <- datasets_table %>%
  pull(dataset_code) %>%
  map_dfr(read_water_chemistry)

lake_info <- datasets_table %>%
  pull(dataset_code) %>%
  map_dfr(read_lake_info)
```

### Creating a mudata object

A mudata object (standing for [(Mostly) Universal Data](http://www.facetsjournal.com/doi/10.1139/facets-2017-0026)) is a way to store cleaned data so that it's amenable to reading, writing, documentation, and subsetting. It's implemented in the [**mudata2** package](https://cran.r-project.org/package=mudata2), and consists of five tables: **data** (a one-measurement-per-row table), **locations** (a table containing location information), **params** (a table containing parameter information), **datasets** (a table containing dataset information), and **columns** (containing column documentation). For a demo of why this might be useful in this context, see the the beginning of the this post.

We already have the raw data for these tables, but the column names used by **mudata2** (`dataset`, `location`, `param`, and `value`) are slightly different than what we have used so far. The datasets table is the closest to its final form, with the `dataset_code` column needing to be renamed to `dataset`.

``` r
datasets_table <- datasets_table %>%
  rename(dataset = dataset_code)
```

The locations table is close as well, but using the benefit of hindsight I can tell you that four locations that are included in the diatom counts are not documented in the lake info table. The `mudata()` function (which combines all these tables) is *really* picky about referential integrity, so we have to add those locations here to avoid an error later.

``` r
locations_table <- lake_info %>%
  rename(location = lake, dataset = dataset_code) %>%
  bind_rows(
    tibble(
      dataset = c("ALERT", "ALERT", "MB", "ISACH"),
      location = c("A-LDL2", "A-Self0cm", "MB-WS", "I-C")
    )
  )
```

The parameter table doesn't explicitly exist yet, but we can create it by finding distinct combinations of `dataset` and `label` in the water chemistry table. The params table is a good place to put units, although the param identifier is not, so I've extracted the unit (if it exists) to a separate column.

``` r
params_water_chemistry <- water_chemistry %>%
  rename(dataset = dataset_code) %>%
  filter(!is.na(value)) %>%
  distinct(dataset, label) %>%
  extract(
    label, 
    into = c("param", "unit"), 
    regex = "(.*?)\\s*\\((.*?)\\)", 
    remove = FALSE
  ) %>%
  mutate(param = coalesce(param, label)) %>%
  mutate(param_type = "water_chemistry")
```

This is usually a good place to make sure that everything is measured in the same unit. Luckily, each parameter only has one unit!

``` r
params_water_chemistry %>%
  group_by(param) %>%
  summarise(n_units = n_distinct(unit)) %>%
  filter(n_units > 1) %>%
  pull(param)
```

    ## character(0)

The diatom counts are a little more complicated, since they are identified by both `diatom_taxon` and `taxon_code`. The mudata format needs one unique identifier for each parameter, and because not all `diatom_taxon` values have a `taxon_code` (which is consistent across the CDD), we can `coalesce()` the two columns. That is, we can *prefer* the value of `taxon_code`, but if it is missing, we can fall back on the value of `diatom_taxon` instead.

It isn't explicit anywhere I could find on the CDD website, but the values associated with each taxon appear to be percent relative abundance values (in general, they all add up to 100 for each location). Because there's a unit for the water chemistry parameters, I decided to add this tidbit of information to the diatom count parameters as well.

``` r
params_diatom_count <- diatom_counts %>%
  rename(dataset = dataset_code) %>%
  filter(!is.na(rel_abundance)) %>%
  distinct(dataset, diatom_taxon, taxon_code) %>%
  mutate(param = coalesce(na_if(taxon_code, "(blank)"), diatom_taxon)) %>%
  mutate(label = diatom_taxon) %>%
  mutate(param_type = "diatom_count") %>%
  mutate(unit = "% rel abund")
```

Finally, we can combine the two parameter tables to make the final table.

``` r
params_table <- bind_rows(params_water_chemistry, params_diatom_count)
```

All that's left is the data table! The `water_chemistry` and `diatom_counts` tables are almost in the right form, but we need to make sure that the `param` identifiers we assigned in the previous step are the same ones that get used in the data table. It's also nice to have the `unit` next to the `value`, which makes reading the table a bit easier.

``` r
data_water_chemistry <- water_chemistry %>%
  filter(!is.na(value)) %>%
  rename(dataset = dataset_code, location = lake) %>%
  left_join(params_table, by = c("dataset", "label")) %>%
  select(dataset, location, param, value, unit)

data_diatom_counts <- diatom_counts %>%
  filter(!is.na(rel_abundance)) %>%
  rename(dataset = dataset_code, location = lake, value = rel_abundance) %>%
  left_join(params_table, by = c("dataset", "diatom_taxon", "taxon_code")) %>%
  select(dataset, location, param, value, unit)

data_table <- bind_rows(data_water_chemistry, data_diatom_counts)
```

Now it's `mudata()` time! Usually the data table contains some kind of qualifying information about each measurement (e.g., time collected, depth collected in the water column), but because of how the original data were structured, there was no place for this information. The `mudata()` function calls these `x_columns` (they're usually on the `x` axis of plots), but here there are none (hence `character(0)`).

``` r
library(mudata2)
cdd_mudata <- mudata(
  data = data_table,
  locations = locations_table,
  params = params_table,
  datasets = datasets_table,
  x_columns = character(0)
)

cdd_mudata
```

    ## A mudata object aligned along <none>
    ##   distinct_datasets():  "ABK", "ALERT" ... and 13 more
    ##   distinct_locations(): "A-A", "A-AA" ... and 569 more
    ##   distinct_params():    "ACaltaic", "ACamoena" ... and 1145 more
    ##   src_tbls():           "data", "locations" ... and 3 more
    ## 
    ## tbl_data() %>% head():
    ## # A tibble: 6 x 5
    ##   dataset location param value unit 
    ##   <chr>   <chr>    <chr> <dbl> <chr>
    ## 1 BYL     BI-01    pH     6.70 <NA> 
    ## 2 BYL     BI-02    pH     6.30 <NA> 
    ## 3 BYL     BI-03    pH     7.00 <NA> 
    ## 4 BYL     BI-04    pH     7.00 <NA> 
    ## 5 BYL     BI-05    pH     7.00 <NA> 
    ## 6 BYL     BI-06    pH     6.70 <NA>

The default `print()` function suggests a few ways to interact with the object, but what we're after here is the `write_mudata()` function, which writes the collection of tables to a folder (or ZIP file or JSON file, but folder is usually the most useful).

``` r
write_mudata_dir(cdd_mudata, "cdd.mudata")
```

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1397</wp:post_id>
		<wp:post_date><![CDATA[2018-04-08 21:13:57]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-09 00:13:57]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[the-circumpolar-diatom-database-using-r-the-tidyverse-and-mudata2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="academia"><![CDATA[Academia]]></category>
		<category domain="post_tag" nicename="circumpolar-diatom-database"><![CDATA[Circumpolar Diatom Database]]></category>
		<category domain="post_tag" nicename="mudata"><![CDATA[mudata]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="tidyverse"><![CDATA[tidyverse]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Stratigraphic diagrams with tidypaleo &amp; ggplot2		</title>
		<link>https://apps.fishandwhistle.net/archives/1417</link>
		<pubDate>Tue, 14 Aug 2018 20:08:59 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">https://apps.fishandwhistle.net/?p=1417</guid>
		<description></description>
		<content:encoded>
				<![CDATA[This post covers creating stratigraphic diagrams using <a href="https://ggplot2.tidyverse.org/">ggplot2</a>, highlighting the helpers contained within the <a href="https://github.com/paleolimbot/tidypaleo">tidypaleo package</a>, which I've been using for the past few months to create diagrams. I chose the <strong>ggplot2</strong> framework because it is quite flexible and can be used to create almost any time-stratigraphic diagram except ones that involve multiple axes (we can have a fight about whether or not those are appropriate anyway, but if you absolutely need to create them I suggest you look elsewhere). To follow along, it will help considerably if you are already familiar with <strong>ggplot2</strong> (<a href="http://r4ds.had.co.nz/data-visualisation.html">a great tutorial can be found here</a>).

[markdown]
First, you will need to load the packages. For this tutorial, I will use **tidypaleo**, and the **tidyverse** (this includes **ggplot2**). Later, I will use **patchwork** to create more complex plots (you can install this from GitHub using `devtools::install_packages("thomasp85/patchwork")`, and **mudata2** because it contains more example data that is useful for illustrating plot data from multiple cores. I prefer the `theme_bw()` theme for **ggplot2** plots, so I will set this here as well. Setting the base font size to a small value (8 pt in this case) helps with strat diagrams.

``` r
library(tidyverse)
library(tidypaleo)
theme_set(theme_bw(8))
```

Next, you will need to load the example data. This vignette uses 3 datasets that are already in the correct format for plotting. These are datasets in the **tidypaleo** package: `alta_lake_geochem`, `keji_lakes_plottable`, and `halifax_lakes_plottable`. These represent three common dataset types for which stratigraphic plots are created.

``` r
data("alta_lake_geochem")
data("keji_lakes_plottable")
data("halifax_lakes_plottable")
```

Geochemical stratigraphic plots
-------------------------------

Geochemical stratigraphic plots include any kind of plot where each panel of the plot should be scaled to the minimum and maximum of the data. This includes (most) non-species abundance data. The example data, `alta_lake_geochem`, is a data frame in parameter-long (one row per measurement) form (you may need to use `gather()` to get your data into this form, which you can learn about in [this tutorial](http://r4ds.had.co.nz/tidy-data.html)). These measurements are from Alta Lake, British Columbia (Canada), and you can read more about them in the [journal article](https://doi.org/10.1007/s10933-016-9919-x).

``` r
alta_lake_geochem
#> # A tibble: 192 x 9
#>    location param depth   age value stdev units     n zone  
#>    <chr>    <chr> <dbl> <dbl> <dbl> <dbl> <chr> <int> <chr> 
#>  1 ALGC2    Cu    0.250  2015  76.0 NA    ppm       1 Zone 3
#>  2 ALGC2    Cu    0.750  2011 108    4.50 ppm       3 Zone 3
#>  3 ALGC2    Cu    1.25   2008 158   NA    ppm       1 Zone 3
#>  4 ALGC2    Cu    1.75   2003 169   NA    ppm       1 Zone 3
#>  5 ALGC2    Cu    2.50   1998 161   NA    ppm       1 Zone 3
#>  6 ALGC2    Cu    3.50   1982 129   NA    ppm       1 Zone 3
#>  7 ALGC2    Cu    4.50   1966  88.7  3.86 ppm       3 Zone 2
#>  8 ALGC2    Cu    5.50   1947  65.0 NA    ppm       1 Zone 2
#>  9 ALGC2    Cu    6.50   1922  62.3  9.53 ppm       3 Zone 2
#> 10 ALGC2    Cu    7.50   1896  48.0 NA    ppm       1 Zone 2
#> # ... with 182 more rows
```

Plotting a single core with `ggplot()` involves several steps:

-   The initial `ggplot()` call, where we set which columns will get mapped to the `x` and `y` values for each layer.
-   Two layers: `geom_lineh()` and `geom_point()`. We use `geom_lineh()` because our plots will be oriented vertically (`geom_line()` is designed for horizontal plots).
-   Specify how data are divided between facets using `facet_geochem_gridh()` (we use `_gridh()` to orient the panels horizontally rather than vertically).
-   Reverse the Y-axis, so that a depth of 0 is at the top.
-   Remove the X label, and set the Y label to "Depth (cm)".

``` r
alta_plot <- ggplot(alta_lake_geochem, aes(x = value, y = depth)) +
  geom_lineh() +
  geom_point() +
  scale_y_reverse() +
  facet_geochem_gridh(vars(param)) +
  labs(x = NULL, y = "Depth (cm)")

alta_plot
```

![](/wp-content/uploads/2018/08/unnamed-chunk-4-1.png)

That's it! Of course, there are many modifications that can be made, which are covered in the next few sections.

### Adding annotations

It is common to highlight certain depths or ages on strat diagrams, or to add things to various places on a plot. This is accomplished by adding additional layers to the plot. For example, highlighting the depth at which the zones change (according to the paper, this is at 4 cm and 16 cm).

``` r
alta_plot +
  geom_hline(yintercept = c(4, 16), col = "red", lty = 2, alpha = 0.7)
```

![](/wp-content/uploads/2018/08/unnamed-chunk-5-1.png)

The `geom_hline()` function creates its own data using the `yintercept` argument, but other geometries require us to create our own data. For example, highlighting the middle zone using `geom_rect()` would look like this:

``` r
zone_data <- tibble(ymin = 4, ymax = 16, xmin = -Inf, xmax = Inf)
alta_plot +
  geom_rect(
    mapping = aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax), 
    data = zone_data, 
    alpha = 0.2,
    fill = "blue",
    inherit.aes = FALSE
  )
```

![](/wp-content/uploads/2018/08/unnamed-chunk-6-1.png)

So far, each new layer we have added has been propogated to all facets. To restrict annotations to a specific facet, we need to include the facetting column (in this case, `param`) in the layer data. The element copper (Cu) has standards set by the Canadian government (ISQG of 35.7 ppm), and highlighting values that are outside that range can be done in a similar way to highlighting the middle zone.

``` r
cu_standard_data <- tibble(param = "Cu", xmin = 35.7, xmax = Inf, ymin = -Inf, ymax = Inf)
alta_plot +
  geom_rect(
    mapping = aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax), 
    data = cu_standard_data, 
    alpha = 0.2,
    fill = "red",
    inherit.aes = FALSE
  )
```

![](/wp-content/uploads/2018/08/unnamed-chunk-7-1.png)

To highlight the syntax that changed from the original plot, I've been using `alta_plot + ...`, however with layers such as these, it may be desirable to have the annotation appear under the original data. To do this, you would have to add your annotation layer (`geom_rect()`) before your data layer (`geom_lineh()` and `geom_point()`).

### Error bars

In the `alta_lake_geochem` data frame, there is a column for standard deviation (`stdev`). You can plot these values as error bars using `geom_errorbarh()`.

``` r
alta_plot + 
  geom_errorbarh(aes(xmin = value - stdev, xmax = value + stdev), height = 0.5)
```

![](/wp-content/uploads/2018/08/unnamed-chunk-8-1.png)

### Reordering facets

To control the order of the facets, the column on which they are facetted (`param`) must be a `factor()`, where the `levels` indicate the order. I suggest using `fct_relevel()` to do this, so that you only need to specify which items should go first. Usually I do this using a pipe (`%>%`) and `mutate()` to avoid assigning any intermediate variables.

``` r
alta_lake_geochem %>%
  mutate(param = fct_relevel(param, "Ti", "Cu", "C/N")) %>%
  ggplot(aes(x = value, y = depth)) +
  ...
```

    #> Warning: package 'bindrcpp' was built under R version 3.4.4

![](/wp-content/uploads/2018/08/unnamed-chunk-10-1.png)

### Plotting a subset of the data

The `halifax_geochem` data frame was designed so that it is a reasonable number of parameters to plot at one time. Usually you will have a data frame with many more parameters, which you can subset using `filter()` before calling `ggplot()`. Usually I do this using a pipe (`%>%`) to avoid assigning any intermediate variables.

``` r
alta_lake_geochem %>%
  filter(param %in% c("d15N", "d13C", "C/N")) %>%
  ggplot(aes(x = value, y = depth)) +
  ...
```

![](/wp-content/uploads/2018/08/unnamed-chunk-12-1.png)

### Using a y-axis with ages and depths

When plotting a single core, it is good practice to plot both ages and depths in the core. In **ggplot2**, this comes as a second axis generated by a helper function, `scale_y_depth_age()`, that uses an `age_depth_model()` to produce a second axis. For Alta Lake, the age depth model as published is included in the `alta_lake_bacon_ages` object. It's possible to set the age breaks using the `age_breaks` argument, and the labels using the `age_labels` argument (this is useful for adding error information to ages on a second axis). The axis title can be set using the `age_name` argument.

``` r
alta_adm <- age_depth_model(
  alta_lake_bacon_ages, 
  depth = depth_cm,
  age = 1950 - age_weighted_mean_year_BP
)

alta_plot +
  scale_y_depth_age(
    alta_adm,
    age_name = "Age (Year AD)"
  )
```

![](/wp-content/uploads/2018/08/unnamed-chunk-13-1.png)

### Adding units to facet labels

Ideally, we'd like to specify the unit in which each of these parameters was measured. This can be done (1) by renaming each parameter before it gets to `ggplot()`, or (2) by using the `units` argument of the `facet_geochem_*()` functions. I suggest using the second option because it maintains the order of the parameters that you've chosen, and keeps the automatic reformatting of `d13C`. The `units` argument takes a named character vector, which you can create using `c("parameter" = "unit")`. If most paremeters are in the same unit, you can pass the default unit using `default_unit = "unit"`, and if you need to suppress a unit for an item, you can use `NA`.

``` r
alta_plot +
  facet_geochem_gridh(
    vars(param),
    units = c("C/N" = NA, "Cu" = "ppm", "d13C" = "‰", "d15N" = "‰"),
    default_units = "%"
  )
```

![](/wp-content/uploads/2018/08/unnamed-chunk-14-1.png)

### Multiple cores

So far we've plotted one core. That's great, but often there is more than one core that needs to be plotted. There are two ways to go about this depending on how you would like identical parameters for each lake to be compared: using `facet_geochem_gridh()` will stretch the axis limits to fit values from both cores, and creating (and combining) two separate plots using `patchwork::wrap_plots()` will result in each panel stretching to the min/max of the data for that parameter. We'll illustrate both of these using some other data from Long Lake, Nova Scotia, that is contained in the **mudata2** package.

``` r
library(mudata2)
combined_data <- long_lake %>%
  tbl_data() %>%
  bind_rows(alta_lake_geochem) %>%
  filter(param %in% c("C/N", "d13C", "d15N"))

combined_data
#> # A tibble: 147 x 14
#>    dataset    location param depth value     sd units zone      n n_detect
#>    <chr>      <chr>    <chr> <dbl> <dbl>  <dbl> <chr> <chr> <int>    <int>
#>  1 long_lake… LL PC2   C/N    1.00  15.4 NA     <NA>  Unit…     1        1
#>  2 long_lake… LL PC2   C/N    6.00  15.8  0.290 <NA>  Unit…     2        2
#>  3 long_lake… LL PC2   C/N   11.0   24.0 NA     <NA>  Unit…     1        1
#>  4 long_lake… LL PC2   C/N   16.0   21.4 NA     <NA>  Unit…     1        1
#>  5 long_lake… LL PC2   C/N   21.0   23.2 NA     <NA>  Unit…     1        1
#>  6 long_lake… LL PC2   C/N   26.0   25.4 NA     <NA>  Unit…     1        1
#>  7 long_lake… LL PC2   C/N   43.0   27.2 NA     <NA>  Unit…     1        1
#>  8 long_lake… LL PC2   C/N   48.0   24.3 NA     <NA>  Unit…     1        1
#>  9 long_lake… LL PC2   C/N   53.0   30.2 NA     <NA>  Unit…     1        1
#> 10 long_lake… LL PC2   C/N   63.0   24.4 NA     <NA>  Unit…     1        1
#> # ... with 137 more rows, and 4 more variables: min_value <dbl>,
#> #   max_value <dbl>, age <dbl>, stdev <dbl>
```

First we'll go over the `facet_geochem_gridh()` approach, which adds `location` as another grouping variable. This will align parameters with themselves vertically, with one row on the plot per core. This usually the best option, as it makes values from one plot to another be directly comparable.

``` r
ggplot(combined_data, aes(x = value, y = depth)) +
  geom_lineh() +
  geom_point() +
  scale_y_reverse() +
  facet_geochem_gridh(vars(param), grouping = vars(location), scales = "free") +
  labs(x = NULL, y = "Depth (cm)")
```

![](/wp-content/uploads/2018/08/unnamed-chunk-16-1.png)

The other way to do this is to create two separate plots, and combine them using the **patchwork** package. This is a good option when values from one core to another are so different that placing them on the same axis results in trends being obscured, or when ages and depths are absolutely necessary for both plots (using `scale_y_depth_age()` doesn't work when combining locations using a facet).

``` r
alta_plot_1 <- combined_data %>% 
  filter(location == "ALGC2") %>% 
  ggplot(aes(x = value, y = depth)) +
  geom_lineh() +
  geom_point() +
  scale_y_reverse() +
  facet_geochem_gridh(vars(param), scales = "free") +
  labs(x = NULL, y = "Depth (cm)", title = "Alta Lake")

long_plot_2 <- combined_data %>% 
  filter(location == "LL PC2") %>% 
  ggplot(aes(x = value, y = depth)) +
  geom_lineh() +
  geom_point() +
  scale_y_reverse() +
  facet_geochem_gridh(vars(param), scales = "free") +
  labs(x = NULL, y = "Depth (cm)", title = "Long Lake")

library(patchwork)
wrap_plots(alta_plot_1, long_plot_2, ncol = 1)
```

![](/wp-content/uploads/2018/08/unnamed-chunk-17-1.png)

### Adding a dendrogram

It's fairly common to see a CONISS dendrogram at the right of plots, and so **tidypaleo** wouldn't be complete if it couldn't make that happen. For a variety of technical reasons, you have to specify that you need `depth` mapped to the `y` aesthetic for both `layer_zone_boundaries()` and `layer_dendrogram()`. Note that you'll have to do CONISS using `nested_chclust_coniss()`, which uses the **rioja** package to do the constrained cluster analysis. By default, this will calculate the number of plausible zones based on a broken stick simulation.

``` r
coniss <- alta_lake_geochem %>%
  nested_data(qualifiers = c(age, depth), key = param, value = value, trans = scale) %>%
  nested_chclust_coniss()

alta_plot +
  layer_dendrogram(coniss, aes(y = depth), param = "CONISS") +
  layer_zone_boundaries(coniss, aes(y = depth))
```

![](/wp-content/uploads/2018/08/unnamed-chunk-18-1.png)

### Using `facet_geochem_wraph()`

If you are familiar with `facet_grid()` in **ggplot2**, you will notice that `facet_geochem_gridh()` has done a few things differently, including automatically renaming `d13C` and `d15N` to the proper formatting. This function also rotates axis labels, because this is the only sure-fire way to make sure they don't overlap. You can un-rotate them using `rotate_axis_labels = 0`, and you can wrap panels in a defined number of rows or columns using `facet_geochem_wraph()`. This works well for plotting a larger number of parameters when there is only one core involved.

``` r
alta_plot + 
  facet_geochem_wraph(vars(param), rotate_axis_labels = 0, ncol = 4)
```

![](/wp-content/uploads/2018/08/unnamed-chunk-19-1.png)

### Going horizontal

Most stratigraphic diagrams are vertical, which is a reflection on the orientation of the core from which the data came. In many cases (especially when age is one of the axes), it may be adventageous to have the plots be horizontal, with time running from left to right along the x-axis. A few things need to change from our original plot, including the mapping, the line geometry, and the facet specification. Here I've used `facet_geochem_grid()` to stack the plots vertically, but you could also use `facet_geochem_wrap()`.

``` r
ggplot(alta_lake_geochem, aes(x = age, y = value)) +
  geom_line() +
  geom_point() +
  scale_y_reverse() +
  facet_geochem_grid(vars(param)) +
  labs(x = "Age (Year AD)", y = NULL)
```

![](/wp-content/uploads/2018/08/unnamed-chunk-20-1.png)

Species abundance stratigraphic plots
-------------------------------------

If our data is relative species abundance, there are a few more restrictions on the plot: each facet must represent values such that 10% abundance on one facet is equal to 10% in another facet, and facet names are often long enough that they must be rotated. The sample data for these exercises is `keji_lakes_plottable`, which again is in the form of one row per measurement. This dataset contains two lakes, Beaverskin Lake and Peskawa Lake, both located within Keji National Park, Nova Scotia, Canada ([Ginn et al. 2007](https://doi.org/10.1007/s10750-007-0644-3)).

``` r
data("keji_lakes_plottable")
keji_lakes_plottable
#> # A tibble: 202 x 5
#>    location        depth taxon                             count rel_abund
#>    <chr>           <dbl> <fct>                             <dbl>     <dbl>
#>  1 Beaverskin Lake 0.125 Asterionella ralfsii var. ameri…   0         0   
#>  2 Beaverskin Lake 0.125 Aulacoseira distans                7.00      3.02
#>  3 Beaverskin Lake 0.125 Aulacoseira lirata                 4.00      1.72
#>  4 Beaverskin Lake 0.125 Cyclotella stelligera              8.00      3.45
#>  5 Beaverskin Lake 0.125 Tabellaria flocculosa (strain I…   0         0   
#>  6 Beaverskin Lake 0.125 Other                            213        91.8 
#>  7 Beaverskin Lake 0.375 Asterionella ralfsii var. ameri…   0         0   
#>  8 Beaverskin Lake 0.375 Aulacoseira distans                8.00      3.25
#>  9 Beaverskin Lake 0.375 Aulacoseira lirata                 5.00      2.03
#> 10 Beaverskin Lake 0.375 Cyclotella stelligera             14.0       5.69
#> # ... with 192 more rows
```

The code to generate the plot is similar to our previous plots, except we use `geom_col_segsh()` (which instead of points or a path, draws lines connected to the x-axis), and `facet_abundanceh()` (which takes care of rotating facet labels, partially italicizing them, and setting the scales for each panel to start at 0.

``` r
keji_plot <- ggplot(keji_lakes_plottable, aes(x = rel_abund, y = depth)) +
  geom_col_segsh() +
  scale_y_reverse() +
  facet_abundanceh(vars(taxon), grouping = vars(location)) +
  labs(x = "Relative abundance (%)", y = "Depth (cm)")

keji_plot
```

![](/wp-content/uploads/2018/08/unnamed-chunk-22-1.png)

Most of the modifications you can make on these plots are identical to the modifications you can make on non-abundance plots (see previous section), except for a few that we describe below.

### Different styles

There are a few famous styles for abundance plots, because a few existing programs (like [Tilia](https://www.tiliait.com/), [C2](https://www.staff.ncl.ac.uk/stephen.juggins/software/C2Home.htm), and [rioja](https://cran.r-project.org/package=rioja)) have some very specific defaults that show up on repeat in the literature. These styles can be replicated by changing or adding to `geom_col_segsh()`. For example, the classic Tilia "area" style for pollen diagrams:

``` r
ggplot(keji_lakes_plottable, aes(x = rel_abund, y = depth)) +
  geom_areah() +
  scale_y_reverse() +
  facet_abundanceh(vars(taxon), grouping = vars(location)) +
  labs(x = "Relative abundance (%)", y = "Depth (cm)")
```

![](/wp-content/uploads/2018/08/strat-area.png)

Or for rioja-style diagrams, use `geom_col_segsh()` plus `geom_lineh()`:

``` r
ggplot(keji_lakes_plottable, aes(x = rel_abund, y = depth)) +
  geom_col_segsh() + 
  geom_lineh() +
  scale_y_reverse() +
  facet_abundanceh(vars(taxon), grouping = vars(location)) +
  labs(x = "Relative abundance (%)", y = "Depth (cm)")
```

![](/wp-content/uploads/2018/08/strat-rioja.png)

### Exaggerating low-abundance taxa

It's common to see some kind of geometry on abundance diagrams exaggerating low-abundance variation. You can use the `*_exaggerate()` varieties of `geom_lineh()`, `geom_areah()`, and `geom_point()` to display this information without affecting the scales. If you want to include it only for certain taxa, you can pass `data = keji_lakes_plottable %>% filter(...)` to only include certain measurements.

``` r
keji_plot +
  geom_lineh_exaggerate(exaggerate_x = 5, col = "grey70", lty = 2)
```

![](/wp-content/uploads/2018/08/strat-exagg.png)

### Adding non-abundance data

To add non-abundance data in the same figure, we need to create two plots and combine them using **patchwork**'s `wrap_plots()` function. In this example, I'm going to use PCA scores, computed by the `nested_prcomp()` function, and get them into parameter-long form using `gather()`. Note that when I create the plots, I make sure the labels still exist, and then I nix the labels inbetween the two plots using a few `theme()` modifications. It's important to make sure that the panel labels and scale lables are the same before removing them!

``` r
keji_pca_scores <- keji_lakes_plottable %>%
  group_by(location) %>%
  nested_data(qualifiers = depth, key = taxon, value = rel_abund, trans = sqrt) %>%
  nested_prcomp() %>%
  unnest(qualifiers, scores) %>%
  gather(key = component, value = value, starts_with("PC")) %>%
  filter(component %in% c("PC1", "PC2"))

keji_pca_plot <- ggplot(keji_pca_scores, aes(x = value, y = depth)) +
  geom_lineh() +
  geom_point() +
  scale_y_reverse() +
  facet_geochem_gridh(vars(component), grouping = vars(location)) +
  labs(x = NULL)
  
library(patchwork)
wrap_plots(
  keji_plot + 
    theme(strip.background = element_blank(), strip.text.y = element_blank()),
  keji_pca_plot +
    theme(axis.text.y.left = element_blank(), axis.ticks.y.left = element_blank()) +
    labs(y = NULL),
  nrow = 1,
  widths = c(4, 1)
)
```

![](/wp-content/uploads/2018/08/unnamed-chunk-23-1.png)

### Adding dendrograms

Adding a CONISS (or other) dendrogram created using `nested_chclust_coniss()` can be done in two ways: if there is already a non-abundance plot that exists, the dendrogram can be added to that plot using `layer_dendrogram()`. If there is not already a non-abundance plot, we need to create that plot. Then, the two plots can be combined using **patchwork**'s `wrap_plots()`.

``` r
keji_coniss <- keji_lakes_plottable %>%
  group_by(location) %>%
  nested_data(qualifiers = depth, key = taxon, value = rel_abund) %>%
  nested_chclust_coniss()

library(patchwork)

# method 1: use existing non-abundance plot
wrap_plots(
  keji_plot + 
    theme(strip.background = element_blank(), strip.text.y = element_blank()),
  keji_pca_plot +
    layer_dendrogram(keji_coniss, component = "CONISS", aes(y = depth)) +
    theme(axis.text.y.left = element_blank(), axis.ticks.y.left = element_blank()) +
    labs(y = NULL),
  nrow = 1,
  widths = c(2, 1)
)
```

![](/wp-content/uploads/2018/08/unnamed-chunk-24-1.png)

``` r
# method 2: create a standalone plot for CONISS
coniss_plot <- ggplot() +
  layer_dendrogram(keji_coniss, aes(y = depth)) +
  scale_y_reverse() +
  facet_geochem_gridh(vars("CONISS"), grouping = vars(location)) +
  labs(x = NULL)

wrap_plots(
  keji_plot + 
    theme(strip.background = element_blank(), strip.text.y = element_blank()),
  coniss_plot +
    theme(axis.text.y.left = element_blank(), axis.ticks.y.left = element_blank()) +
    labs(y = NULL),
  nrow = 1,
  widths = c(6, 1)
)
```

![](/wp-content/uploads/2018/08/unnamed-chunk-25-1.png)

Top/bottom/spatial diagrams
---------------------------

So far we have looked at data that have depth as an axis. Another sample strategy includes collecting cores with a small number of samples in each (such as top/bottom, or sometimes just a surface sample). A sample dataset of this type was collected by [Ginn et al. (2015)](https://doi.org/10.1080/10402381.2015.1013648) in lakes near Halifax, Nova Scotia. A subset of these measurements can be found in the `halifax_lakes_plottable` data frame.

``` r
data("halifax_lakes_plottable")
halifax_lakes_plottable
#> # A tibble: 114 x 5
#>    location      sample_type taxon                         count rel_abund
#>    <chr>         <chr>       <fct>                         <dbl>     <dbl>
#>  1 Anderson Lake bottom      Aulacoseira distans           26.0      4.65 
#>  2 Anderson Lake bottom      Eunotia exigua                13.5      2.42 
#>  3 Anderson Lake bottom      Fragilariforma exigua          6.50     1.16 
#>  4 Anderson Lake bottom      Tabellaria fenestrata         23.0      4.11 
#>  5 Anderson Lake bottom      Tabellaria flocculosa (stra…  21.0      3.76 
#>  6 Anderson Lake bottom      Other                        469       83.9  
#>  7 Anderson Lake top         Aulacoseira distans            8.50     1.87 
#>  8 Anderson Lake top         Eunotia exigua                 0        0    
#>  9 Anderson Lake top         Fragilariforma exigua          1.50     0.330
#> 10 Anderson Lake top         Tabellaria fenestrata         26.0      5.71 
#> # ... with 104 more rows
```

These diagrams are essentially bar charts, and use `geom_colh()` to draw their geometry (this works better than `geom_col_segsh()`, which only draws a thin line). We can separate top/bottom samples by setting an additional aesthetic, `fill = sample_type`, and make sure they dodge vertically by setting `position = "dodgev"`.

``` r
halifax_plot <- ggplot(halifax_lakes_plottable, aes(x = rel_abund, y = location, fill = sample_type)) +
  geom_colh(width = 0.5, position = "dodgev") +
  facet_abundanceh(vars(taxon)) +
  labs(x = "Relative abundance (%)", y = NULL, fill = "Sample Type")

halifax_plot
```

![](/wp-content/uploads/2018/08/unnamed-chunk-27-1.png)

### Ordering lakes on the y-axis

Diagrams with a discrete axis (in this case, the y-axis) tend to be most efffective when they are ordered in some way. By default, **ggplot2** orders them alphabetically and lists them bottom to top. We can change this order in a similar way to changing the facet order, which is to convert the column containing the y-variable to a `factor()` using `fct_relevel()`. In this context, `fct_rev()` is also useful, as it will result in a top-to-bottom ordering rather than a bottom-to-top one.

``` r
halifax_lakes_plottable %>%
  mutate(location = fct_relevel(location, "Bell Lake", "Bayers", "Little Springfield") %>% fct_rev()) %>%
  ggplot(aes(x = rel_abund, y = location, fill = sample_type)) +
  ...
```

![](/wp-content/uploads/2018/08/unnamed-chunk-29-1.png)

### Adding a dendrogram

Adding a dendrogram to a plot with a discrete axis is only tricky because the order of the axes depends on the dendrogram itself. This means that the axes for both plots rely on the result of the clustering.

``` r
halifax_clust <- halifax_lakes_plottable %>%
  filter(sample_type == "top") %>%
  nested_data(qualifiers = location, key = taxon, value = rel_abund) %>%
  nested_hclust(method = "average")

dendro_order <- halifax_clust %>%
  unnest(qualifiers, dendro_order) %>%
  arrange(dendro_order) %>% 
  pull(location)

library(patchwork)
wrap_plots(
  halifax_plot + 
    scale_y_discrete(limits = dendro_order) +
    theme(legend.position = "left"),
  ggplot() + 
    layer_dendrogram(halifax_clust, aes(y = location)) +
    scale_y_discrete(limits = dendro_order) +
    labs(x = "Dispersion", y = NULL) +
    theme(axis.text.y.left = element_blank(), axis.ticks.y.left = element_blank()),
  widths = c(4, 1)
)
```

![](/wp-content/uploads/2018/08/unnamed-chunk-30-1.png)

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1417</wp:post_id>
		<wp:post_date><![CDATA[2018-08-14 17:08:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-14 20:08:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[stratigraphic-diagrams-with-tidypaleo-ggplot2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ggplot"><![CDATA[ggplot]]></category>
		<category domain="post_tag" nicename="paleolimnology"><![CDATA[paleolimnology]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Pourbaix-ish diagrams using PHREEQC and R		</title>
		<link>https://apps.fishandwhistle.net/archives/1445</link>
		<pubDate>Thu, 16 Aug 2018 19:03:00 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">https://apps.fishandwhistle.net/?p=1445</guid>
		<description></description>
		<content:encoded>
				<![CDATA[A side project of mine recently has been to play with <a href="https://wwwbrr.cr.usgs.gov/projects/GWC_coupled/phreeqc/phreeqc3-html/phreeqc3.htm">PHREEQC</a>, which is a powerful geochemical modelling platform put out by the USGS. In order to make the <a href="https://cran.r-project.org/package=phreeqc">R package for phreeqc</a> more accessible, I've started to wrap a few common uses of PHREEQC in a new R package, <a href="https://github.com/paleolimbot/tidyphreeqc">tidyphreeqc</a>. In particular, I'm interested in using PHREEQC to take a look at the classic <a href="https://en.wikipedia.org/wiki/Pourbaix_diagram">Pourbaix diagram</a>, which is almost always represented in pure solution at a particular concentration of the target element, at 25°C.

[markdown]

![Pourbaix diagram for Mn from Wikimedia Commons](https://upload.wikimedia.org/wikipedia/commons/6/66/Pourbaix_diagram_for_Manganese.svg)

([Pourbaix diagram for Mn from Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Pourbaix_diagram_for_Manganese.svg))

There's obviously more to it than that. The lines on the diagram change with temperature, concentration, and other properties of the solution in question. Also, some phases on the diagram are solid phases, and others are aqueous. Diagrams are represented like this probably because they are relatively time-consuming to create. But what if we could generate a few thousand hypothetical solutions and see what happens? This is a problem that is well-suited for PHREEQC.

The packages I'm using in this post are the [tidyverse](https://tidyverse.org) and [tidyphreeqc](https://github.com/paleolimbot/tidyphreeqc), the later of which you'll have to install using `devtools::install_github()`.

``` r
library(tidyverse)
# devtools::install_github("paleolimbot/tidyphreeqc")
library(tidyphreeqc)
# the minteq database is slightly more comprehensive than the default
phr_use_db_minteq()
```

The basic usage of **tidyphreeqc** is to call `phr_run()` with some input arguments. For our purposes, we're going to create a solution, then look at the output (the default units for components of the solution are mmol/kg water).

``` r
phr_run(
  phr_solution(Mn = 0.1)
) %>%
  phr_print_output()
```

    ## ------------------------------------
    ## Reading input data for simulation 1.
    ## ------------------------------------
    ## 
    ##  SOLUTION 1
    ##      Mn    0.1
    ## -------------------------------------------
    ## Beginning of initial solution calculations.
    ## -------------------------------------------
    ## 
    ## Initial solution 1.  
    ## 
    ## -----------------------------Solution composition------------------------------
    ## 
    ##  Elements           Molality       Moles
    ## 
    ##  Mn                1.000e-04   1.000e-04
    ## 
    ## ----------------------------Description of solution----------------------------
    ## 
    ##                                        pH  =   7.000    
    ##                                        pe  =   4.000    
    ##                         Activity of water  =   1.000
    ##                  Ionic strength (mol/kgw)  =   2.001e-04
    ##                        Mass of water (kg)  =   1.000e+00
    ##                  Total alkalinity (eq/kg)  =   2.499e-08
    ##                     Total carbon (mol/kg)  =   0.000e+00
    ##                        Total CO2 (mol/kg)  =   0.000e+00
    ##                          Temperature (oC)  =  25.00
    ##                   Electrical balance (eq)  =   2.000e-04
    ##  Percent error, 100*(Cat-|An|)/(Cat+|An|)  =  99.90
    ##                                Iterations  =   4
    ##                                   Total H  = 1.110124e+02
    ##                                   Total O  = 5.550622e+01
    ## 
    ## ----------------------------Distribution of species----------------------------
    ## 
    ##                                                Log       Log       Log    mole V
    ##    Species          Molality    Activity  Molality  Activity     Gamma   cm3/mol
    ## 
    ##    OH-             1.021e-07   1.005e-07    -6.991    -6.998    -0.007     (0)  
    ##    H+              1.016e-07   1.000e-07    -6.993    -7.000    -0.007      0.00
    ##    H2O             5.551e+01   1.000e+00     1.744    -0.000     0.000     18.07
    ## H(0)          1.416e-25
    ##    H2              7.079e-26   7.079e-26   -25.150   -25.150     0.000     (0)  
    ## Mn(2)         1.000e-04
    ##    Mn+2            9.998e-05   9.372e-05    -4.000    -4.028    -0.028     (0)  
    ##    MnOH+           2.448e-08   2.409e-08    -7.611    -7.618    -0.007     (0)  
    ##    Mn(OH)3-        1.510e-18   1.485e-18   -17.821   -17.828    -0.007     (0)  
    ## Mn(3)         3.366e-26
    ##    Mn+3            3.366e-26   2.916e-26   -25.473   -25.535    -0.062     (0)  
    ## Mn(6)         0.000e+00
    ##    MnO4-2          0.000e+00   0.000e+00   -50.440   -50.468    -0.028     (0)  
    ## Mn(7)         0.000e+00
    ##    MnO4-           0.000e+00   0.000e+00   -55.845   -55.852    -0.007     (0)  
    ## O(0)          0.000e+00
    ##    O2              0.000e+00   0.000e+00   -42.080   -42.080     0.000     (0)  
    ## 
    ## ------------------------------Saturation indices-------------------------------
    ## 
    ##   Phase               SI** log IAP   log K(298 K,   1 atm)
    ## 
    ##   Birnessite      -11.63      6.46   18.09  MnO2
    ##   Bixbyite         -8.46     -9.07   -0.61  Mn2O3
    ##   Hausmannite      -9.62     51.92   61.54  Mn3O4
    ##   Manganite        -4.30     -4.54   -0.24  MnOOH
    ##   Nsutite         -11.04      6.46   17.50  MnO2
    ##   O2(g)           -39.12     44.00   83.12  O2
    ##   Pyrocroite       -5.12      9.97   15.09  Mn(OH)2
    ##   Pyrolusite       -9.40      6.46   15.86  MnO2
    ## 
    ## **For a gas, SI = log10(fugacity). Fugacity = pressure * phi / 1 atm.
    ##   For ideal gases, phi = 1.
    ## 
    ## ------------------
    ## End of simulation.
    ## ------------------
    ## 
    ## ------------------------------------
    ## Reading input data for simulation 2.
    ## ------------------------------------
    ## 
    ## ---------------------------------
    ## End of Run after 1.79396 Seconds.
    ## ---------------------------------

Alright! This suggests that the species we should consider in the water are Mn<sup>+2</sup>, MnOH<sup>+</sup>, Mn(OH)<sub>3</sub><sup>-</sup>, Mn<sup>+3</sup>, MnO<sub>4</sub><sup>-2</sup>, and MnO<sub>4</sub><sup>-</sup>. Additionally, there are a number of solid phases that should be considered. Usually when running a PHREEQC run, its best to look at the output of one or two equilibrium calculations to see what species are being considered (if you change databases, this list will change! You can also add your own phases/reactions if you know what you're up to with PHREEQC).

To get machine-readable output, we need to add things to the `phr_selected_output()` block. There are [extensive instructions](https://wwwbrr.cr.usgs.gov/projects/GWC_coupled/phreeqc/phreeqc3-html/phreeqc3-38.htm) on this block in the documentation, but the gist of it is that for equilibrium calculations, soluble species show up in the `activities` argument, and solid phases show up in the `saturation_indicies` argument (and you have to use the mineral name, not the chemical formula).

``` r
phr_run(
  phr_solution(Mn = 0.1),
  phr_selected_output(
    activities = c("Mn+2", "MnOH+", "Mn(OH)3-", "Mn+3", "MnO4-2", "MnO4-"),
    saturation_indices = c(
      "Birnessite", "Bixbyite", "Hausmannite", "Manganite",
      "Nsutite", "Pyrocroite", "Pyrolusite"
    ),
    temp = TRUE
  )
) %>%
  as_tibble() %>%
  select(pH, pe, `temp(C)`, starts_with("la_"), starts_with("si_"))
```

    ## # A tibble: 1 x 16
    ##      pH    pe `temp(C)` `la_Mn+2` `la_MnOH+` `la_Mn(OH)3-` `la_Mn+3`
    ##   <dbl> <dbl>     <dbl>     <dbl>      <dbl>         <dbl>     <dbl>
    ## 1  7.00  4.00      25.0     -4.03      -7.62         -17.8     -25.5
    ## # ... with 9 more variables: `la_MnO4-2` <dbl>, `la_MnO4-` <dbl>,
    ## #   si_Birnessite <dbl>, si_Bixbyite <dbl>, si_Hausmannite <dbl>,
    ## #   si_Manganite <dbl>, si_Nsutite <dbl>, si_Pyrocroite <dbl>,
    ## #   si_Pyrolusite <dbl>

You'll notice that all the things you put in the `activities` list are prefixed with `la_` (log activity), and all the things from the `solution_indicies` get prefixed with `si_`. If you misspell something (or a perfectly reasonable species isn't in the database), you will get all `NA` values in the column.

The real power of PHREEQC is that it can run lots of simulations very quickly, and `phr_solution_list()` can generate lots of solutions along a gradient. For example, we could take a look at species of Mn in solution along a pH gradient:

``` r
phr_run(
  phr_solution_list(Mn = 0.1, pe = 4, pH = seq(1, 13, by = 0.1)),
  phr_selected_output(
    activities = c("Mn+2", "MnOH+", "Mn(OH)3-", "Mn+3", "MnO4-2", "MnO4-")
  )
)  %>%
  as_tibble() %>%
  select(pH, starts_with("la_")) %>%
  gather(key = "species", value = "log_activity", -pH) %>%
  mutate(activity = 10^log_activity) %>%
  ggplot(aes(pH, activity, col = species)) +
  geom_line()
```

    ## Warning: package 'bindrcpp' was built under R version 3.4.4

![](/wp-content/uploads/2018/08/pH-1.png)

Or, by passing a vector of temperature values, we could look at the temperature-pH relationship:

``` r
phr_run(
  phr_solution_list(Mn = 0.1, temp = c(25, 50, 75), pH = seq(1, 13, by = 0.1)),
  phr_selected_output(
    activities = c("Mn+2", "MnOH+", "Mn(OH)3-", "Mn+3", "MnO4-2", "MnO4-"),
    temp = TRUE
  )
)  %>%
  as_tibble() %>%
  select(pH, temp = `temp(C)`, starts_with("la_")) %>%
  gather(key = "species", value = "log_activity", -pH, -temp) %>%
  mutate(activity = 10^log_activity) %>%
  ggplot(aes(pH, activity, col = species, lty = factor(temp))) +
  geom_line()
```

![](/wp-content/uploads/2018/08/temp-ph-1.png)

Finally, by looking at pH, pe, and temperature, we can build a 3D pourbaix diagram! The code for this is quite complex because it involves converting regions into polygons using **raster** and **sf**, but you can find it at [this gist](https://gist.github.com/paleolimbot/e854c098c6d95330c361014276b5a8db). It's an interesting perspective on the Pourbaix diagram, because it is asking a slightly different question. Rather than "what is dominant", a pH-pe diagram based on PHREEQC can ask both "what species is most likely to occur in the water", and "what species is most likely to precipitate". I've done this animation along the temperature axis, but it's possible to do this under many different scenarios using PHREEQC.

![](/wp-content/uploads/2018/08/pourbaix-anim-1.gif)
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1445</wp:post_id>
		<wp:post_date><![CDATA[2018-08-16 16:03:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-08-16 19:03:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[pourbaix-ish-diagrams-using-phreeqc-and-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="geochemistry"><![CDATA[geochemistry]]></category>
		<category domain="post_tag" nicename="phreeqc"><![CDATA[PHREEQC]]></category>
		<category domain="post_tag" nicename="pourbaix"><![CDATA[pourbaix]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="post_tag" nicename="tidyphreeqc"><![CDATA[tidyphreeqc]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Reconstructing a coagulant optimization model using R and Shiny		</title>
		<link>https://apps.fishandwhistle.net/archives/1461</link>
		<pubDate>Wed, 30 Jan 2019 22:02:25 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">https://apps.fishandwhistle.net/?p=1461</guid>
		<description></description>
		<content:encoded>
				<![CDATA[<p>Inspired by <a href="https://twitter.com/lindserinanders">Lindsay Anderson’s</a> suggestion that “somebody code up that <a href="http://doi.org/10.1002/j.1551-8833.1997.tb08229.x">Edwards 1997 model</a>”, I successfully avoided proofreading my Ph.D. thesis proposal for much of a day doing just that. The paper is a classic paper, and one of many that attempt to predict dissolved organic carbon removal using easily measurable water quality parameters (in this case, absorbance of UV radiation at 254 nm, influent DOC concentration, and coagulant dose). Lindsay has a <a href="http://doi.org/10.1021/acs.est.6b04889">great paper</a> talking about why coagulant dosing is important in an era of rapidly changing source water quality!</p>

<p>I was curious if there was enough information in the paper to replicate its results, having just attended <a href="https://resources.rstudio.com/rstudio-conf-2019/r-markdown-the-bigger-picture">Garret Grolemund's fantastic talk at RStudio::conf(2019)</a> about the reproducibility crisis. To make a long story short, the coefficients in the paper seem to hold up, although there isn’t any data to validate the model fits. Feel free to skip straight to the <a href="https://paleolimbot.shinyapps.io/edwards97/">shiny app</a> to skip the math bits.</p>

<p>The paper presents a non-linear set of equations that describe how organic carbon interacts with a coagulant (in this case, an iron- or alum-based one). The equations are summarised in their final form part-way through the paper:</p>

<p><span class="math display">\[
\frac{\text{DOC}_\text{initial}(1 - \text{SUVA} \cdot K_1 - K_2) - \text{[C]}_\text{eq}}{M} = 
  \frac{a b \text{[C]}_\text{eq}}{1 + b \text{[C]}_\text{eq}}
\]</span></p>
<p>Here, <span class="math inline">\(a\)</span> is related to pH via a 3rd-degree polynomial (<span class="math inline">\(a = x_3\text{pH}^3 + x_2\text{pH}^2 + x_1\text{pH}\)</span>). The even though the paper is trying to predict the final DOC conentration (C<sub>eq</sub>), the equation never appears in the article in its solved form! This is because the solving happened using Excel Solver, and so it never needed to be. To do this in R, however, we’ll need a solved form.</p>
<p>The details: Let <span class="math inline">\(S = 1 - \text{SUVA} \cdot K_1 - K_2\)</span>, <span class="math inline">\(D = \text{DOC}_\text{initial}\)</span>, <span class="math inline">\(C = \text{[C]}_\text{eq}\)</span></p>
<p><span class="math display">\[
\frac{DS - C}{M} = \frac{abC}{1 + bC}
\]</span></p>
<p>Cross-multiply:</p>
<p><span class="math display">\[
(DS - C) (1 + bC) = MabC
\]</span></p>
<p>Expand left side, subtract <span class="math inline">\(MabC\)</span></p>
<p><span class="math display">\[
DS + DSbC - C - bC^2 - MabC = 0
\]</span></p>
<p>Simplify to a quadratic:</p>
<p><span class="math display">\[
-bC^2 + (DSb - 1 - Mab)C + DS = 0
\]</span></p>
<p>Use the quadratic formula <span class="math inline">\(x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}\)</span>:</p>
<p><span class="math display">\[
C = \frac{ -(DSb - 1 - Mab) \pm \sqrt{(DSb - 1 - Mab)^2 -4(-b)(DS)} }{2(-b)}
\]</span></p>

<p>Note that this solution requires that <span class="math inline">\(b \ne 0\)</span> and <span class="math inline">\(M \ne 0\)</span>. The b paramter is one we set, so that can be arranged, but the fact that a zero dose is not allowed suggests that the model isn’t valid when no dose (or low dose, probably) is applied. If you play with the model parameters on the <a href="https://paleolimbot.shinyapps.io/edwards97/">shiny app</a>, you’ll notice it’s possible to get a higher final dose than initial dose…this is probably why.</p>

[markdown]
Instead of plugging back in the substitutions (which seems unlikely to go right without a lot of checking), I'll code them in such that we have an R function that can predict the final DOC concentration given the model inputs. I've included the coefficients for Edwards' general model for alum as the default parameter arguments.

``` r
param_S <- function(SUVA, K1, K2) 1 - SUVA * K1 - K2
param_a <- function(pH, x1, x2, x3) pH^3 * x3 + pH^2 * x2 + pH * x1
param_SUVA <- function(UV254_per_cm, DOC_mg_L) 100 * UV254_per_cm / DOC_mg_L
param_UV254_per_cm = function(SUVA, DOC_mg_L) SUVA * DOC_mg_L / 100

doc_final <- function(dose_mmol_L, DOC_initial_mg_L, pH, UV254_per_cm, 
                      K1 = -0.054, K2 = 0.54, x1 = 383, x2 = -98.6, x3 = 6.42, b = 0.107,
                      root = c("plus", "minus")) {
  root <- match.arg(root)
  coeff <- unname(c(plus = 1, minus = -1)[root])
  
  D <- DOC_initial_mg_L
  M <- dose_mmol_L
  SUVA <- param_SUVA(UV254_per_cm, DOC_initial_mg_L)
  S <- param_S(SUVA, K1, K2)
  a <- param_a(pH, x1, x2, x3)
  
  first_term <- -(D * S * b - 1 - M * a * b)
  sqrt_term <- sqrt((D * S * b - 1 - M * a * b)^2 - 4 * (-b * D * S))
  denominator <- 2 * (-b)
  
  Ceq <- (first_term + coeff * sqrt_term) / denominator
  Ceq
}
```

In the paper, it is hinted that a number of source waters were tested, although the data is not available to validate the model fits. Still, the basic parameters are provided, so we can see if the model produces reasonable results.

``` r
waters <- tribble(
  ~water_number,  ~SUVA,   ~DOC_initial_mg_L,
    1, 1.42,   2.6,
    2,  2.2,  2.63,
    3, 3.08,   2.3,
    4, 2.66,   2.7,
    5, 3.52,  2.56,
    6,  3.1,   3.1,
    7, 2.71,  3.94,
    8, 2.77,   4.8,
    9, 3.27,   4.4,
   10, 3.43,   6.5,
   11, 2.43,  12.7,
   12, 3.65,  11.3,
   13, 4.09,  11.6,
   14, 4.67,  14.3,
   15, 6.11,  8.03,
   16, 5.11, 26.54
) %>%
  mutate(
    label = paste(DOC_initial_mg_L, "mg/L, SUVA:", SUVA),
    UV254_per_cm = param_UV254_per_cm(SUVA, DOC_initial_mg_L)
  )

waters
```

    ## # A tibble: 16 x 5
    ##    water_number  SUVA DOC_initial_mg_L label                  UV254_per_cm
    ##           <dbl> <dbl>            <dbl> <chr>                         <dbl>
    ##  1            1  1.42             2.6  2.6 mg/L, SUVA: 1.42         0.0369
    ##  2            2  2.2              2.63 2.63 mg/L, SUVA: 2.2         0.0579
    ##  3            3  3.08             2.3  2.3 mg/L, SUVA: 3.08         0.0708
    ##  4            4  2.66             2.7  2.7 mg/L, SUVA: 2.66         0.0718
    ##  5            5  3.52             2.56 2.56 mg/L, SUVA: 3.52        0.0901
    ##  6            6  3.1              3.1  3.1 mg/L, SUVA: 3.1          0.0961
    ##  7            7  2.71             3.94 3.94 mg/L, SUVA: 2.71        0.107 
    ##  8            8  2.77             4.8  4.8 mg/L, SUVA: 2.77         0.133 
    ##  9            9  3.27             4.4  4.4 mg/L, SUVA: 3.27         0.144 
    ## 10           10  3.43             6.5  6.5 mg/L, SUVA: 3.43         0.223 
    ## 11           11  2.43            12.7  12.7 mg/L, SUVA: 2.43        0.309 
    ## 12           12  3.65            11.3  11.3 mg/L, SUVA: 3.65        0.412 
    ## 13           13  4.09            11.6  11.6 mg/L, SUVA: 4.09        0.474 
    ## 14           14  4.67            14.3  14.3 mg/L, SUVA: 4.67        0.668 
    ## 15           15  6.11             8.03 8.03 mg/L, SUVA: 6.11        0.491 
    ## 16           16  5.11            26.5  26.54 mg/L, SUVA: 5.11       1.36

In addition to a number of source waters, I'll also try a number of parameter combinations.

``` r
params <- list(
  pH = seq(3, 8, by = 1),
  AlSO4_mg_L = seq(5, 60, by = 1)
) %>%
  cross_df()

params
```

    ## # A tibble: 336 x 2
    ##       pH AlSO4_mg_L
    ##    <dbl>      <dbl>
    ##  1     3          5
    ##  2     4          5
    ##  3     5          5
    ##  4     6          5
    ##  5     7          5
    ##  6     8          5
    ##  7     3          6
    ##  8     4          6
    ##  9     5          6
    ## 10     6          6
    ## # … with 326 more rows

Finally, we can "run" the model. In this case, I coded the model such that it was vectorized, so given a data frame with all the conbinations, the model results can all be generated in one function call.

``` r
model_output <- crossing(waters, params) %>%
  mutate(
    AlSO4_mmol_L = AlSO4_mg_L / 123.04,
    DOC_final_mg_L = doc_final(
      dose_mmol_L = AlSO4_mmol_L, 
      DOC_initial_mg_L = DOC_initial_mg_L,
      pH = pH,
      UV254_per_cm = UV254_per_cm,
      root = "minus"
    )
  )

model_output
```

    ## # A tibble: 5,376 x 9
    ##    water_number  SUVA DOC_initial_mg_L label UV254_per_cm    pH AlSO4_mg_L
    ##           <dbl> <dbl>            <dbl> <chr>        <dbl> <dbl>      <dbl>
    ##  1            1  1.42              2.6 2.6 …       0.0369     3          5
    ##  2            1  1.42              2.6 2.6 …       0.0369     4          5
    ##  3            1  1.42              2.6 2.6 …       0.0369     5          5
    ##  4            1  1.42              2.6 2.6 …       0.0369     6          5
    ##  5            1  1.42              2.6 2.6 …       0.0369     7          5
    ##  6            1  1.42              2.6 2.6 …       0.0369     8          5
    ##  7            1  1.42              2.6 2.6 …       0.0369     3          6
    ##  8            1  1.42              2.6 2.6 …       0.0369     4          6
    ##  9            1  1.42              2.6 2.6 …       0.0369     5          6
    ## 10            1  1.42              2.6 2.6 …       0.0369     6          6
    ## # … with 5,366 more rows, and 2 more variables: AlSO4_mmol_L <dbl>,
    ## #   DOC_final_mg_L <dbl>

Finally, we can visualize!

``` r
ggplot(model_output, aes(x = AlSO4_mg_L, y = DOC_final_mg_L, col = factor(pH))) +
  geom_hline(aes(yintercept = DOC_initial_mg_L, lty = "Initial DOC"), data = waters, alpha = 0.7) +
  scale_linetype_manual(values = 2) +
  geom_line() +
  scale_color_brewer(direction = -1) +
  theme_bw() +
  facet_wrap(vars(label), scales = "free_y") +
  scale_x_continuous(expand = expand_scale(0, 0)) +
  theme(legend.position = "bottom") +
  labs(x = expression(AlSO[4]~(mg/L)), y = "DOC (mg/L)", col = "pH", linetype = NULL)
```

![](/wp-content/uploads/2019/01/unnamed-chunk-5-1.png)

If you have't already, check out the [shiny app](https://paleolimbot.shinyapps.io/edwards97/) and play with a few of the parameters in the model! In short, it would have been nice to have the data to validate the model, although to Edwards' credit, it *was* 1997.

[/markdown]

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1461</wp:post_id>
		<wp:post_date><![CDATA[2019-01-30 18:02:25]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-01-30 22:02:25]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[reconstructing-a-coagulant-optimization-model-using-r-and-shiny]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="academia"><![CDATA[Academia]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Public Data Dive: 2018 Boeing 737 MAX flights		</title>
		<link>https://apps.fishandwhistle.net/archives/1475</link>
		<pubDate>Sun, 17 Mar 2019 20:15:49 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">https://apps.fishandwhistle.net/?p=1475</guid>
		<description></description>
		<content:encoded>
				<![CDATA[The recent grounding of almost all <a href="https://en.wikipedia.org/wiki/Boeing_737_MAX">Boeing 737 MAX-series aircraft</a> in the world is, according to a recent CBC commentator, unprecedented. I’m not an aircraft expert (or even a hobbyist), but I do love data and mining publicly-available datasets. Inspired by the <a href="https://github.com/hadley/nycflights13">nycflights13 R package</a> (a dataset of all the flights in and out of New York City in 2013) and the <a href="https://www.flightradar24.com/blog/flightradar24-data-regarding-lion-air-flight-jt610/">FlightRadar24 blog post regarding Lion Air flight JT610</a>, I thought I would see what information is accessible to the public about flights that used the 737 MAX-series aircraft.

[markdown]
The [nycflights13 R package](https://github.com/hadley/nycflights13) uses the [Bureau of Transportation Statistics Reporting Carrier On-Time Performance](https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236) and the [FAA Releasable Aircraft](http://www.faa.gov/licenses_certificates/aircraft_certification/aircraft_registry/releasable_aircraft_download/) datasets, both of which are downloadable for free. As per my usual MO, I'll be using the [tidyverse](http://tidyverse.org) to script up the analysis. Because I'll be reading in a lot of foreign CSVs with weird names, I'll also make myself a name cleaner-upper to make `nice_columns` from `BADLY FORMATTED-COLUMNS`.

``` r
library(tidyverse)
nice_names <- . %>%
  str_to_lower() %>%
  str_replace_all("[- ]", "_")
```

I opted to get the plane information first. This code was heavily inspired by the scripts in the `data-raw/` folder of the [nycflights13 R package](https://github.com/hadley/nycflights13).

``` r
planes_src <- "http://registry.faa.gov/database/yearly/ReleasableAircraft.2018.zip"
planes_tmp <- tempfile(fileext = ".zip")
curl::curl_download(planes_src, planes_tmp)
dir.create("planes")
unzip(planes_tmp, exdir = "planes")
```

The "planes" folder now contains several oddly named files, of which we are interested in two: "ACFTREF.txt" (airplane model information), and "planes/MASTER.txt" (all registered planes in the US). According to the [emergency order by the FAA](https://www.faa.gov/news/updates/media/Emergency_Order.pdf), the models that were grounded were the Boeing 737-8 and 737-9, although for this analysis I'll include all 737s for comparison. To do this, I'll need to build a clean table of plane model info for the planes we're interested in.

``` r
planes_models <- read_csv(
  "planes/ACFTREF.txt",
  col_types = cols(
    .default = col_character(),
    X12 = col_skip()
  ),
  # there are quotation marks as parts of values in this table
  quote = ''
) %>%
  rename_all(nice_names)

planes_models_keep <- planes_models %>%
  filter(mfr == "BOEING", str_detect(model, "^737")) %>%
  mutate(model_label = if_else(str_detect(model, "^737-[89]$"), "737 MAX", "737")) %>%
  select(mfr_mdl_code = code, model_label, model_mfr = mfr, model)

planes_models_keep
```

    ## # A tibble: 606 x 4
    ##    mfr_mdl_code model_label model_mfr model  
    ##    <chr>        <chr>       <chr>     <chr>  
    ##  1 1382526      737         BOEING    737-66N
    ##  2 1382527      737         BOEING    737-7K2
    ##  3 1382528      737         BOEING    737-9K2
    ##  4 1382529      737         BOEING    737-8DR
    ##  5 1382530      737         BOEING    737-8DV
    ##  6 1382531      737         BOEING    737-7HF
    ##  7 1382532      737         BOEING    737-46J
    ##  8 1382540      737         BOEING    737-6Q8
    ##  9 1383333      737         BOEING    737-2E7
    ## 10 1384400      737         BOEING    737-7FB
    ## # … with 596 more rows

Next, I'll use this to create a table of planes that we're interested in. I modify the `n_number` to `paste0("N", n_number)` here because the `tail_num` variable in the flights dataset below has the tail number in that form.

``` r
planes_master <- read_csv(
  "planes/MASTER.txt",
  col_types = cols(
    .default = col_character(),
    `YEAR MFR` = col_integer(),
    X35 = col_skip()
  )
) %>%
  rename_all(nice_names)

planes_keep <- planes_master %>%
  inner_join(planes_models_keep, by = "mfr_mdl_code") %>%
  mutate(tail_num = paste0("N", n_number)) %>%
  select(tail_num, model_label, model_mfr, model, model_year = year_mfr)

planes_keep
```

    ## # A tibble: 2,197 x 5
    ##    tail_num model_label model_mfr model   model_year
    ##    <chr>    <chr>       <chr>     <chr>        <int>
    ##  1 N103WT   737         BOEING    737-700         NA
    ##  2 N108MS   737         BOEING    737-7BC       2002
    ##  3 N108WT   737         BOEING    737-700         NA
    ##  4 N11206   737         BOEING    737-824       2000
    ##  5 N114JF   737         BOEING    737-4H6       1993
    ##  6 N12216   737         BOEING    737-824       1998
    ##  7 N12218   737         BOEING    737-824       1998
    ##  8 N12221   737         BOEING    737-824       1998
    ##  9 N12225   737         BOEING    737-824       1998
    ## 10 N12238   737         BOEING    737-824       1999
    ## # … with 2,187 more rows

Interestingly, you can use the `tail_num` to look up flight data from [FlightRadar24](https://flightradar24.com) like this `https://www.flightradar24.com/data/aircraft/<TAIL NUMBER>`. You can see an example for [one 737 MAX-8 here](https://www.flightradar24.com/data/aircraft/n27503) (note that there aren't any flights after March 13, when the ban went into effect).

Unfortunately, the pre-zipped option for the on-time performance data was not working at the time of this writing, so I had to download that data by hand (this amounts to one click per month...not too bad for just 2018, but still annoying). I downloaded these files to "flight\_zips/2018-{month}.zip". The code to clean this up was also inspired by the [nycflights13 R package](https://github.com/hadley/nycflights13) cleaning scripts. Note that this is about 300 MB of compressed data and 2.5 GB of uncompressed data before filtering.

``` r
extract_month <- function(year = 2018, month) {
  zipfile <- sprintf("flight_zips/%s_%s.zip", year, month)

  files <- unzip(zipfile, list = TRUE)
  # Only extract biggest file
  csv <- files$Name[order(files$Length, decreasing = TRUE)[1]]

  unzip(zipfile, exdir = "flights", junkpaths = TRUE, files = csv)

  src <- paste0("flights/", csv)
  dst <- paste0("flights/", year, "-", month, ".csv")
  file.rename(src, dst)
  dst
}

flight_raw_csvs <- tibble(
  year = 2018,
  month = 1:12,
  csv_file = map2_chr(year, month, extract_month)
)

flight_data <- flight_raw_csvs$csv_file %>%
  map(read_csv, col_types = cols(
    .default = col_character(),
    OP_CARRIER_FL_NUM = col_integer(),
    FL_DATE = col_date(),
    DIVERTED = col_double(),
    AIR_TIME = col_double(),
    CRS_ELAPSED_TIME = col_double(),
    ACTUAL_ELAPSED_TIME = col_double()
  )) %>%
  bind_rows() %>%
  rename_all(nice_names)

flights_keep <- flight_data %>%
  mutate(
    fl_num = paste0(op_unique_carrier, op_carrier_fl_num),
    diverted = if_else(diverted == 1, TRUE, FALSE)
  ) %>%
  select(
    fl_date, fl_num,
    tail_num, origin, dest, diverted,
    schedule_elapsed = crs_elapsed_time,
    actual_elapsed = actual_elapsed_time,
    air_time
  ) %>%
  inner_join(planes_keep, by = "tail_num")

flights_keep
```

Now that we have a big ol' data frame of all the flights that took place on a 737 in the US during 2018, we can answer at least one question: were more 737-MAX flights diverted than flights on other 737 aircraft?

``` r
flights_keep %>%
  group_by(model_label) %>%
  summarise(
    n = n(),
    n_diverted = sum(diverted),
    pct_diverted = mean(diverted) * 100
  )
```

    ## # A tibble: 2 x 4
    ##   model_label       n n_diverted pct_diverted
    ##   <chr>         <int>      <int>        <dbl>
    ## 1 737         2378407       5322        0.224
    ## 2 737 MAX       38472         98        0.255

The answer appears to be yes, but not by much: 0.25% of 737 MAX flights were diverted, whereas on other 737 flights, 0.22% of flights were diverted. I'm not much of a statistician, but I believe this is a classic case for a Chi-square test, although I think I remember that Chi-squared tests don't perform well with respect to rare events (flight diversions are definitely rare events). Still, according to this test, even though there were slightly more diverted 737 MAX flights, it is reasonably likely that we would observe this by chance (1 in 5).

``` r
observed_diverted <- 98
(observed_not_diverted <- 38472 - 98)
```

    ## [1] 38374

``` r
(expected_diverted <- 0.2237632 / 100 * 38472)
```

    ## [1] 86.08618

``` r
(expected_not_diverted <- 38472 - expected_diverted)
```

    ## [1] 38385.91

``` r
(chi_sq <- sum(
  (observed_diverted - expected_diverted)^2 / expected_diverted,
  (observed_not_diverted - expected_not_diverted)^2 / expected_not_diverted
))
```

    ## [1] 1.652501

``` r
(p_value <- 1 - pchisq(chi_sq, df = 1))
```

    ## [1] 0.1986189

In the off chance this is an artifact of diverted flights being rare, we could also randomly sample the non-MAX flights to see how likely it is that 98 flights of 38472 would be diverted.

``` r
non_max_flights <- flights_keep %>%
  filter(model_label == "737") %>%
  select(fl_date, fl_num, diverted)

set.seed(4937)
non_max_flights_boot <- tibble(
  sample_number = 1:1000,
  sample = map(sample_number, ~sample_n(non_max_flights, 38472))
) %>%
  unnest() %>%
  group_by(sample_number) %>%
  summarise(n_diverted = sum(diverted))

ggplot(non_max_flights_boot, aes(n_diverted)) +
  geom_histogram(bins = 30) +
  geom_vline(xintercept = 98) +
  theme_bw()
```

![](/wp-content/uploads/2019/03/unnamed-chunk-8-1.png)

This is a confirmation of our earlier calculation: it is fairly reasonable that 98 flights of 38472 could be diverted, assuming the two datasets are comparable.

It would be interesting to see similar data as that used by the [FlightRadar24 blog post regarding Lion Air flight JT610](https://www.flightradar24.com/blog/flightradar24-data-regarding-lion-air-flight-jt610/) (elevation trajectory of the plane during the first 20 minutes of flight) but for all 737 MAX flights. Were there other flights that looked like pilots were fighting with the automatic control system? This data exists at 5-second resolution from FlightRadar24, although even the highest of the paid data plans only lets you download 100 or so of these per day (and there are 2.5 million rows in our `flights_keep` dataset to examine).
[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1475</wp:post_id>
		<wp:post_date><![CDATA[2019-03-17 17:15:49]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-03-17 20:15:49]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[1475]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="public-data-dive"><![CDATA[public data dive]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Visualizing Canadian Climate Normals		</title>
		<link>https://apps.fishandwhistle.net/archives/1490</link>
		<pubDate>Tue, 26 Mar 2019 14:08:40 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">https://apps.fishandwhistle.net/?p=1490</guid>
		<description></description>
		<content:encoded>
				<![CDATA[
I’m an avid Twitter follower of <a href="https://twitter.com/simongerman600">Simon Kuestenmacher</a> (<span class="citation">@simongerman600</span>), who is a prolific tweeter of maps (all sorts). The other day I saw this tweet, which links to a <a href="https://www.reddit.com/r/MapPorn/comments/b4lmg4/the_march_of_the_seasons_normal_average_daily/">reddit thread</a> that used the <a href="http://prism.oregonstate.edu/projects/public/alaska/grids/">PRISM dataset</a> to make an animated map of precipitation in the US. A few weeks ago I had a colleague email me asking for the <a href="http://climate.weather.gc.ca/climate_normals/results_1981_2010_e.html">Canadian climate normals</a> raw data (which can be found <a href="ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/">here</a>), and having made an <a href="https://twitter.com/paleolimbot/status/1031704526036729856">animated map of Earth’s paleogeography</a>, I decided to give it a go for Canada.

<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
Average (1981-2010) daily precipitation for the contiguous USA. Precipitation is "any product of the condensation of atmospheric water vapor that falls under gravity". Essentially drizzle, rain, sleet, snow, graupel and hail. Source: <a href="https://t.co/5UV3nEpj7m">https://t.co/5UV3nEpj7m</a> <a href="https://t.co/eKYPksjxnq">pic.twitter.com/eKYPksjxnq</a>
</p>
— Simon Kuestenmacher (@simongerman600) <a href="https://twitter.com/simongerman600/status/1109924176222187520?ref_src=twsrc%5Etfw">March 24, 2019</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

[markdown]
The first hurdle is getting the data. The data is currently on an FTP server, split into many different folders (one per province) and files (my guess would be to get around Excel's historic limit of 65,000 (ish) rows). Rather than copy-and-paste the links to all 30 files, I thought I would see what the **curl** package did with FTP directory listings. With HTTP listings, you can use **rvest** to scrape the links (`html_nodes("a") %>% html_attr("href")`), but with FTP, **curl** returns something like a whitespace-separated list. I'm sure this is documented somewhere, but I used the following code to parse the directory listing. Note that you can't separate purely on whitespace, because filenames can contain whitespace. Thus, here I used `tidyr::separate()` with `extra = "merge"`, since this limits the number of splits that occur.

``` r
list_ftp <- function(dir_url) {
  dir_url %>%
    curl::curl_fetch_memory() %>%
    .$content %>%
    read_lines() %>%
    tibble(txt = .) %>%
    separate(
      txt, 
      into = c("perms", "user_n", "user", "group", "size", "month", "day", "time", "name"),
      sep = "\\s+",
      extra = "merge"
    ) %>%
    mutate(
      dir_url = dir_url, 
      is_dir = str_detect(perms, "^d"),
      size = if_else(is_dir, NA_real_, as.numeric(size))
    ) %>%
    select(dir_url, name, is_dir, size)
}

list_ftp("ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/")
```

    ## # A tibble: 15 x 4
    ##    dir_url                                     name  is_dir  size
    ##    <chr>                                       <chr> <lgl>  <dbl>
    ##  1 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ .     TRUE      NA
    ##  2 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ ..    TRUE      NA
    ##  3 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ ALTA  TRUE      NA
    ##  4 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ BC    TRUE      NA
    ##  5 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ MAN   TRUE      NA
    ##  6 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ NB    TRUE      NA
    ##  7 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ NFLD  TRUE      NA
    ##  8 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ NS    TRUE      NA
    ##  9 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ NU    TRUE      NA
    ## 10 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ NWT   TRUE      NA
    ## 11 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ ONT   TRUE      NA
    ## 12 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ PEI   TRUE      NA
    ## 13 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ QUE   TRUE      NA
    ## 14 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ SASK  TRUE      NA
    ## 15 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ YT    TRUE      NA

``` r
list_ftp("ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/QUE/")
```

    ## # A tibble: 6 x 4
    ##   dir_url                                  name              is_dir    size
    ##   <chr>                                    <chr>             <lgl>    <dbl>
    ## 1 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGL… .                 TRUE        NA
    ## 2 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGL… ..                TRUE        NA
    ## 3 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGL… QUE_ALBA-KUUJ_EN… FALSE  8344164
    ## 4 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGL… QUE_L'AL-QUEB_EN… FALSE  8493881
    ## 5 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGL… QUE_RAPI-ST P_EN… FALSE  8375096
    ## 6 ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGL… QUE_ST R-WRIG_EN… FALSE  5387169

The second challenge is to pull a list of files recursively (much like `list.files(..., recursive = TRUE)`). There's a few ways to go about this, but my solution is below:

``` r
list_ftp_recursive <- function(dir_url) {
  listing <- list_ftp(dir_url) %>%
    mutate(url = paste0(dir_url, name, if_else(is_dir, "/", ""))) %>%
    filter(!(name %in% c(".", "..")))
  files <- listing %>% filter(!is_dir)
  dirs <- listing %>% filter(is_dir) %>% pull(url)
  
  bind_rows(
    files,
    map_dfr(dirs, list_ftp_recursive)
  )
}

normals_files <- list_ftp_recursive("ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/")
normals_files %>%
  select(name, url)
```

    ## # A tibble: 30 x 2
    ##    name               url                                                  
    ##    <chr>              <chr>                                                
    ##  1 ALTA_ACAD-FIVE_EN… ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ALTA/ALTA…
    ##  2 ALTA_FORE-ROCK_EN… ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ALTA/ALTA…
    ##  3 ALTA_RONA-WORS_EN… ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/ALTA/ALTA…
    ##  4 BC_100-COWI_ENG.c… ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/BC/BC_100…
    ##  5 BC_CRAN-JOE_ENG.c… ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/BC/BC_CRA…
    ##  6 BC_KAML-NORT_ENG.… ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/BC/BC_KAM…
    ##  7 BC_OCHI-SILV_ENG.… ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/BC/BC_OCH…
    ##  8 BC_SIMI-WOOD_ENG.… ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/BC/BC_SIM…
    ##  9 BC_YOHO-YOHO_ENG.… ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/BC/BC_YOH…
    ## 10 MAN_ALEX-OCHR_ENG… ftp://ftp.tor.ec.gc.ca/Pub/Normals/ENGLISH/MAN/MAN_A…
    ## # … with 20 more rows

Now that we have a list of files (and filenames that are unique), we can download them! It's about 191 MB of CSV files, which took about 30 mins on my computer (probably a limitation of Service Canada's computers rather than my internet connection). Note the use of `purrr::walk2()` instead of `purrr::map2()`, since we don't care about the output.

``` r
walk2(normals_files$url, normals_files$name, curl::curl_download)
```

Finally, it would be nice to have some context on which to plot the data, namely the provincial borders. A while ago I made a [GitHub repo of the 1:1,000,000 Atlas of Canada data as a QGIS project](https://github.com/paleolimbot/canadamap_template) (I would link directly to the data but I can't find it anymore). In this case, 1:1M is a bit overkill, so I went looking for a smaller filesize version with less detail. The [Open Government Canada dataset](https://open.canada.ca/data/en/dataset/f77c2027-ed4a-5f6e-9395-067af3e9fc1e) has a broken link, but from fishing around on the FTP site, I was able to [find a copy](http://ftp.geogratis.gc.ca/pub/nrcan_rncan/vector/atlas/75m_prod/geog/shape). The data are in a format I have never before seen, which is *individually zipped shapefile components*! The code below downloads the components and unzips them in a new "aoc\_75M" directory.

``` r
dir.create("aoc_75M")
aoc_url <- "http://ftp.geogratis.gc.ca/pub/nrcan_rncan/vector/atlas/75m_prod/geog/shape"
aoc_layers <- list(
  layer = "pvp", 
  ext = c(".dbf", ".shp", ".shx")
) %>% 
  cross_df() %>%
  mutate(
    url = paste0(aoc_url, "/", layer, ext, ".zip"),
    zip_name = map2_chr(url, file.path("aoc_75M", basename(url)), curl::curl_download)
  )

walk(aoc_layers$zip_name, unzip, exdir = "aoc_75M")
```

Finally, we can read in the data. For this post (and for all good maps of Canada), I use the Atlas of Canada projection (EPSG:3978). I'll start with the spatial data (I filter out `REG_CODE == 0`, which are polygons that are not land, found by trial and error).

``` r
provinces <- read_sf("aoc_75M/pvp.shp") %>%
  filter(REG_CODE != 0) %>%
  st_set_crs(4326) %>%
  st_transform(3978)
```

Next, I'll read in the climate data. It should be said that I started this post with a single file, and then scaled it up to all 30 (I commonly do this...see if it works with a small batch, then scale up).

``` r
climate_normals_raw <- list.files(pattern = ".csv") %>%
  map_dfr(
    read_csv,
    col_types = cols(
      CLIMATE_ID = col_character(),
      STATION_NAME = col_character(),
      PROVINCE = col_character(),
      LATITUDE_DECIMAL_DEGREES = col_double(),
      LONGITUDE_DECIMAL_DEGREES = col_double(),
      ELEVATION = col_double(),
      NORMAL_ID = col_double(),
      NORMAL_ELEMENT_NAME = col_character(),
      MONTH = col_double(),
      VALUE = col_double(),
      MONTHLY_NORMAL_CODE = col_character(),
      ANNUAL_NORMAL_CODE = col_character(),
      STATION_NORMAL_CODE = col_character(),
      FIRST_OCCURRENCE_DATE = col_character(),
      FIRST_YEAR = col_double(),
      LAST_YEAR = col_double()
    )
) %>%
  rename_all(nice_names)

climate_normals_raw
```

    ## # A tibble: 1,359,781 x 16
    ##    climate_id station_name province latitude_decima… longitude_decim…
    ##    <chr>      <chr>        <chr>               <dbl>            <dbl>
    ##  1 3020025    ACADIA VALL… ALTA                 51.2            -110.
    ##  2 3020025    ACADIA VALL… ALTA                 51.2            -110.
    ##  3 3020025    ACADIA VALL… ALTA                 51.2            -110.
    ##  4 3020025    ACADIA VALL… ALTA                 51.2            -110.
    ##  5 3020025    ACADIA VALL… ALTA                 51.2            -110.
    ##  6 3020025    ACADIA VALL… ALTA                 51.2            -110.
    ##  7 3020025    ACADIA VALL… ALTA                 51.2            -110.
    ##  8 3020025    ACADIA VALL… ALTA                 51.2            -110.
    ##  9 3020025    ACADIA VALL… ALTA                 51.2            -110.
    ## 10 3020025    ACADIA VALL… ALTA                 51.2            -110.
    ## # … with 1,359,771 more rows, and 11 more variables: elevation <dbl>,
    ## #   normal_id <dbl>, normal_element_name <chr>, month <dbl>, value <dbl>,
    ## #   monthly_normal_code <chr>, annual_normal_code <chr>,
    ## #   station_normal_code <chr>, first_occurrence_date <chr>,
    ## #   first_year <dbl>, last_year <dbl>

We can see from the file that it is in parameter-long form (one row per measurement), and contains repeated location and parameter metadata (for a discussion of this topic, you should check out the [mudata2 package](https://cran.r-project.org/package=mudata2) and the [paper I wrote describing it](https://doi.org/10.1139/facets-2017-0026)). I'll pull out the location data first using `dplyr::distinct()` (renaming the lat and lon variables to something more type-able).

``` r
climate_stations <- climate_normals_raw %>%
  rename(latitude = latitude_decimal_degrees, longitude = longitude_decimal_degrees) %>%
  distinct(climate_id, station_name, province, longitude, latitude, elevation) %>%
  mutate(elevation = elevation / 1000) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(3978)
```

Let's have a look! Do they all plot within Canada? (You would be surprised how many stations in the full Environment Canada climate dataset have erroneous lat/lons that make them plot outside Canada). All of these look to be in the right place.

``` r
ggplot() + 
  layer_spatial(provinces, size = 0.1) +
  layer_spatial(climate_stations) +
  annotation_scale()
```

![](/wp-content/uploads/2019/03/stations-1.png)

Next I'll pull out the parameter metadata, contained in the `normal_id` and `normal_element_name` column. I'll also give them a column name, since in the next chunk I'm going to make one column for each. There are upwards of 50 variables to choose from, although not every variable is available for every station.

``` r
climate_params <- climate_normals_raw %>%
  distinct(normal_id, normal_element_name) %>%
  filter(normal_element_name %in% c("Mean daily temperature deg C", "Total precipitation mm")) %>%
  mutate(col_name = fct_recode(
    normal_element_name,
    "mean_temp" = "Mean daily temperature deg C",
    "total_precip" = "Total precipitation mm"
  ))

climate_params
```

    ## # A tibble: 2 x 3
    ##   normal_id normal_element_name          col_name    
    ##       <dbl> <chr>                        <fct>       
    ## 1         1 Mean daily temperature deg C mean_temp   
    ## 2        56 Total precipitation mm       total_precip

Finally, I'll recombine the data using `inner_join()`s. I do this because I've filtered out some `climate_params` that I didn't want to use, and I could have filtered out some locations (I mean, I didn't, but I *could* have, and would want that reflected in my final dataset). I have to use `st_as_sf()` at the end, because doing a join with an **sf** object keeps the geometry column, but strips the table class (and it's helpful for R to know it's a spatial object later on).

``` r
climate_normals <- climate_normals_raw %>%
  filter(month %in% 1:12) %>%
  select(climate_id, normal_id, month, value) %>%
  inner_join(climate_params, by = "normal_id") %>%
  select(-normal_element_name, -normal_id) %>%
  spread(col_name, value) %>%
  inner_join(climate_stations, by = "climate_id") %>%
  st_as_sf()

climate_normals
```

    ## Simple feature collection with 21240 features and 7 fields
    ## geometry type:  POINT
    ## dimension:      XY
    ## bbox:           xmin: -2325537 ymin: -691201.7 xmax: 3003840 ymax: 3810547
    ## epsg (SRID):    3978
    ## proj4string:    +proj=lcc +lat_1=49 +lat_2=77 +lat_0=49 +lon_0=-95 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs
    ## # A tibble: 21,240 x 8
    ##    climate_id month mean_temp total_precip station_name province elevation
    ##    <chr>      <dbl>     <dbl>        <dbl> <chr>        <chr>        <dbl>
    ##  1 1011467        1        NA        139   CENTRAL SAA… BC            53.3
    ##  2 1011467        2        NA         97.5 CENTRAL SAA… BC            53.3
    ##  3 1011467        3        NA         71.5 CENTRAL SAA… BC            53.3
    ##  4 1011467        4        NA         61.0 CENTRAL SAA… BC            53.3
    ##  5 1011467        5        NA         44.5 CENTRAL SAA… BC            53.3
    ##  6 1011467        6        NA         33.6 CENTRAL SAA… BC            53.3
    ##  7 1011467        7        NA         21.8 CENTRAL SAA… BC            53.3
    ##  8 1011467        8        NA         24.0 CENTRAL SAA… BC            53.3
    ##  9 1011467        9        NA         24.0 CENTRAL SAA… BC            53.3
    ## 10 1011467       10        NA         73.0 CENTRAL SAA… BC            53.3
    ## # … with 21,230 more rows, and 1 more variable: geometry <POINT [m]>

Given this, we can plot total monthly precipitation for (say) June:

``` r
climate_normals_june <- climate_normals %>% 
  filter(month == 6)

ggplot() + 
  layer_spatial(provinces, size = 0.1) +
  layer_spatial(climate_normals_june, aes(col = total_precip)) +
  labs(col = "June Precip (mm)") +
  scale_color_viridis_c() +
  annotation_scale()
```

![](/wp-content/uploads/2019/03/stations-june-precip-1.png)

A nice feature of the U.S. map that inspired this post was that you could see areas of high and low precipitation move in space, which requires interpolating between stations. Canada is massive, and interpolating gets hard to justify when the nearest station is 500 km away (like in eastern Northwest Territories). Justifying spatial 2D smoothing is beyond my area of expertise, but there's a great [tutorial on it here](https://rspatial.org/analysis/4-interpolation.html). I opted for a thin-plate regression spline smooth from the **fields** package, which I would say is good enough for a blog post but maybe you should contact an expert before using it in a paper. The **raster** package provides a nice helper to get these smooth objects into a plottable `Raster` object (note that I'm using the `provinces` object to create a `Raster` template from its bounding box, and again to make sure all the values outside Canada are assigned a no-data value). I picked 200x200 because it renders in a reasonable amount of time (I started with 10x10 when I was still getting the model code right).

``` r
june_precip_model <- fields::Tps(
  st_coordinates(climate_normals_june$geometry), 
  climate_normals_june$total_precip
)
june_precip_raster <- raster::raster(provinces, nrows = 200, ncol = 200) %>%
  raster::interpolate(june_precip_model) %>%
  raster::mask(provinces)
```

Now we can see how it looks! I'm using `cut_number()` here on the values because there are some very high precipitation values (I'm looking at you, west coast of British Columbia) that obscure variation in the drier parts. In QGIS you can do this using "quantile breaks". I picked 12 values because there's still enough difference between the colours that you can see the line move, which for me was the coolest part of the U.S. map that inspired this post.

``` r
ggplot() + 
  layer_spatial(june_precip_raster, aes(fill = cut_number(stat(band1), 12)), hjust = 1, vjust = 1) +
  layer_spatial(provinces, size = 0.1, fill = NA, col = "white") +
  labs(fill = "June Precip (mm)") +
  scale_fill_viridis_d(na.value = NA, direction = -1) +
  annotation_scale()
```

![](/wp-content/uploads/2019/03/raster-june-precip-1.png)

Now we need to scale this up to all 12 months. It's always a bit awkward to do this, but I'm going to do a the `group_by()`, `nest()`, then `mutate()` approach, utilizing the fact that we can refer to previous things we created in `mutate()`. This takes about 2 minutes on my computer (I tested it out by sticking a `slice(1)` right after `group_by(month)`, which makes sure that everything works at least once).

``` r
precip_raster <- climate_normals %>%
  as_tibble() %>%
  group_by(month) %>%
  nest() %>% 
  mutate(
    precip_model = map(data, ~fields::Tps(st_coordinates(.x$geometry), .x$total_precip)),
    precip_raster = map(
      precip_model,
      ~raster::raster(provinces, nrows = 200, ncols = 200) %>%
        raster::interpolate(.x) %>%
        raster::mask(provinces)
    ),
    precip_raster_df = map(precip_raster, df_spatial)
  ) %>%
  unnest(precip_raster_df) %>%
  rename(total_precip = band1) %>%
  filter(!is.na(total_precip))
```

We're getting closer! The problem is, we only have one data point per month, and the U.S. map had one per day. This form of interpolation I'm more familiar with (time series), and the reddit post gives some clues as to how they did it (using a 3rd-order cubic spline polynomial). Let's see if we can make that happen using a some stations I'm familiar with. I'm pretty sure that one can fit a cubic spline using **mgcv** (they are in the family of GAM), although I'm not up on the details. [Gavin Simpson](https://www.fromthebottomoftheheap.net/) has a [great paper](https://doi.org/10.3389/fevo.2018.00149) on GAMs for paleolimnology, and even if you can't pronounce paleolimnology, the paper is the most readable explanation of GAMs that I've come across to date. In **ggplot2**, we don't even have to leave `stat_smooth()` to get a sense of what `mgcv::gam()` will give us. I opted for using 12 knots (the places where each cubic joins the next) here, or one per month, at four cells in the raster we just created.

``` r
climate_normals_selected <- precip_raster %>%
  filter(feature_id %in% c("104", "5482", "9659", "39526")) %>%
  mutate(daily_precip = total_precip / lubridate::days_in_month(month))

ggplot(climate_normals_selected, aes(x = month, y = daily_precip)) +
  geom_point() +
  stat_smooth(method = "gam", formula = y ~ s(x, k = 12)) +
  scale_x_continuous(breaks = 1:12) +
  facet_wrap(vars(feature_id))
```

![](/wp-content/uploads/2019/03/unnamed-chunk-10-1.png)

It looks here like some smooths are better than others due to the fact that the climate in some places is pretty volatile! I'm not going to get into the various options for smoothing here, mostly because I don't know them. However, the mechanics of scaling up our smooth to all 15,000 (give or take) cells within Canada is similar for any smoother. I'm using `k = 14` here because we've added two points (one to the beginning, one to the end), and since we're allowed any `k` up to the number of data points, I decided to max it out to help the smoother take into account the volatility of some of the cells. There's also some code in here about the Julian Day, which is a number from 1 to 365 that more accurately describes the "middle of the month", and allows us to pull out a day (January 29th, for example) from the interpolation more readily.

``` r
month_info <- tibble(
  month = 1:12,
  days_in_month = lubridate::days_in_month(month),
  julian_day_start = lag(cumsum(lubridate::days_in_month(month)), default = 0) + 1,
  julian_day = julian_day_start + days_in_month / 2
)

precip_daily_raster <- rbind(
  precip_raster %>% filter(month == 12) %>% mutate(julian_day = 350.5 - 365),
  precip_raster %>% left_join(month_info %>% select(month, julian_day), by = "month"),
  precip_raster %>% filter(month == 1) %>% mutate(julian_day = 365 + 16.5)
) %>%
  mutate(daily_precip = total_precip / lubridate::days_in_month(month)) %>%
  group_by(x, y) %>%
  nest() %>%
  mutate(
    fit = map(data, ~mgcv::gam(daily_precip ~ s(julian_day, k = 14), data = .x)),
    new_data = list(tibble(julian_day = 1:365)),
    daily_precip = map2(fit, new_data, predict)
  ) %>%
  unnest(new_data, daily_precip)
```

Finally, we're ready for animation. I've made a function to label the Julian Day with a calendar date, and used `transition_time()` to continuously loop through the days (one frame per day). This type of animation takes a long time to render (although improvements are coming in ggplot2 and gganimate soon!). I left the final version to render overnight (my guess is that it took an hour or two), but to get the plot right I started off with the data filtered to `julian_day == 1`, then `julian_day %in% c(1, 100, 200)` with the animation code, then finally the whole dataset. Early versions also used a smaller raster (10 x 10, then 100 x 100, then the final 200 x 200), which also improves rendering time while tweaking the code.

``` r
label_time <- function(julian_day) {
  date <- as.Date("2019-01-01") + julian_day - 1
  strftime(date, "%B %d")
}

raster_anim <- ggplot(precip_daily_raster) +
  geom_raster(aes(x, y, fill = cut_number(daily_precip, 9), group = paste(x, y)), hjust = 1, vjust = 1) +
  layer_spatial(provinces, size = 0.1, fill = NA, col = "white") +
  labs(
    fill = "Daily Precip (mm)",
    title = "{label_time(frame_time)}", 
    x = NULL, y = NULL,
    caption = "Source: Canadian Climate Normals, 1981-2010 (ECCC)"
  ) +
  scale_fill_viridis_d(direction = -1) +
  annotation_scale() +
  transition_time(julian_day)

animate(raster_anim, nframes = 365, width = 500, height = 400, res = 96)
```

![](/wp-content/uploads/2019/03/precip-animate-1.gif)

I'm not good enough at smoothing to know if there are any obvious artifacts of smoothing in this diagram, but I do know that it accurately reflects my experience living in Alberta for a few years (there really is a wet season and a dry season). Whereas living in Nova Scotia, things are quite rainy, much of the time (but definitely not as rainy as British Columbia, even though that doesn't come through in the animation).

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1490</wp:post_id>
		<wp:post_date><![CDATA[2019-03-26 11:08:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-03-26 14:08:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[visualizing-canadian-climate-normals]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="climate"><![CDATA[climate]]></category>
		<category domain="post_tag" nicename="gganimate"><![CDATA[gganimate]]></category>
		<category domain="post_tag" nicename="ggplot"><![CDATA[ggplot]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Summarising SQL Translation for multiple dbplyr backends		</title>
		<link>https://apps.fishandwhistle.net/archives/1503</link>
		<pubDate>Tue, 09 Apr 2019 22:09:19 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">https://apps.fishandwhistle.net/?p=1503</guid>
		<description></description>
		<content:encoded>
				<![CDATA[Inspired by <a href="http://twitter.com/gshotwell">@gshotwell</a>, I decided to have a look into bulk translating a ton of functions to SQL. <a href="https://db.rstudio.com/dplyr/">The dplyr system to translate R code to SQL</a> is really cool, but I’ve had some trouble in the past using it to write backend-agnostic code because of slightly different implementations of functions in different database backends.

<blockquote class="twitter-tweet" data-lang="en">

<p lang="en" dir="ltr">

Is there a reference document somewhere of which dplyr commands work on
various database backends?
<a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">\#rstats</a>

</p>

— Gordon Shotwell (@gshotwell)
<a href="https://twitter.com/gshotwell/status/1115653121269796865?ref_src=twsrc%5Etfw">April
9,
2019</a>

</blockquote>

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

I should also mention that Bob Rudis posted a solution to this as well that includes more backends (this post only considers the ones directly in <b>dbplyr</b>).

<blockquote class="twitter-tweet" data-lang="en">

<p lang="en" dir="ltr">

So still not as hard as I thought but did require some wrangling
<a href="https://t.co/YMSG2XdXR7">pic.twitter.com/YMSG2XdXR7</a>

</p>

— boB 🇷udis (@hrbrmstr)
<a href="https://twitter.com/hrbrmstr/status/1115685618426818561?ref_src=twsrc%5Etfw">April
9,
2019</a>

</blockquote>

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

[markdown]

As per my usual analysis, I’ll be using the **tidyverse**, with the addition of **dbplyr** since that’s what this post is about.

``` r
library(dbplyr)
library(tidyverse)
```

The first step for me was to figure out which translations are available. The `sql_translate_env()` function provides a custom object that contains the translations, with the `names()` attribute containing a translated function listing. That’s great, because to automatically make a list of translated function calls, we’ll need their names.

``` r
sql_translate_env(simulate_dbi())
```

    ## <sql_variant>
    ## scalar:    -, :, !, !=, (, [, [[, {, *, /, &, &&, %%, %>%, %in%,
    ## scalar:    ^, +, <, <=, ==, >, >=, |, ||, $, abs, acos, as_date,
    ## scalar:    as_datetime, as.character, as.Date, as.double,
    ## scalar:    as.integer, as.integer64, as.logical, as.numeric,
    ## scalar:    as.POSIXct, asin, atan, atan2, between, bitwAnd,
    ## scalar:    bitwNot, bitwOr, bitwShiftL, bitwShiftR, bitwXor, c,
    ## scalar:    case_when, ceil, ceiling, coalesce, cos, cosh, cot,
    ## scalar:    coth, day, desc, exp, floor, hour, if, if_else, ifelse,
    ## scalar:    is.na, is.null, log, log10, mday, minute, month, na_if,
    ## scalar:    nchar, now, paste, paste0, pmax, pmin, qday, round,
    ## scalar:    second, sign, sin, sinh, sql, sqrt, str_c, str_conv,
    ## scalar:    str_count, str_detect, str_dup, str_extract,
    ## scalar:    str_extract_all, str_flatten, str_glue, str_glue_data,
    ## scalar:    str_interp, str_length, str_locate, str_locate_all,
    ## scalar:    str_match, str_match_all, str_order, str_pad,
    ## scalar:    str_remove, str_remove_all, str_replace,
    ## scalar:    str_replace_all, str_replace_na, str_sort, str_split,
    ## scalar:    str_split_fixed, str_squish, str_sub, str_subset,
    ## scalar:    str_to_lower, str_to_title, str_to_upper, str_trim,
    ## scalar:    str_trunc, str_view, str_view_all, str_which, str_wrap,
    ## scalar:    substr, switch, tan, tanh, today, tolower, toupper,
    ## scalar:    trimws, wday, xor, yday, year
    ## aggregate: cume_dist, cummax, cummean, cummin, cumsum, dense_rank,
    ## aggregate: first, lag, last, lead, max, mean, median, min,
    ## aggregate: min_rank, n, n_distinct, nth, ntile, order_by,
    ## aggregate: percent_rank, quantile, rank, row_number, sum, var
    ## window:    cume_dist, cummax, cummean, cummin, cumsum, dense_rank,
    ## window:    first, lag, last, lead, max, mean, median, min,
    ## window:    min_rank, n, n_distinct, nth, ntile, order_by,
    ## window:    percent_rank, quantile, rank, row_number, sum, var

``` r
names(sql_translate_env(simulate_dbi())) %>% head()
```

    ## [1] "-"  ":"  "!"  "!=" "("  "["

To generate the translated SQL, I used the `translate_sql()` function. This can lead to a few endpoints, which include valid SQL, or various error messages.

``` r
translate_sql(abs(arg1))
```

    ## <SQL> ABS(`arg1`)

``` r
translate_sql(arg1 %in% arg2)
```

    ## <SQL> `arg1` IN `arg2`

``` r
translate_sql(str_match(arg1))
```

    ## Error: str_match() is not available in this SQL variant

``` r
translate_sql(abs(arg1, arg2))
```

    ## Error: Invalid number of args to SQL ABS. Expecting 1

To make this automated, we’ll need a way to test individual functions with arguments. The `translate_sql_()` function is designed for pre-quoted calls, which we can generate using the `call()` function.

``` r
translate_sql_(
  list(call("%in%", quote(arg1), quote(arg2))), 
  con = simulate_dbi()
)
```

    ## <SQL> `arg1` IN `arg2`

Finally, we need a function to (1) generate a test call with an arbitrary number of arguments, and (2) a function to turn that call into SQL. There is probably a more elegant way to do this than calling `do.call` on the `call` function, but I’m not sure what it is since these functions don’t to tidy evaluation (it’s possible that `translate_sql_()` handles tidy evaluation). The `arg1`, `arg2` ... pattern is a bit crude, but I couldn’t find a way to get the signatures of the functions for each SQL variant.

``` r
test_call <- function(fun_name, n_args = 1) {
  args <- map(seq_len(n_args), ~sym(paste0("arg", .x))) %>%
    map(enquote)
  do.call(call, c(list(fun_name), args))
}

test_translate <- function(call, con = simulate_dbi()) {
  translate_sql_(
    list(call),
    con = con
  )
}

test_translate(test_call("abs", 1))
```

    ## <SQL> ABS(`arg1`)

``` r
test_translate(test_call("%in%", 2))
```

    ## <SQL> `arg1` IN `arg2`

``` r
test_translate(test_call("avg", 3))
```

    ## <SQL> avg(`arg1`, `arg2`, `arg3`)

The whole point of this post is to look at the different SQL variants, and to do that we need connection objects to each of them. For testing, **dbplyr** provides the `simulate_*()` family of functions to generate fake connection objects. This is also a bit of clumsy code, but it does provide us with a tibble of connection objects and variant names.

``` r
sql_variants <- tibble(
  variant = getNamespace("dbplyr") %>%
    names() %>%
    str_subset("^simulate_") %>%
    str_remove("^simulate_"),
  test_connection = map(
    variant,
    ~getNamespace("dbplyr")[[paste0("simulate_", .x)]]()
  ),
  fun_name = map(test_connection, ~unique(names(sql_translate_env(.x)))),
)

sql_variants
```

    ## # A tibble: 11 x 3
    ##    variant  test_connection            fun_name   
    ##    <chr>    <list>                     <list>     
    ##  1 hive     <S3: Hive>                 <chr [164]>
    ##  2 mysql    <S3: MySQLConnection>      <chr [163]>
    ##  3 access   <S3: ACCESS>               <chr [167]>
    ##  4 sqlite   <S3: SQLiteConnection>     <chr [166]>
    ##  5 postgres <S3: PostgreSQLConnection> <chr [168]>
    ##  6 odbc     <S3: OdbcConnection>       <chr [164]>
    ##  7 dbi      <S3: TestConnection>       <chr [162]>
    ##  8 teradata <S3: Teradata>             <chr [166]>
    ##  9 impala   <S3: Impala>               <chr [164]>
    ## 10 oracle   <S3: Oracle>               <chr [164]>
    ## 11 mssql    <S3: Microsoft SQL Server> <chr [166]>

Now we need to make a very long list of function calls and (try to) translate them to SQL. As we saw above, we’ll need to be able to handle errors, warnings, and messages, in addition to capturing the result. I did this using the `safely()` and `quietly()` adverbs in the **purrr** package. I also use the `crossing()` function from the **tidyr**
package, which is kind of like `expand.grid()` but with data frames, in that it generates a new data frame with lots of combinations. In this case, I chose to evaluate each function with 0, 1, 2, 3, and 50 arguments, for every SQL variant, for every function. This works out to about 9,000 function calls and takes about a minute to complete.

``` r
translations <- crossing(
  tibble(
    n_args = c(0:3, 50)
  ),
  sql_variants
) %>%
  unnest(fun_name, .drop = FALSE) %>%
  mutate(
    call = map2(fun_name, n_args, test_call),
    translation = map2(
      call, test_connection, 
      quietly(safely(test_translate))
    ),
    r = map_chr(call, ~paste(format(.x), collapse = "")),
    sql = map_chr(translation, ~first(as.character(.x$result$result))),
    messages = map_chr(translation, ~paste(.x$messages, collapse = "; ") %>% na_if("")),
    warnings = map_chr(translation, ~paste(.x$warnings, collapse = "; ") %>% na_if("")),
    errors = map_chr(translation, ~first(as.character(.x$result$error)))
  )

translations %>%
  filter(!is.na(sql), n_args == 1) %>%
  select(variant, n_args, r, sql)
```

    ## # A tibble: 900 x 4
    ##    variant n_args r                  sql                      
    ##    <chr>    <dbl> <chr>              <chr>                    
    ##  1 hive         1 -arg1              -`arg1`                  
    ##  2 hive         1 !arg1              NOT(`arg1`)              
    ##  3 hive         1 (arg1)             (`arg1`)                 
    ##  4 hive         1 {    arg1}         (`arg1`)                 
    ##  5 hive         1 abs(arg1)          ABS(`arg1`)              
    ##  6 hive         1 acos(arg1)         ACOS(`arg1`)             
    ##  7 hive         1 as_date(arg1)      CAST(`arg1` AS DATE)     
    ##  8 hive         1 as_datetime(arg1)  CAST(`arg1` AS TIMESTAMP)
    ##  9 hive         1 as.character(arg1) CAST(`arg1` AS STRING)   
    ## 10 hive         1 as.Date(arg1)      CAST(`arg1` AS DATE)     
    ## # … with 890 more rows

Of course, this doesn’t take into account window or aggregation functions in their entirity, but it does a reasonable job at summarising how various functions are translated by `translate_sql()`. It isn’t a perfect summary, but below is my best attempt at capturing this in one graphic. In the future this could turn into a useful summary of how
things are translated and how consistent the results are, but for that it needs a bit more rigour. Enjoy\!

![](/wp-content/uploads/2019/04/unnamed-chunk-8-1.png)<!-- -->

[/markdown]
]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1503</wp:post_id>
		<wp:post_date><![CDATA[2019-04-09 19:09:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-04-09 22:09:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[summarising-sql-translation-for-multiple-dbplyr-backends]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="dbplyr"><![CDATA[dbplyr]]></category>
		<category domain="post_tag" nicename="r"><![CDATA[R]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>
				Doing Bayesian Lead-210 interpretation		</title>
		<link>https://apps.fishandwhistle.net/archives/1510</link>
		<pubDate>Wed, 01 May 2019 13:05:28 +0000</pubDate>
		<dc:creator><![CDATA[dewey]]></dc:creator>
		<guid isPermaLink="false">https://apps.fishandwhistle.net/?p=1510</guid>
		<description></description>
		<content:encoded>
				<![CDATA[Paleolimnologists have been using the <a href="http://doi.org/10.1007/BF00028424">Constant Rate of Supply (CRS) and Constant Initial Concentration (CIC)</a> model to interpret <sup>210</sup>Pb dates for a very long time. Some variations like propagating error using a Monte Carlo simulation (<a href="https://doi.org/10.1007/BF00219461">Binford 1990</a> and <a href="https://doi.org/10.1016/j.quageo.2014.06.002">Sanchez-Cabeza et al. 2014</a>) instead of the usual “quadrature” method have been used, but for the most part, we have been interpreting lead-210 dates the same way since 1978 (40 years!).

[markdown]
We can definitely do better. Similar to how [Bacon](https://cran.r-project.org/package=rbacon) revolutionized the <sup>14</sup>C dating world, Bayesian methods are the future of <sup>210</sup>Pb dating as well. Bayesian methods are a bit hard to understand, but essentially, they involve creating millions and millions of age-depth relationships, only selecting the ones that are probable given some constraints. In this case, the constraints are that we have <sup>210</sup>Pb activities that were measured (with some error), <sup>210</sup>Pb decays at a known rate (if we're being technical there's error here too, but it's always ignored), and the sedimentation rate of a given slice is related to the sedimentation rate of the previous slice. You can imagine that it would be possible to "check" any age-depth model to see if it actually would produce the <sup>210</sup>Pb activities that you measured. Bayesian age-depth model creation is that. But millions and millions of times, keeping only the age-depth models that make sense.

In 2018, the first (to my knowledge) [Bayesian <sup>210</sup>Pb interpretation paper](https://doi.org/10.1007/s13253-018-0328-7) was published, and included a [program to do the interpretation](https://github.com/maquinolopez/Plum). Having just wrote an [R package implementing the CIC and CRS models](http://github.com/paleolimbot/pb210), I thought I'd give it a go! It turns out that the program as written is difficult to use (the authors acknowledge that this version is just a draft, and the paper indicates that it may be translated to C++ in the future). Still, it's a massive step forward for those of us that rely on <sup>210</sup>Pb dating (which is almost all of us paleolimnologists).

### Some data

I'll use the Alta Lake <sup>210</sup>Pb measurements I have from my [first published paper](https://doi.org/0.1007/s10933-016-9919-x), which also happens to be the sample dataset for the [pb210 package](http://github.com/paleolimbot/pb210).

``` r
library(pb210)
alta_lake_pb210
```

    ## # A tibble: 32 x 13
    ##    sample_id depth_cm total_pb210_Bq_… total_pb210_sd section_start_cm
    ##    <chr>        <dbl>            <dbl>          <dbl>            <dbl>
    ##  1 AL-GC2-0      0.25            338.            8.71              0  
    ##  2 AL-GC2-0…     0.75             NA            NA                 0.5
    ##  3 AL-GC2-1      1.25            415.           11.8               1  
    ##  4 AL-GC2-1…     1.75            411.           11.0               1.5
    ##  5 AL-GC2-2      2.5             378.           10.5               2  
    ##  6 AL-GC2-3      3.5             237.            7.79              3  
    ##  7 AL-GC2-4      4.5             146.            5.25              4  
    ##  8 AL-GC2-5      5.5             117.            4.61              5  
    ##  9 AL-GC2-6      6.5              67.5           3.86              6  
    ## 10 AL-GC2-7      7.5              54.6           3.44              7  
    ## # … with 22 more rows, and 8 more variables: section_end_cm <dbl>,
    ## #   water_percent <dbl>, slice_mass_g <dbl>, dry_density_cm3 <dbl>,
    ## #   c_percent <dbl>, n_percent <dbl>, published_age_yr <dbl>,
    ## #   published_age_sd <dbl>

``` r
ggplot(alta_lake_pb210, aes(x = depth_cm, y = total_pb210_Bq_kg)) +
  geom_point() +
  geom_errorbar(
    aes(
      ymin = total_pb210_Bq_kg - total_pb210_sd, 
      ymax = total_pb210_Bq_kg + total_pb210_sd
    )
  )
```

![](/wp-content/uploads/2019/05/fig-pb210-1.png)

### Using Plum

Plum (in its current form) is an R package that calls Python code to do the heavy lifting for the Bayesian statistics. Because it uses the **rPython** package, it can only be used on Unix-alike systems (Mac OS included!). On my MacOS machine, the following worked to install the R package.

```r
install.packages("rPython", configure.vars= "RPYTHON_PYTHON_VERSION=2")
remotes::install_github("maquinolopez/Plum")
```

If you can load the package, it's installed!

``` r
library(Plum)
```

    ## Loading required package: rPython

    ## Loading required package: RJSONIO

The **Plum** package requires its input as a CSV file (much like **rbacon**). I don't particularly like this kind of interface, but I do very much appreciate the ability to do Bayesian lead-210 analysis, and with the **tidyverse** functions at one's disposal, it's not too bad. The CSV has to have the columns `Depth (cm)`, `Density g/cm^3`, `210Pb (Bq/kg)`, `sd(210Pb)`, `Thickness (cm)`, `226Ra (Bq/kg)`, and `sd(226Ra)` (in that order!). The `226Ra (Bq/kg)`, and `sd(226Ra)` columns are options, according to the author's tutorial. In my case, I don't have any <sup>226</sup>Ra data, so I'll omit those columns. Note that everything has to be finite (no `NA` values!).

``` r
alta_lake_plum <- alta_lake_pb210 %>%
  filter(!is.na(total_pb210_Bq_kg)) %>%
  transmute(
    `Depth (cm)` = depth_cm, 
    `Density g/cm^3` = dry_density_cm3, 
    `210Pb (Bq/kg)` = total_pb210_Bq_kg, 
    `sd(210Pb)` = total_pb210_sd, 
    `Thickness (cm)` = section_end_cm - section_start_cm
  )

alta_lake_plum
```

    ## # A tibble: 18 x 5
    ##    `Depth (cm)` `Density g/cm^3` `210Pb (Bq/kg)` `sd(210Pb)`
    ##           <dbl>            <dbl>           <dbl>       <dbl>
    ##  1         0.25            0.169           338.         8.71
    ##  2         1.25            0.153           415.        11.8 
    ##  3         1.75            0.150           411.        11.0 
    ##  4         2.5             0.183           378.        10.5 
    ##  5         3.5             0.188           237.         7.79
    ##  6         4.5             0.229           146.         5.25
    ##  7         5.5             0.216           117.         4.61
    ##  8         6.5             0.269            67.5        3.86
    ##  9         7.5             0.191            54.6        3.44
    ## 10         8.5             0.197            39.2        2.95
    ## 11         9.5             0.276            39.7        3.13
    ## 12        10.5             0.280            42.1        3.22
    ## 13        11.5             0.183            37.7        2.90
    ## 14        12.5             0.210            31.5        2.93
    ## 15        13.5             0.254            29.2        2.03
    ## 16        14.5             0.276            23.7        2.00
    ## 17        15.5             0.343            23.9        2.11
    ## 18        16.5             0.494            25.0        1.98
    ## # … with 1 more variable: `Thickness (cm)` <dbl>


The CSV file needs to be in a very specific location in order for **Plum** to be able to find it. Apparently it needs to be given a core name, placed in a directory with that name, and be written to the file `<core_name>.csv` (within that directory). The documentation says this can be any folder, but I can't make it work in anything other than the default (`~/Plum`).

```r
dir.create("~/Plum/ALGC2", recursive = TRUE)
write_csv(alta_lake_plum, "cores/ALGC2/ALGC2.csv")
```

To run Plumb, use the `runPlumb()` function. Note that this takes about 5 minutes on my machine, and may take longer on others (if you have <sup>226</sup>Ra data and require that there is a different supported <sup>210</sup>Pb concentration for each depth, apparently it will take longer). You have to tell it how many samples (counting from the bottom) are "background" (if there's no <sup>226</sup>Ra data). There's some other options but it's difficult to tell exactly what they do.

```r
runPlum(Core.name = "ALGC2", number_supported = 9)
```

This function gave me a ton of errors, but I think they are all at the end when it's trying to plot everything. I don't want it to plot anything (I want the data output), so that's fine with me. At least, the functions to extract the age distribution of each sample (the whole point!) seem to work. From some trial and error, it looks like you can get the age distribution at any age down to 8.25 cm. I'm not sure what defines this depth, but it's helpful to know you will get a weird error if you request a depth that's too deep.

``` r
first_age <- ageof(x = 0.1, folder = "~/Plum/ALGC2")
```

![](/wp-content/uploads/2019/05/figs-age-dist-1.png)<!-- -->

    ## [1] "the age of depth 0.1 is between"
    ## [1] 0.8861204 1.0776776
    ## [1] "with a 0.95% condifence interval and a mean of:"
    ## [1] 0.9904968

``` r
last_age <- ageof(x = 8.25, folder = "~/Plum/ALGC2")
```

![](/wp-content/uploads/2019/05/figs-age-dist-2.png)<!-- -->

    ## [1] "the age of depth 8.25 is between"
    ## [1]  71.80778 204.28006
    ## [1] "with a 0.95% condifence interval and a mean of:"
    ## [1] 126.7365

Unfortunately, it insists on plotting the distribution for you. I really just want the data, which in this case is a vector of ages (1999 of them) according to a bunch (1999) of probable age-depth models given the data we collected. It's actually ingenious and is totally the way we should be interpreting age-depth models (but I could do without the plotting when I just want numbers).

Because `ageof()` fails given certain depths, extracting the distribution of ages for each sample is a little awkward. Here I use a little custom function that returns a `tibble()` instead of the list that is returned by `ageof()`.

```r
plum_ages <- function(depth) {
  plum_obj <- try(ageof(x = depth, folder = "~/Plum/ALGC2"), silent = TRUE)
  if(inherits(plum_obj, "try-error")) {
    tibble(
      mean_age = NA_real_, 
      lower_age = NA_real_, 
      upper_age = NA_real_, 
      data = list(numeric(0))
    )
  } else {
    tibble(
      mean_age = plum_obj$Mean, 
      lower_age = plum_obj$Lower_lim, 
      upper_age = plum_obj$Upper_lim, 
      data = list(plum_obj$Data)
    )
  }
}

alta_lake_ages <- alta_lake_pb210 %>%
  mutate(plum_raw = map(depth_cm, plum_ages)) %>%
  unnest()
```

``` r
alta_lake_ages %>%
  select(depth_cm, mean_age, lower_age, upper_age)
```

    ## # A tibble: 32 x 4
    ##    depth_cm mean_age lower_age upper_age
    ##       <dbl>    <dbl>     <dbl>     <dbl>
    ##  1     0.25     2.48      2.22      2.69
    ##  2     0.75     7.43      6.65      8.08
    ##  3     1.25    16.1      14.3      17.2 
    ##  4     1.75    24.9      21.9      26.8 
    ##  5     2.5     35.7      30.5      38.7 
    ##  6     3.5     49.5      40.6      54.8 
    ##  7     4.5     64.6      50.2      73.5 
    ##  8     5.5     82.7      59.1      98.6 
    ##  9     6.5    100.       65.3     131.  
    ## 10     7.5    116.       69.2     177.  
    ## # … with 22 more rows

Cool! Now we've got the data in an object we can plot. I'll put the ages that [we published](https://doi.org/0.1007/s10933-016-9919-x) on plot as well, for comparison.

``` r
alta_lake_ages %>%
  filter(depth_cm < 10) %>%
  ggplot(aes(x = depth_cm)) +
  geom_point(aes(y = mean_age, col = "Plum")) +
  geom_errorbar(
    aes(ymin = lower_age, ymax = upper_age, col = "Plum"), 
    width = 0.2
  ) +
  geom_point(aes(y = published_age_yr, col = "CRS")) +
  geom_errorbar(
    aes(
      ymin = published_age_yr - published_age_sd, 
      ymax = published_age_yr + published_age_sd,
      col = "CRS"
    ),
    width = 0.2
  )
```

![](/wp-content/uploads/2019/05/fig-age-compare-1.png)<!-- -->

It's an interesting difference. It looks like the Plum model encourages more of a constant sedimentation rate than the CRS model (at least in my very limited dataset). Indeed, this is one of the assumptions of the Plum (and Bacon) age-depth model...that the sedimentation rate of one core slice is somewhat similar to that before it. It's mostly a good assumption (probably a better assumption than a wildly varying sedimentation rate that is frequently inferred by the CRS model). It's possible to set the prior distribution for this (I think it's called "memory"), which may help steer Plum in the right direction.

### The next step

Black-box solutions like Plum and Bacon are hard. They are hard to understand, and hard to depend on (code-wise) because they don't have any unit tests and thus may fail in unexpected ways. I think that it may be possible to code these models in [STAN](https://mc-stan.org/), which has [excellent R support](https://cran.r-project.org/package=rstan). STAN is a domain-specific language for Bayesian models that allows a model such as that for Bacon and Plum to be coded in a 10-20 lines, rather than hundreds of lines of Python or C++. I'm [taking a break from paleolimnology this summer](https://resources.rstudio.com/rstudio-blog/summer-interns-2019), but look forward to getting back to this in the fall!

[/markdown]]]>		</content:encoded>
		<excerpt:encoded>
				<![CDATA[]]>		</excerpt:encoded>
		<wp:post_id>1510</wp:post_id>
		<wp:post_date><![CDATA[2019-05-01 10:05:28]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-05-01 13:05:28]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[doing-bayesian-lead-210-interpretation]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="academia"><![CDATA[Academia]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
				</channel>
</rss>
	